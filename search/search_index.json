{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hybrid Integration Reference Architecture IT environments are becoming hybrid in nature; most businesses use cloud computing as part of their overall IT environment. While businesses continue to operate enterprise applications, processes, and systems of record on premises, they are rapidly developing cloud-native applications on cloud. The hybrid integration reference architecture describes an approach to connect components which are split across cloud and on-premises environments, or across public and private clouds -- even across different cloud providers. This repository is also linked to the Event Driven Architecture repository where integration between microservices is supported by using event backbone and pub/sub integration pattern. Target audiences This solution implementation covers a lot of different and interesting subjects. If you are... an architect, you will get a deeper understanding of how all the components work together, and how to address API management, how to support cloud native polyglot applications and micro service while leveraging your existing investments in SOA and ESB pattern. a developer, you will get a broader view of the solution end to end and get existing starting code, and practices you may want to reuse during your future implementation. We focus on hybrid cloud and private cloud so some interesting areas like CI/CD in hybrid are covered. Test Driven Development with consumer driven contract testing. a project manager, you may understand all the artifacts to develop in an hybrid integration solution, and we may help in the future to do project estimation. a marketing person, you may want to google something else... What you will learn One of the goal of this implementation is to reflect what is commonly found in IT landscape in 2017, and provides recommendations on how to manage hybrid architecture with the cloud programming model by addressing non-functional requirements as scalability, security, monitoring and resiliency. By studying the set of projects and articles linked to this top repository, you will learn: How to develop a SOAP app in Java using JPA, JAXWS deployed on WebSphere Liberty How to develop gateway message flow with IBM Integration Bus How to define API product with API Connect, and use secure communication with TLS for backend APIs How to set up secure connection from IBM Cloud public to on-premise service using IBM Secure Gateway How to develop a Single Page Application with Angular 6 using a Test Driven Development approach with nodejs/expressjs back end How to secure the web app with passport How to access existing LDAP service for user authentication How to perform CI/CD in hybrid world How to monitor all those components using Application Performance Monitoring How to deploy most of the components of the solution to IBM Cloud Private How to call BPM process from Watson Conversation (orchestration), and how to integrate chat user interface connected to Watson Conversation into BPM coach. How to support service mesh with kubernetes What is the journey story to adopt hybrid cloud Introduction In this architecture, existing applications are moved to the infrastructure as a service (IaaS) of cloud providers, new applications are built on the cloud as a platform as a service (PaaS), using pre-built cloud-based software as a service (SaaS) services. The following diagram presents the high level view of the components involved in the hybrid integration reference architecture. For a deeper explanation of this architecture read this note . Each component may run on-premises, IaaS, PaaS or SaaS. This current project provides a reference implementation for building and running an hybrid integration solution, using cloud native web application securely connected to an enterprise data source and SOA services running on on-premise servers. We want to illustrate how to leverage existing SOA / Traditional IT landscape with products such as ESB, BPM, rule engine, Java based web service applications or even event driven publishers. Remember that the core purpose of SOA was to expose data and functions buried in systems of record over well-formed, simple-to-use, synchronous interfaces such as web services. In the longer term the brown compute will support the multiple integration patterns as presented in the figure below: Application Overview As an hybrid cloud solution implementation the set of projects of this solution cover different functional requirements: A web based portal to integrate internal applications for internal users. One of the function is to manage a simple computer product inventory, with warehouse and suppliers. A second feature is to implement a IT support chat bot so internal user can ask IT support questions and get response quickly, reducing the cost of operation of the support team. Support customer management, buyer of the telco products, used to support Analytics and machine learning Product recommendations based on business rules System context As architect we need to develop a system context, so the following diagram illustrates the logical components involved in the current solution, with the numbered items for short explanation: (the links below send you to the corresponding git repository where you can get more specific information and how tos.) 1. Web App \"Case Portal\" Portal web app (Angular 4) exposes a set of capabilities to internal users for inventory management, chatbots... 1. Interaction APIs exposes API products for public WebApp consumptions. Those APIs support specific resources needed by user interface app and the channels they serve. 1. Back End For Front End to support business logic of the web app, and simple integration of RESTful services. This is currently the server part of the web app ). As of now this BFF ( Back-end For Front-end pattern ) is done with nodejs app serving the Angular single page application. BFF pattern is still prevalent for mobile applications and single-page web applications. In this pattern, APIs are created specifically for the front-end application and perfectly suited to its needs with rationalized data models, ideal granularity of operations, specialized security models, and more. 1. System API to define backend service API products ( inventory APIs ), and customer APIs, used by multiple consumers. 1. Mediation flow deployed on Integration Bus to connect to back end systems and SOA services, and do interfaces mapping and mediation flows . 1. Data SOA, Java WS service to expose a data access layer on top of relational item, inventory, supplier database 1. Db2 deployment of the Inventory and Supplier database. 1. Watson conversation broker micro service to facade and implement orchestration and business logic for chatbots using Watson Conversation IBM Cloud service. 1. Supplier on boarding process , deployed as human centric process on IBM BPM on Cloud and triggered by Watson Conversation chatbot, or integration chat bot into BPM coach 1. Customer management for analytics micro services to support RESTful API. 1. Decision engine to automate business rules execution and Management for product recommendation in the context of user moving in different location with how to install ODM helm chart on IBM Cloud Private 1. LDAP for user Management to centralize authentication use cases. 1. Inventory update from the warehouse using IBM MQ , event producer and MDB deployed on WebSphere. We propose to extend the IT chatbot using Event processing for application state management to combine Decision Insight with MQ message and chat bot to manage inventory plus state. We have also other repositories to address... * Testing This repository includes a set of test cases to do component testing, functional testing and integration tests. User interface To demonstrate the set of features of this solution , a front end application, representing an internal portal is used to plug and play the different use cases. There is a login mechanism connected to a directory service (LDAP) This front end application is an extension of the \"CASE.inc\" retail store introduced in cloud native solution or \"Blue compute\" which manages old computers, extended with IT support chat bot and other goodies. Further Readings We are presenting Hybrid Cloud Integration body of knowledge in this article . We are compiling a ICP FAQ with kubernetes references.","title":"Hybrid Cloud Integration introduction"},{"location":"#hybrid-integration-reference-architecture","text":"IT environments are becoming hybrid in nature; most businesses use cloud computing as part of their overall IT environment. While businesses continue to operate enterprise applications, processes, and systems of record on premises, they are rapidly developing cloud-native applications on cloud. The hybrid integration reference architecture describes an approach to connect components which are split across cloud and on-premises environments, or across public and private clouds -- even across different cloud providers. This repository is also linked to the Event Driven Architecture repository where integration between microservices is supported by using event backbone and pub/sub integration pattern.","title":"Hybrid Integration Reference Architecture"},{"location":"#target-audiences","text":"This solution implementation covers a lot of different and interesting subjects. If you are... an architect, you will get a deeper understanding of how all the components work together, and how to address API management, how to support cloud native polyglot applications and micro service while leveraging your existing investments in SOA and ESB pattern. a developer, you will get a broader view of the solution end to end and get existing starting code, and practices you may want to reuse during your future implementation. We focus on hybrid cloud and private cloud so some interesting areas like CI/CD in hybrid are covered. Test Driven Development with consumer driven contract testing. a project manager, you may understand all the artifacts to develop in an hybrid integration solution, and we may help in the future to do project estimation. a marketing person, you may want to google something else...","title":"Target audiences"},{"location":"#what-you-will-learn","text":"One of the goal of this implementation is to reflect what is commonly found in IT landscape in 2017, and provides recommendations on how to manage hybrid architecture with the cloud programming model by addressing non-functional requirements as scalability, security, monitoring and resiliency. By studying the set of projects and articles linked to this top repository, you will learn: How to develop a SOAP app in Java using JPA, JAXWS deployed on WebSphere Liberty How to develop gateway message flow with IBM Integration Bus How to define API product with API Connect, and use secure communication with TLS for backend APIs How to set up secure connection from IBM Cloud public to on-premise service using IBM Secure Gateway How to develop a Single Page Application with Angular 6 using a Test Driven Development approach with nodejs/expressjs back end How to secure the web app with passport How to access existing LDAP service for user authentication How to perform CI/CD in hybrid world How to monitor all those components using Application Performance Monitoring How to deploy most of the components of the solution to IBM Cloud Private How to call BPM process from Watson Conversation (orchestration), and how to integrate chat user interface connected to Watson Conversation into BPM coach. How to support service mesh with kubernetes What is the journey story to adopt hybrid cloud","title":"What you will learn"},{"location":"#introduction","text":"In this architecture, existing applications are moved to the infrastructure as a service (IaaS) of cloud providers, new applications are built on the cloud as a platform as a service (PaaS), using pre-built cloud-based software as a service (SaaS) services. The following diagram presents the high level view of the components involved in the hybrid integration reference architecture. For a deeper explanation of this architecture read this note . Each component may run on-premises, IaaS, PaaS or SaaS. This current project provides a reference implementation for building and running an hybrid integration solution, using cloud native web application securely connected to an enterprise data source and SOA services running on on-premise servers. We want to illustrate how to leverage existing SOA / Traditional IT landscape with products such as ESB, BPM, rule engine, Java based web service applications or even event driven publishers. Remember that the core purpose of SOA was to expose data and functions buried in systems of record over well-formed, simple-to-use, synchronous interfaces such as web services. In the longer term the brown compute will support the multiple integration patterns as presented in the figure below:","title":"Introduction"},{"location":"#application-overview","text":"As an hybrid cloud solution implementation the set of projects of this solution cover different functional requirements: A web based portal to integrate internal applications for internal users. One of the function is to manage a simple computer product inventory, with warehouse and suppliers. A second feature is to implement a IT support chat bot so internal user can ask IT support questions and get response quickly, reducing the cost of operation of the support team. Support customer management, buyer of the telco products, used to support Analytics and machine learning Product recommendations based on business rules","title":"Application Overview"},{"location":"#system-context","text":"As architect we need to develop a system context, so the following diagram illustrates the logical components involved in the current solution, with the numbered items for short explanation: (the links below send you to the corresponding git repository where you can get more specific information and how tos.) 1. Web App \"Case Portal\" Portal web app (Angular 4) exposes a set of capabilities to internal users for inventory management, chatbots... 1. Interaction APIs exposes API products for public WebApp consumptions. Those APIs support specific resources needed by user interface app and the channels they serve. 1. Back End For Front End to support business logic of the web app, and simple integration of RESTful services. This is currently the server part of the web app ). As of now this BFF ( Back-end For Front-end pattern ) is done with nodejs app serving the Angular single page application. BFF pattern is still prevalent for mobile applications and single-page web applications. In this pattern, APIs are created specifically for the front-end application and perfectly suited to its needs with rationalized data models, ideal granularity of operations, specialized security models, and more. 1. System API to define backend service API products ( inventory APIs ), and customer APIs, used by multiple consumers. 1. Mediation flow deployed on Integration Bus to connect to back end systems and SOA services, and do interfaces mapping and mediation flows . 1. Data SOA, Java WS service to expose a data access layer on top of relational item, inventory, supplier database 1. Db2 deployment of the Inventory and Supplier database. 1. Watson conversation broker micro service to facade and implement orchestration and business logic for chatbots using Watson Conversation IBM Cloud service. 1. Supplier on boarding process , deployed as human centric process on IBM BPM on Cloud and triggered by Watson Conversation chatbot, or integration chat bot into BPM coach 1. Customer management for analytics micro services to support RESTful API. 1. Decision engine to automate business rules execution and Management for product recommendation in the context of user moving in different location with how to install ODM helm chart on IBM Cloud Private 1. LDAP for user Management to centralize authentication use cases. 1. Inventory update from the warehouse using IBM MQ , event producer and MDB deployed on WebSphere. We propose to extend the IT chatbot using Event processing for application state management to combine Decision Insight with MQ message and chat bot to manage inventory plus state. We have also other repositories to address... * Testing This repository includes a set of test cases to do component testing, functional testing and integration tests.","title":"System context"},{"location":"#user-interface","text":"To demonstrate the set of features of this solution , a front end application, representing an internal portal is used to plug and play the different use cases. There is a login mechanism connected to a directory service (LDAP) This front end application is an extension of the \"CASE.inc\" retail store introduced in cloud native solution or \"Blue compute\" which manages old computers, extended with IT support chat bot and other goodies.","title":"User interface"},{"location":"#further-readings","text":"We are presenting Hybrid Cloud Integration body of knowledge in this article . We are compiling a ICP FAQ with kubernetes references.","title":"Further Readings"},{"location":"TLS/","text":"Configure TLS end to end between IBM CLoud app and back end service The connection between bluemix app to back end data access service needs to be over HTTPS, HTTP over SSL. To make SSL working end to end we need to do certificate management, configure trust stores, understand handshaking, and other details that must be perfectly aligned to make the secure communication work. Quick TLS summary TLS and SSL uses public key/private key cryptography to encrypt data communication between the server and client, to allow the server to prove its identity to the client, and the client to prove its identity to the server. Three fundamental components are involved in setting up an SSL connection between a server and client: a certificate, a public key, a private key. Certificates are used to identify an identity: (CN, owner, location, state,... using the X509 distinguished name). Entity can be a person or a computer. As part of the identity, the CN or Common Name attribute is the name used to identify the domain name of the server host. To establish a secure connection to API Connect server, a client first resolves the domain name as specified in CN. After the SSL connection has been initiated, one of the first things the server will do is to send its digital certificate. The client will perform a number of validation steps before determining if it will continue with the connection. Most importantly, the client will compare the domain name of the server it intended to connect to (in this case, 172.16.50.8) with the common name (the \u201cCN\u201d field) found in the subject\u2019s identity on the certificate. If these names do not match, it means the client does not trust the identity of the server. This is the hand shake step. Public keys and private keys are number pairs with a special relationship. Any data encrypted with one key can be decrypted with the other. This is known as asymmetric encryption. The server\u2019s public key is embedded within its certificate. The public key is freely distributed so anyone wishing to establish an encrypted channel with the server may encrypt their data using the server\u2019s public key. Data encrypted with a private key may be decrypted with the corresponding public key. This property of keys is used to ensure the integrity of a digital certificate in a process called digital signing. In term of server / certificate we need to prepare a set of things, the following schema may help to understand the dependencies: API Gateway has its own public certificate, as IBM Secure Gateway. Get SSL Certificate for the API Connect Gateway end point from a Certificate Agency giving your domain name, with assured identity. The IBM self certified certificate should not work when the service consumer will do a hostname validation. For Brown Compute we are still using the self certified certificate and we will highlight the impact on the client code to bypass host validation. Define TLS profile configuration for API Connect using its Cloud Management console Get certificate for each of the components in the path Let go over those steps in details: pre-requisites You need to have openssl installed on your computer. For MAC users it is already installed. If you need to install see instruction here 1. Get IBM Cloud Secure Gateway certificate The following command returns a lot of helpful information from a server like the IBM Secure Gateway we configured on Bluemix. openssl s_client -connect cap-sg-prd-5.integration.ibmcloud.com:16582` Adapt the URL with your own Secure Gateway service end point* In the returned output, we can see the certificate chain presented by the server with the subject and issuer information: Certificate chain 0 s:/C=US/ST=NC/L=Durham/O=IBM Corporation/OU=SWG/CN=*.integration.ibmcloud.com i:/C=US/O=DigiCert Inc/CN=DigiCert SHA2 Secure Server CA 1 s:/C=US/O=DigiCert Inc/CN=DigiCert SHA2 Secure Server CA i:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert Global Root CA one important output is the protocol and cipher suite used: SSL-Session: Protocol : TLSv1 Cipher : AES256-SHA If we need to keep the server's PEM-encoded certificate save the string \"\"---BEGIN CERTIFICATE to END CERTIFICATE\" to a file. (e.g sg.pem). The following command will do it for you: echo | openssl s_client -connect cap-sg-prd-5.integration.ibmcloud.com:16582 -showcerts 2>&1 | sed -n '/BEGIN CERTIFICATE/,/-END CERTIFICATE-/p'> sg.pem By default, s_client will print only the leaf certificate; as we want to print the entire chain, we use -showcerts switch. Use this certificate for the client code in Bluemix. As an alternate we can download the authentication files from the Secure Gateway destination: as explained in this article and use all those certificates files to define the connection. 2. Get the APIC server certificate When connected via VPN to your on-premise environment, you can get the TLS certificate for the API Connect Gateway server via the command: echo | openssl s_client -connect 172.16.50.8:443 -showcerts 2>&1 | sed -n '/BEGIN CERTIFICATE/,/-END CERTIFICATE-/p'> apicgw.pem 3. Using Self certified TSL certificates in a client app To make TSL working end to end we need to do certificate management, configure trust stores, understand handshaking, and other details that must be perfectly aligned to make the secure communication work. We assume we downloaded the different certificates from secure gateway, the connection between the client app and the secure gateway is via TLS mutual auth. Nodejs app Using the request module we can use the different certificates as settings to the options argument of the connection. Here is an example of a HTTP GET over TLS: request . get ( { url : 'https://cap-sg-prd-5.integration.ibmcloud.com:16582/csplab/sb/sample-inventory-api/items' , timeout : 10000 , headers : { 'x-ibm-client-id' : '1dc939dd-xxxx' , 'accept' : 'application/json' , 'content-type' : 'application/json' } }); So we need to prepare those input files on the client side (in the Case inc portal code source for example): Create a key store with the openssl tool. openssl pkcs12 -export -in \"./ssl/qsn47KM8iTa_O495D_destCert.pem\" -inkey \"./ssl/qsn47KM8iTa_O495D_destKey.pem\" -out \"ssl/sg_key.p12\" -name \"CaseIncCliCert\" -noiter -password pass:\"asuperpwd\" The file is sg_key.p12 Create a trust store from the DigiCertCA2.pem file keytool - import - alias PrimaryCA - file / ssl / DigiCertCA2 . pem - storepass password - keystore / ssl / sg_trust . jks Then for the secondary CA keytool - import - alias SecondaryCA - file / ssl / DigiCertTrustedRoot . pem - storepass password - keystore / ssl / sg_trust . jks finally the certificate for the secure gateway keytool - import - alias BmxGtwServ - file / ssl / secureGatewayCert . pem - storepass password - keystore / ssl / sg_trust . jks Step 5- Download the API Connect certificate To access the certificate use a Web browser, like Firefox, to the target URL using HTTPS. Access the Security > Certificate from the locker icon on left side of the URL field. (Each web browser has their own way to access to the self certified certificates) Use the export button to create a new local file with suffix .crt. From there you need to persist the file on the operating system trust store. ``` get certificate in the form of a .crt file, then mv it to ca-certificate $ sudo mv APIConnect.crt /usr/local/share/ca-certificate To add en try to the certificates use the command $ sudo update-ca-certificates verify with $ ls -al /etc/ssl/certs | grep APIConnect $ openssl s_client -showcerts -connect 172.16.50.8:443 ## Specific to Java Trust store Java Runtime Environment comes with a pre-configure set of trusted certificate authorities. The collection of trusted certificates can be found at $JAVA_HOME/jre/lib/security/cacerts The tests are run on the utility server, so the API Connect server CA certificate needs to be in place. To do so the following needs to be done: Remote connect to the API Connect Gateway Server with a Web Browser and download the certificate as .crt file $ sudo keytool -import -trustcacerts -alias brownapic -file APIConnect.crt -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass changeit $ keytool -list -keystore $JAVA_HOME/jre/lib/security/cacerts ``` Attention these steps will make the Java program using HTTP client working only if the certificate is defined by a certified agency. The self generated certificate has a CN attribute sets to a non-hostname, and HTTP client in Java when doing SSL connection are doing a hostname verification. See the test project for the detail on how it was bypassed, in Brown compute. References Open SSL web site SSL Cookbook","title":"TLS"},{"location":"TLS/#configure-tls-end-to-end-between-ibm-cloud-app-and-back-end-service","text":"The connection between bluemix app to back end data access service needs to be over HTTPS, HTTP over SSL. To make SSL working end to end we need to do certificate management, configure trust stores, understand handshaking, and other details that must be perfectly aligned to make the secure communication work.","title":"Configure TLS end to end between IBM CLoud app and back end service"},{"location":"TLS/#quick-tls-summary","text":"TLS and SSL uses public key/private key cryptography to encrypt data communication between the server and client, to allow the server to prove its identity to the client, and the client to prove its identity to the server. Three fundamental components are involved in setting up an SSL connection between a server and client: a certificate, a public key, a private key. Certificates are used to identify an identity: (CN, owner, location, state,... using the X509 distinguished name). Entity can be a person or a computer. As part of the identity, the CN or Common Name attribute is the name used to identify the domain name of the server host. To establish a secure connection to API Connect server, a client first resolves the domain name as specified in CN. After the SSL connection has been initiated, one of the first things the server will do is to send its digital certificate. The client will perform a number of validation steps before determining if it will continue with the connection. Most importantly, the client will compare the domain name of the server it intended to connect to (in this case, 172.16.50.8) with the common name (the \u201cCN\u201d field) found in the subject\u2019s identity on the certificate. If these names do not match, it means the client does not trust the identity of the server. This is the hand shake step. Public keys and private keys are number pairs with a special relationship. Any data encrypted with one key can be decrypted with the other. This is known as asymmetric encryption. The server\u2019s public key is embedded within its certificate. The public key is freely distributed so anyone wishing to establish an encrypted channel with the server may encrypt their data using the server\u2019s public key. Data encrypted with a private key may be decrypted with the corresponding public key. This property of keys is used to ensure the integrity of a digital certificate in a process called digital signing. In term of server / certificate we need to prepare a set of things, the following schema may help to understand the dependencies: API Gateway has its own public certificate, as IBM Secure Gateway. Get SSL Certificate for the API Connect Gateway end point from a Certificate Agency giving your domain name, with assured identity. The IBM self certified certificate should not work when the service consumer will do a hostname validation. For Brown Compute we are still using the self certified certificate and we will highlight the impact on the client code to bypass host validation. Define TLS profile configuration for API Connect using its Cloud Management console Get certificate for each of the components in the path Let go over those steps in details:","title":"Quick TLS summary"},{"location":"TLS/#pre-requisites","text":"You need to have openssl installed on your computer. For MAC users it is already installed. If you need to install see instruction here","title":"pre-requisites"},{"location":"TLS/#1-get-ibm-cloud-secure-gateway-certificate","text":"The following command returns a lot of helpful information from a server like the IBM Secure Gateway we configured on Bluemix. openssl s_client -connect cap-sg-prd-5.integration.ibmcloud.com:16582` Adapt the URL with your own Secure Gateway service end point* In the returned output, we can see the certificate chain presented by the server with the subject and issuer information: Certificate chain 0 s:/C=US/ST=NC/L=Durham/O=IBM Corporation/OU=SWG/CN=*.integration.ibmcloud.com i:/C=US/O=DigiCert Inc/CN=DigiCert SHA2 Secure Server CA 1 s:/C=US/O=DigiCert Inc/CN=DigiCert SHA2 Secure Server CA i:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert Global Root CA one important output is the protocol and cipher suite used: SSL-Session: Protocol : TLSv1 Cipher : AES256-SHA If we need to keep the server's PEM-encoded certificate save the string \"\"---BEGIN CERTIFICATE to END CERTIFICATE\" to a file. (e.g sg.pem). The following command will do it for you: echo | openssl s_client -connect cap-sg-prd-5.integration.ibmcloud.com:16582 -showcerts 2>&1 | sed -n '/BEGIN CERTIFICATE/,/-END CERTIFICATE-/p'> sg.pem By default, s_client will print only the leaf certificate; as we want to print the entire chain, we use -showcerts switch. Use this certificate for the client code in Bluemix. As an alternate we can download the authentication files from the Secure Gateway destination: as explained in this article and use all those certificates files to define the connection.","title":"1. Get IBM Cloud Secure Gateway certificate"},{"location":"TLS/#2-get-the-apic-server-certificate","text":"When connected via VPN to your on-premise environment, you can get the TLS certificate for the API Connect Gateway server via the command: echo | openssl s_client -connect 172.16.50.8:443 -showcerts 2>&1 | sed -n '/BEGIN CERTIFICATE/,/-END CERTIFICATE-/p'> apicgw.pem","title":"2. Get the APIC server certificate"},{"location":"TLS/#3-using-self-certified-tsl-certificates-in-a-client-app","text":"To make TSL working end to end we need to do certificate management, configure trust stores, understand handshaking, and other details that must be perfectly aligned to make the secure communication work. We assume we downloaded the different certificates from secure gateway, the connection between the client app and the secure gateway is via TLS mutual auth.","title":"3. Using Self certified TSL certificates in a client app"},{"location":"TLS/#nodejs-app","text":"Using the request module we can use the different certificates as settings to the options argument of the connection. Here is an example of a HTTP GET over TLS: request . get ( { url : 'https://cap-sg-prd-5.integration.ibmcloud.com:16582/csplab/sb/sample-inventory-api/items' , timeout : 10000 , headers : { 'x-ibm-client-id' : '1dc939dd-xxxx' , 'accept' : 'application/json' , 'content-type' : 'application/json' } }); So we need to prepare those input files on the client side (in the Case inc portal code source for example): Create a key store with the openssl tool. openssl pkcs12 -export -in \"./ssl/qsn47KM8iTa_O495D_destCert.pem\" -inkey \"./ssl/qsn47KM8iTa_O495D_destKey.pem\" -out \"ssl/sg_key.p12\" -name \"CaseIncCliCert\" -noiter -password pass:\"asuperpwd\" The file is sg_key.p12 Create a trust store from the DigiCertCA2.pem file keytool - import - alias PrimaryCA - file / ssl / DigiCertCA2 . pem - storepass password - keystore / ssl / sg_trust . jks Then for the secondary CA keytool - import - alias SecondaryCA - file / ssl / DigiCertTrustedRoot . pem - storepass password - keystore / ssl / sg_trust . jks finally the certificate for the secure gateway keytool - import - alias BmxGtwServ - file / ssl / secureGatewayCert . pem - storepass password - keystore / ssl / sg_trust . jks","title":"Nodejs app"},{"location":"TLS/#step-5-download-the-api-connect-certificate","text":"To access the certificate use a Web browser, like Firefox, to the target URL using HTTPS. Access the Security > Certificate from the locker icon on left side of the URL field. (Each web browser has their own way to access to the self certified certificates) Use the export button to create a new local file with suffix .crt. From there you need to persist the file on the operating system trust store. ```","title":"Step 5- Download the API Connect certificate"},{"location":"TLS/#get-certificate-in-the-form-of-a-crt-file-then-mv-it-to-ca-certificate","text":"$ sudo mv APIConnect.crt /usr/local/share/ca-certificate","title":"get certificate in the form of a .crt file, then mv it to ca-certificate"},{"location":"TLS/#to-add-en-try-to-the-certificates-use-the-command","text":"$ sudo update-ca-certificates","title":"To add en try to the certificates use the command"},{"location":"TLS/#verify-with","text":"$ ls -al /etc/ssl/certs | grep APIConnect $ openssl s_client -showcerts -connect 172.16.50.8:443 ## Specific to Java Trust store Java Runtime Environment comes with a pre-configure set of trusted certificate authorities. The collection of trusted certificates can be found at $JAVA_HOME/jre/lib/security/cacerts The tests are run on the utility server, so the API Connect server CA certificate needs to be in place. To do so the following needs to be done: Remote connect to the API Connect Gateway Server with a Web Browser and download the certificate as .crt file $ sudo keytool -import -trustcacerts -alias brownapic -file APIConnect.crt -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass changeit $ keytool -list -keystore $JAVA_HOME/jre/lib/security/cacerts ``` Attention these steps will make the Java program using HTTP client working only if the certificate is defined by a certified agency. The self generated certificate has a CN attribute sets to a non-hostname, and HTTP client in Java when doing SSL connection are doing a hostname verification. See the test project for the detail on how it was bypassed, in Brown compute.","title":"verify with"},{"location":"TLS/#references","text":"Open SSL web site SSL Cookbook","title":"References"},{"location":"compendium/","text":"Compendium Here is a list of important public content to improve your skill on the products and concepts used in this solution. Architecture discussion on hybrid integration: How to ensure your integration landscape keeps pace with digital transformation article: The evolving hybrid integration reference architecture How the 12 factors to measure component for cloud native app and micro service apply to hybrid integration: The 12 factors integration Achieving lightweight integration with IBM Integration Bus Redbook: \"A practical Guide for IBM Hybrid Integration Platform\" Product related knowledge based: Integration The fate of ESB- Integration pattern Moving to lightweight integration API Connect API Connect knowledge center IBM Integration Bus IIB developing integration solution - knowledge center Tutorials on IIB View the IBM Integration Bus Dockerfile repository on Github Learn more about IBM Integration Bus Docker Tips and Tricks Learn more about IBM Integration Bus and Kubernetes Learn more about running IBM Integration Bus in the Bluemix Container Service Learn more about IBM Integration Bus Learn more about IBM Integration Bus and Docker Lightweight integration: Using microservices principles in integration Kubernetes Compendium Official site Kubernetes concepts Helm and Kubernetes package management Very good tutorial from kubernetes web site Garage method tutorial on Kubernetes IBM Cloud Private Compendium IBM technical community ICP blog Our install ICP 2.1 Community edition on ubuntu VM ICP Enterprise deployment tutorial Build your own helm repository IBM Operational Decision management Kubernetes helm charts for ODM ODM on Docker, Kubernetes, and IBM Cloud Private Secure Gateway IBM Secure Gateway - IBM Cloud documentation Open source used Angular 4 tutorial from the angular site this tutorial is updated with new release and covers the most important features used in our angular app. Helm getting started Kubernetes tutorials : you need to study basic, 101, 201 at least.","title":"Hybrid Cloud Compendium"},{"location":"compendium/#compendium","text":"Here is a list of important public content to improve your skill on the products and concepts used in this solution.","title":"Compendium"},{"location":"compendium/#architecture-discussion-on-hybrid-integration","text":"How to ensure your integration landscape keeps pace with digital transformation article: The evolving hybrid integration reference architecture How the 12 factors to measure component for cloud native app and micro service apply to hybrid integration: The 12 factors integration Achieving lightweight integration with IBM Integration Bus Redbook: \"A practical Guide for IBM Hybrid Integration Platform\"","title":"Architecture discussion on hybrid integration:"},{"location":"compendium/#product-related-knowledge-based","text":"","title":"Product related knowledge based:"},{"location":"compendium/#integration","text":"The fate of ESB- Integration pattern Moving to lightweight integration","title":"Integration"},{"location":"compendium/#api-connect","text":"API Connect knowledge center","title":"API Connect"},{"location":"compendium/#ibm-integration-bus","text":"IIB developing integration solution - knowledge center Tutorials on IIB View the IBM Integration Bus Dockerfile repository on Github Learn more about IBM Integration Bus Docker Tips and Tricks Learn more about IBM Integration Bus and Kubernetes Learn more about running IBM Integration Bus in the Bluemix Container Service Learn more about IBM Integration Bus Learn more about IBM Integration Bus and Docker Lightweight integration: Using microservices principles in integration","title":"IBM Integration Bus"},{"location":"compendium/#kubernetes-compendium","text":"Official site Kubernetes concepts Helm and Kubernetes package management Very good tutorial from kubernetes web site Garage method tutorial on Kubernetes","title":"Kubernetes Compendium"},{"location":"compendium/#ibm-cloud-private-compendium","text":"IBM technical community ICP blog Our install ICP 2.1 Community edition on ubuntu VM ICP Enterprise deployment tutorial Build your own helm repository","title":"IBM Cloud Private Compendium"},{"location":"compendium/#ibm-operational-decision-management","text":"Kubernetes helm charts for ODM ODM on Docker, Kubernetes, and IBM Cloud Private","title":"IBM Operational Decision management"},{"location":"compendium/#secure-gateway","text":"IBM Secure Gateway - IBM Cloud documentation","title":"Secure Gateway"},{"location":"compendium/#open-source-used","text":"Angular 4 tutorial from the angular site this tutorial is updated with new release and covers the most important features used in our angular app. Helm getting started Kubernetes tutorials : you need to study basic, 101, 201 at least.","title":"Open source used"},{"location":"hybrid-ref-arch/","text":"Hybrid Integration Reference Architecture The hybrid integration reference architecture can be depicted at the highest level as the following diagram: The on-premise capabilities are in the lower parts and most likely host the system of records (SoR), the lower level of connectivity like Java based SOA services, Integration BUS flows, BPEL flows, service orchestration. This level offers core, reusable business operations, applying the strong SOA patterns. On top of this layer co-exist different IT capabilities, like asynchronous event based architecture, API composition and aggregation, and data synchronization, data movement. These capabilities offer a set of features for the 'digital team', who develops new cloud native applications. Those capabilities can run on-premise, on private cloud or even some of them on public cloud. As part of the API economy those capabilities can be exposed as managed APIs with API and event gateway used to control API access, governance and monitoring. The can represent 'System APIs'. When we develop new application on cloud, web apps or micro services is to support system of engagement business logic, meaning support specific business requirements for a specific business or technology channels. Those applications need to be developed quickly to address a new business opportunity. Cloud based development with scripting language, continuous deployment and integration, container, enables this quick around time for such development. Those micro services do not need to be just nodejs app, they can for sure being java based, but even micro flows in Integration Bus. They need to do service mash up, aggregation of API, data model transformation, asynchronous event consumptions or emission. The deployment of these capabilities will reside on cloud based platform using container orchestration. Those business functions can also be exposed via API managed with API gateway. They represent external APIs. Finally at the top reside the client apps consumer of the system of engagement services. Hybrid integration platform The following diagram presents the different hybrid integration patterns, you may find in modern IT where cloud native applications deployed on public cloud (top) can access on-premise resource deployed on IBM Cloud Private (ICP) or more traditional back end systems. Modern enterprise IT has applications of engagement which typically reside on a public cloud since they are internet facing. These could be mobile, web or applications from business partners that consume external APIs published by the enterprise. These APIs are exposed externally using a dedicated API Gateway for isolation and provide the required integration to enterprise backend systems. While enterprise systems are usually considered as systems of record, the adoption of microservices within enterprise IT introduces a layer of separation between backend systems like Databases, ERP systems and application microservices. Applications that are traditionally considered as backend systems could be refactored as microservices for IT modernization. Microservices running on IBM Cloud Private ( ICP ) could have different requirements for integration to backend systems depending on their composition. Following are the integration patterns for applications running on ICP: Application migrated to ICP connecting to backend systems using integration services running in backend layer. Applications and the required integration services are migrated to ICP. Multiple applications can share the integration services. Each microservice within an application has a dedicated integration service. This pattern would apply where the microservice and integration service are owned by the same team. The integration service can be shared by other microservices as well e.g integration service defines integration with a backend HR system. The integration services will follow the principles of lightweight integration - https://www.ibm.com/developerworks/cloud/library/cl-lightweight-integration-2/index.html Applications with decoupled microservices for robustness could use lightweight publish/subscribe messaging for inter-communication and call backend systems using internal APIs. Internal APIs are exposed using a dedicated API Gateway and consumed by internal applications. The figure below zoom into the pattern of using micro services and application as a group of micro services to support the system of engagement. Those applications are exposing API to the external consumers (mobile app, Single page application) and consume internal / system APIs. Micro service component could also be used to improve the agility, scalability, and resilience of a system of record. They offer a lot of benefits on the right circumstances. These patterns provide integration architectures for modern enterprise applications as they progress through the IT modernization journey. It is possible that a modern enterprise may have to use all the above integration patterns to meet the application requirements.","title":"Hybrid Cloud Integration reference architecture"},{"location":"hybrid-ref-arch/#hybrid-integration-reference-architecture","text":"The hybrid integration reference architecture can be depicted at the highest level as the following diagram: The on-premise capabilities are in the lower parts and most likely host the system of records (SoR), the lower level of connectivity like Java based SOA services, Integration BUS flows, BPEL flows, service orchestration. This level offers core, reusable business operations, applying the strong SOA patterns. On top of this layer co-exist different IT capabilities, like asynchronous event based architecture, API composition and aggregation, and data synchronization, data movement. These capabilities offer a set of features for the 'digital team', who develops new cloud native applications. Those capabilities can run on-premise, on private cloud or even some of them on public cloud. As part of the API economy those capabilities can be exposed as managed APIs with API and event gateway used to control API access, governance and monitoring. The can represent 'System APIs'. When we develop new application on cloud, web apps or micro services is to support system of engagement business logic, meaning support specific business requirements for a specific business or technology channels. Those applications need to be developed quickly to address a new business opportunity. Cloud based development with scripting language, continuous deployment and integration, container, enables this quick around time for such development. Those micro services do not need to be just nodejs app, they can for sure being java based, but even micro flows in Integration Bus. They need to do service mash up, aggregation of API, data model transformation, asynchronous event consumptions or emission. The deployment of these capabilities will reside on cloud based platform using container orchestration. Those business functions can also be exposed via API managed with API gateway. They represent external APIs. Finally at the top reside the client apps consumer of the system of engagement services.","title":"Hybrid Integration Reference Architecture"},{"location":"hybrid-ref-arch/#hybrid-integration-platform","text":"The following diagram presents the different hybrid integration patterns, you may find in modern IT where cloud native applications deployed on public cloud (top) can access on-premise resource deployed on IBM Cloud Private (ICP) or more traditional back end systems. Modern enterprise IT has applications of engagement which typically reside on a public cloud since they are internet facing. These could be mobile, web or applications from business partners that consume external APIs published by the enterprise. These APIs are exposed externally using a dedicated API Gateway for isolation and provide the required integration to enterprise backend systems. While enterprise systems are usually considered as systems of record, the adoption of microservices within enterprise IT introduces a layer of separation between backend systems like Databases, ERP systems and application microservices. Applications that are traditionally considered as backend systems could be refactored as microservices for IT modernization. Microservices running on IBM Cloud Private ( ICP ) could have different requirements for integration to backend systems depending on their composition. Following are the integration patterns for applications running on ICP: Application migrated to ICP connecting to backend systems using integration services running in backend layer. Applications and the required integration services are migrated to ICP. Multiple applications can share the integration services. Each microservice within an application has a dedicated integration service. This pattern would apply where the microservice and integration service are owned by the same team. The integration service can be shared by other microservices as well e.g integration service defines integration with a backend HR system. The integration services will follow the principles of lightweight integration - https://www.ibm.com/developerworks/cloud/library/cl-lightweight-integration-2/index.html Applications with decoupled microservices for robustness could use lightweight publish/subscribe messaging for inter-communication and call backend systems using internal APIs. Internal APIs are exposed using a dedicated API Gateway and consumed by internal applications. The figure below zoom into the pattern of using micro services and application as a group of micro services to support the system of engagement. Those applications are exposing API to the external consumers (mobile app, Single page application) and consume internal / system APIs. Micro service component could also be used to improve the agility, scalability, and resilience of a system of record. They offer a lot of benefits on the right circumstances. These patterns provide integration architectures for modern enterprise applications as they progress through the IT modernization journey. It is possible that a modern enterprise may have to use all the above integration patterns to meet the application requirements.","title":"Hybrid integration platform"},{"location":"messaging-usecase/","text":"Processing messaging with MQ, DSI and Conversation Use case Paul is using the Brown Portal application and he cannot access inventory plus the old computer inventory management application, he is asking to the IT Support chat bot to get advise on what to do. The bot identifies the intent is to access the Inventory plus application, so the response is to get application status. The orchestration layer is calling the ODM Decision Server Insight to get the status of the app via REST api. The application is degraded so the recommendation from the bot is to wait, put the email or userid of the impacted user into the list of impacted user so when the application is coming back online an email will be automatically sent to the user by ODM DSI. So how DSI knows about the application status? Inventory Plus is an application running on ICP and hybrid cloud and has a set of sub component and microservices as part of the solution. We are managing each component with APM. Here is a schema of the Inventory Plus application: When one component is degraged APM sends a message to a topics managed by MQ manager deployed on ICP: DSI subscribes to this topic and processes the message to change the state of the application. MQ configuration Simulator of APM event APM configuration ODM DSI solution The DSI solution is documented here","title":"Processing messaging with MQ, DSI and Conversation"},{"location":"messaging-usecase/#processing-messaging-with-mq-dsi-and-conversation","text":"","title":"Processing messaging with MQ, DSI and Conversation"},{"location":"messaging-usecase/#use-case","text":"Paul is using the Brown Portal application and he cannot access inventory plus the old computer inventory management application, he is asking to the IT Support chat bot to get advise on what to do. The bot identifies the intent is to access the Inventory plus application, so the response is to get application status. The orchestration layer is calling the ODM Decision Server Insight to get the status of the app via REST api. The application is degraded so the recommendation from the bot is to wait, put the email or userid of the impacted user into the list of impacted user so when the application is coming back online an email will be automatically sent to the user by ODM DSI. So how DSI knows about the application status? Inventory Plus is an application running on ICP and hybrid cloud and has a set of sub component and microservices as part of the solution. We are managing each component with APM. Here is a schema of the Inventory Plus application: When one component is degraged APM sends a message to a topics managed by MQ manager deployed on ICP: DSI subscribes to this topic and processes the message to change the state of the application.","title":"Use case"},{"location":"messaging-usecase/#mq-configuration","text":"","title":"MQ configuration"},{"location":"messaging-usecase/#simulator-of-apm-event","text":"","title":"Simulator of APM event"},{"location":"messaging-usecase/#apm-configuration","text":"","title":"APM configuration"},{"location":"messaging-usecase/#odm-dsi-solution","text":"The DSI solution is documented here","title":"ODM DSI solution"},{"location":"nfr/","text":"Non Functional Requirements In this section we are covering some of the non functional Requirements Table of contents Security Resiliency DevOps Service Management Security Multiple security concerns are addressed by the hybrid integration compute model. The first one is to support the deployment of private on-premise LDAP directory. The installation and configuration of the Open LDAP on the Utility server is described here . Second, to control the access from a IBM Cloud app, we first implemented an adhoc solution integrating passport.js and using a /login path defined in our inventory product in API Connect. See explanation here on how we did it. The connection between the web app, front end of hybrid integration compute and the back end is done over TLS socket, we present a quick summary of TLS and how TLS end to end is performed in this article The front end login mechanism on how we support injecting secure token for API calls is documented here Add a IBM Secure Gateway IBM Cloud Service To authorize the web application running on IBM Cloud to access the API Connect gateway running on on-premise servers (or any end-point on on-premise servers), we use the IBM Secure Gateway product and the IBM Cloud Secure Gateway service: the configuration details and best practices can be found in this article Resiliency Making the Portal App Resilient Please check this repository for instructions and tools to improve availability and performances of the hybrid integration Compute front end application. High availability We do not plan to implement complex topology for the on-premise servers to support HA, mostly because of cost and time reason and the fact that it is covered a lot in different articles. For a basic introduction of HA and DR in a two or three data center you can read this article . For IBM Cloud Private read the following ICP cluster HA article Using container helps to control the server and OS configuration with the application code in a unique integrated deployment unit. This is a huge advantage for high availability of those applications. With modern single page application for web application, the micro services are more stateless and so the complexity of high availability between data center resides in the following areas: code replication as part of the continuous deployment data replication for the different datasource, SQL, non SQL and files. The different type of data may lead to different RPO and even loss tolerance. May be loosing, a tweet, a comment, or the picture of the aunt's cat are not that important, while a sale or bank transactions are. load balancing at the edge services number of instances for each of the different component of the solution. Is it possible to have at least three instances for each micro service running in parallel. network latency security token management","title":"Non-functional requirements"},{"location":"nfr/#non-functional-requirements","text":"In this section we are covering some of the non functional Requirements","title":"Non Functional Requirements"},{"location":"nfr/#table-of-contents","text":"Security Resiliency DevOps Service Management","title":"Table of contents"},{"location":"nfr/#security","text":"Multiple security concerns are addressed by the hybrid integration compute model. The first one is to support the deployment of private on-premise LDAP directory. The installation and configuration of the Open LDAP on the Utility server is described here . Second, to control the access from a IBM Cloud app, we first implemented an adhoc solution integrating passport.js and using a /login path defined in our inventory product in API Connect. See explanation here on how we did it. The connection between the web app, front end of hybrid integration compute and the back end is done over TLS socket, we present a quick summary of TLS and how TLS end to end is performed in this article The front end login mechanism on how we support injecting secure token for API calls is documented here","title":"Security"},{"location":"nfr/#add-a-ibm-secure-gateway-ibm-cloud-service","text":"To authorize the web application running on IBM Cloud to access the API Connect gateway running on on-premise servers (or any end-point on on-premise servers), we use the IBM Secure Gateway product and the IBM Cloud Secure Gateway service: the configuration details and best practices can be found in this article","title":"Add a IBM Secure Gateway IBM Cloud Service"},{"location":"nfr/#resiliency","text":"Making the Portal App Resilient Please check this repository for instructions and tools to improve availability and performances of the hybrid integration Compute front end application.","title":"Resiliency"},{"location":"nfr/#high-availability","text":"We do not plan to implement complex topology for the on-premise servers to support HA, mostly because of cost and time reason and the fact that it is covered a lot in different articles. For a basic introduction of HA and DR in a two or three data center you can read this article . For IBM Cloud Private read the following ICP cluster HA article Using container helps to control the server and OS configuration with the application code in a unique integrated deployment unit. This is a huge advantage for high availability of those applications. With modern single page application for web application, the micro services are more stateless and so the complexity of high availability between data center resides in the following areas: code replication as part of the continuous deployment data replication for the different datasource, SQL, non SQL and files. The different type of data may lead to different RPO and even loss tolerance. May be loosing, a tweet, a comment, or the picture of the aunt's cat are not that important, while a sale or bank transactions are. load balancing at the edge services number of instances for each of the different component of the solution. Is it possible to have at least three instances for each micro service running in parallel. network latency security token management","title":"High availability"},{"location":"buildrun/","text":"Build, Deploy and Run To address the hybrid cloud integration pattern, we are proposing two deployment configurations combining public cloud, private cloud and on-premise traditional servers: Configuration 1 : the Portal Web Application, BFF micro service and API Management run on public cloud and the other legacy components on-premise servers: On left side is the cloud public deployment, on the right side, the bear metal or VM servers. The Web Application is deployed using container and IBM Cloud container service. (See this note for explanations). Using docker image helps us to move workload to Kubernetes cluster running on-premise as presented in the second configuration. The communication between the two environments is done using IBM Secure Gateway service or can be done using private VPN. 1. Configuration 2 : covers the deployment of most of the solution components on to IBM Cloud Private, the IBM extension to Kubernetes. The BFF micro service is calling IBM cloud cognitive services running on IBM Cloud and on-premise backend services: The light blue area represents on-premise servers, while the green area represents the IBM Cloud Private, kubernetes cluster. See detail in Deployment to IBM Cloud Private article Prerequisites You need your own github.com account You need a git client code. For example for Windows and for Mac Install npm and nodejs . Normally getting nodejs last stable version will bring npm too. You need to have some knowledge on using virtual machine images and product like VMWare vSphere. As we are migrating most of the workload to IBM Cloud Private, we delivered, for each component, dockerfiles and helm charts to deploy on Kubernetes. Get application source code Clone this base repository using git client: git clone https://github.com/ibm-cloud-architecture/refarch-integration.git Then under the refarch-integration/scripts folder use the command clonePeers.sh to clone the peer repositories of the 'hybrid integration' solution. Working on your own The script fork-repos.sh should help you to fork all the repositories of this solution within your github account. Continuous integration and deployment We are detailing how to install a 'jenkins' server on ICP and delivering jenkins file and scripts for each project to do CI/CD. See this note for installation detail. Specifics deployment Each project covers in detail how to build and run their own components. For the Case Inc Portal app see the deployment note. For API Connect , installed on-premise, is used as API gateway to the different API run times. The IBM Integration Bus deployment , is used to do interfaces mapping between the SOAP data access layer, implemented in Java, and the RESTful API exposed to the public applications, and other mediation or orchestration flow. The Data Access Layer is a JAXWS application running on WebSphere Liberty server and exposing a set of SOAP services. The server is BrownLibertyAppServer . See this repository for detail. The inventory database is running on DB2 and is not directly accessed from API connect, but applying SOA principles, it is accessed via a Data Access Layer app. The server is BrownDB2*. To run the backend solution, we will deliver VM images for you to install on your servers... stay tuned, from now we are describing how each server is configured in each of the specific github repository. We are using VmWare vSphere product to manage all the virtual machines. The figure below presents the Brown Resource Pool with the current servers: Configuration 1 Physical Deployment The Configuration 1 Physical deployment includes six servers, and each installation description is done in each git hub repository so you can replicate the configuration if you want to. It should take you one hour per server. DB2 server read this note Liberty App server read this article IBM Integration Bus see this article . API Connect see Server config Open LDAP Server running on the utility server LDAP Configuration Utility Server runs IBM Secure Gateway and Jenkins server As an alternate and easier approach we are delivering a Vagrant file to combine Liberty, DB2 servers in one image: the explanation on how to use it is here Run on premise servers There are multiple steps to make the solution working. Be sure to start each sever in the following order: Start DB2 server Start App server Start IIB Start API Connect servers: Gateway, Management and Portal Start Utility server Start 'case inc' portal APP The testing project implements a set of test cases to validate each of the component of this n-tier architecture. It is possible to validate each component work independently. The demonstration script instructions are here For demonstration purpose not all back end servers are set in high availability. Run on IBM Cloud Private Most of the components of this solution can run on IBM Cloud Private we are detailing it here Run on IBM Cloud Container Service See this detail note here to deploy and run the Web App as container inside the IBM Cloud Container Service . Run on IBM Cloud Cloud Foundry See this detail note here to deploy the Web App as cloud foundry app on IBM Cloud","title":"DevOps Build, deploy and run"},{"location":"buildrun/#build-deploy-and-run","text":"To address the hybrid cloud integration pattern, we are proposing two deployment configurations combining public cloud, private cloud and on-premise traditional servers: Configuration 1 : the Portal Web Application, BFF micro service and API Management run on public cloud and the other legacy components on-premise servers: On left side is the cloud public deployment, on the right side, the bear metal or VM servers. The Web Application is deployed using container and IBM Cloud container service. (See this note for explanations). Using docker image helps us to move workload to Kubernetes cluster running on-premise as presented in the second configuration. The communication between the two environments is done using IBM Secure Gateway service or can be done using private VPN. 1. Configuration 2 : covers the deployment of most of the solution components on to IBM Cloud Private, the IBM extension to Kubernetes. The BFF micro service is calling IBM cloud cognitive services running on IBM Cloud and on-premise backend services: The light blue area represents on-premise servers, while the green area represents the IBM Cloud Private, kubernetes cluster. See detail in Deployment to IBM Cloud Private article","title":"Build, Deploy and Run"},{"location":"buildrun/#prerequisites","text":"You need your own github.com account You need a git client code. For example for Windows and for Mac Install npm and nodejs . Normally getting nodejs last stable version will bring npm too. You need to have some knowledge on using virtual machine images and product like VMWare vSphere. As we are migrating most of the workload to IBM Cloud Private, we delivered, for each component, dockerfiles and helm charts to deploy on Kubernetes.","title":"Prerequisites"},{"location":"buildrun/#get-application-source-code","text":"Clone this base repository using git client: git clone https://github.com/ibm-cloud-architecture/refarch-integration.git Then under the refarch-integration/scripts folder use the command clonePeers.sh to clone the peer repositories of the 'hybrid integration' solution.","title":"Get application source code"},{"location":"buildrun/#working-on-your-own","text":"The script fork-repos.sh should help you to fork all the repositories of this solution within your github account.","title":"Working on your own"},{"location":"buildrun/#continuous-integration-and-deployment","text":"We are detailing how to install a 'jenkins' server on ICP and delivering jenkins file and scripts for each project to do CI/CD. See this note for installation detail.","title":"Continuous integration and deployment"},{"location":"buildrun/#specifics-deployment","text":"Each project covers in detail how to build and run their own components. For the Case Inc Portal app see the deployment note. For API Connect , installed on-premise, is used as API gateway to the different API run times. The IBM Integration Bus deployment , is used to do interfaces mapping between the SOAP data access layer, implemented in Java, and the RESTful API exposed to the public applications, and other mediation or orchestration flow. The Data Access Layer is a JAXWS application running on WebSphere Liberty server and exposing a set of SOAP services. The server is BrownLibertyAppServer . See this repository for detail. The inventory database is running on DB2 and is not directly accessed from API connect, but applying SOA principles, it is accessed via a Data Access Layer app. The server is BrownDB2*. To run the backend solution, we will deliver VM images for you to install on your servers... stay tuned, from now we are describing how each server is configured in each of the specific github repository. We are using VmWare vSphere product to manage all the virtual machines. The figure below presents the Brown Resource Pool with the current servers:","title":"Specifics deployment"},{"location":"buildrun/#configuration-1-physical-deployment","text":"The Configuration 1 Physical deployment includes six servers, and each installation description is done in each git hub repository so you can replicate the configuration if you want to. It should take you one hour per server. DB2 server read this note Liberty App server read this article IBM Integration Bus see this article . API Connect see Server config Open LDAP Server running on the utility server LDAP Configuration Utility Server runs IBM Secure Gateway and Jenkins server As an alternate and easier approach we are delivering a Vagrant file to combine Liberty, DB2 servers in one image: the explanation on how to use it is here","title":"Configuration 1 Physical Deployment"},{"location":"buildrun/#run-on-premise-servers","text":"There are multiple steps to make the solution working. Be sure to start each sever in the following order: Start DB2 server Start App server Start IIB Start API Connect servers: Gateway, Management and Portal Start Utility server Start 'case inc' portal APP The testing project implements a set of test cases to validate each of the component of this n-tier architecture. It is possible to validate each component work independently. The demonstration script instructions are here For demonstration purpose not all back end servers are set in high availability.","title":"Run on premise servers"},{"location":"buildrun/#run-on-ibm-cloud-private","text":"Most of the components of this solution can run on IBM Cloud Private we are detailing it here","title":"Run on IBM Cloud Private"},{"location":"buildrun/#run-on-ibm-cloud-container-service","text":"See this detail note here to deploy and run the Web App as container inside the IBM Cloud Container Service .","title":"Run on IBM Cloud Container Service"},{"location":"buildrun/#run-on-ibm-cloud-cloud-foundry","text":"See this detail note here to deploy the Web App as cloud foundry app on IBM Cloud","title":"Run on IBM Cloud Cloud Foundry"},{"location":"buildrun/run-bmx-cf/","text":"Run on WebApp on IBM Cloud as Cloud Foundry App You need to have a IBM Cloud account, and know how to use cloud foundry command line interface and the container CLI to push to IBM Cloud, the containized web application used to demonstrate the solution. Add a new space: 1. Click on the IBM Cloud account in the top right corner of the web interface. 2. Click Create a new space. 3. Enter a name like \"ra-integration\" for the space name and complete the wizard steps. Step 1: Install the different CLI needed It includes bluemix, cf, and kubernetes. A script exists in the scripts folder to automate the CLI installation: ./scripts/install_cli.sh Step 2: Connect to IBM Cloud via CLI Get you IBM Cloud API end points where your account belong to. For IBM Cloud see the region end point in this documentation For US South region the API is cf login -a https://api.ng.bluemix.net Enter userid, password, organization and space. You are ready to push your application. Step 3: Push the app using CF To push the application as defined in the Manifest. The only application to push is the web app. cd refarch-caseinc-app * Edit the manifest.yml file to specify the hostname of the server as it has to be unique: The URL is based on those parameter. So once the application is deployed the URL will be http://caseincapp.mybluemix.net applications : - path : . name : refarch-caseinc-app host : caseincapp instances : 1 domain : mybluemix.net memory : 256M disk_quota : 1024M services : - ITSupportConversation The cloud foundry command is push: cf push","title":"Run on WebApp on IBM Cloud as Cloud Foundry App"},{"location":"buildrun/run-bmx-cf/#run-on-webapp-on-ibm-cloud-as-cloud-foundry-app","text":"You need to have a IBM Cloud account, and know how to use cloud foundry command line interface and the container CLI to push to IBM Cloud, the containized web application used to demonstrate the solution. Add a new space: 1. Click on the IBM Cloud account in the top right corner of the web interface. 2. Click Create a new space. 3. Enter a name like \"ra-integration\" for the space name and complete the wizard steps.","title":"Run on WebApp on IBM Cloud as Cloud Foundry App"},{"location":"buildrun/run-bmx-cf/#step-1-install-the-different-cli-needed","text":"It includes bluemix, cf, and kubernetes. A script exists in the scripts folder to automate the CLI installation: ./scripts/install_cli.sh","title":"Step 1: Install the different CLI needed"},{"location":"buildrun/run-bmx-cf/#step-2-connect-to-ibm-cloud-via-cli","text":"Get you IBM Cloud API end points where your account belong to. For IBM Cloud see the region end point in this documentation For US South region the API is cf login -a https://api.ng.bluemix.net Enter userid, password, organization and space. You are ready to push your application.","title":"Step 2: Connect to IBM Cloud via CLI"},{"location":"buildrun/run-bmx-cf/#step-3-push-the-app-using-cf","text":"To push the application as defined in the Manifest. The only application to push is the web app. cd refarch-caseinc-app * Edit the manifest.yml file to specify the hostname of the server as it has to be unique: The URL is based on those parameter. So once the application is deployed the URL will be http://caseincapp.mybluemix.net applications : - path : . name : refarch-caseinc-app host : caseincapp instances : 1 domain : mybluemix.net memory : 256M disk_quota : 1024M services : - ITSupportConversation The cloud foundry command is push: cf push","title":"Step 3: Push the app using CF"},{"location":"buildrun/run-bmx-cs/","text":"Run on IBM Cloud Container Service The kubernetes cluster is optional as the Case Inc Portal app can run in a docker container or as a cloud foundry application. We still encourage to use Kubernetes to deploy microservice as it offers a lot of added values we need. You may have the CLI for bluemix container installed via the script: ./install_cli.sh Step1: Provision Kubernetes Cluster on IBM IBM Cloud In the IBM Cloud Catalog go to Containers on the left navigation panel and select Kubernetes Cluster : Select the Lite configuration for your development environment. For production use the Standard configuration. Once created, it can take sometime to get all the related infrastructure provisioned, you should be able to see the dashboard. One Worker node is running. So if you want to deploy to Kubernetes you need to do the following instructions: $ bx login $ bx cs init --host https://us-south.containers.bluemix.net The following table summarized the command you may need | Command | Description | Comment| |---|---|---| | bx cs clusters | list the clusters from your org | return the name, ID, state... | | | Command to set the context of your interaction with the cluster | |---|---|---| The web application is dockerized and detail on the deployment is done in the related repository .","title":"Run on IBM Cloud Container Service"},{"location":"buildrun/run-bmx-cs/#run-on-ibm-cloud-container-service","text":"The kubernetes cluster is optional as the Case Inc Portal app can run in a docker container or as a cloud foundry application. We still encourage to use Kubernetes to deploy microservice as it offers a lot of added values we need. You may have the CLI for bluemix container installed via the script: ./install_cli.sh","title":"Run on IBM Cloud Container Service"},{"location":"buildrun/run-bmx-cs/#step1-provision-kubernetes-cluster-on-ibm-ibm-cloud","text":"In the IBM Cloud Catalog go to Containers on the left navigation panel and select Kubernetes Cluster : Select the Lite configuration for your development environment. For production use the Standard configuration. Once created, it can take sometime to get all the related infrastructure provisioned, you should be able to see the dashboard. One Worker node is running. So if you want to deploy to Kubernetes you need to do the following instructions: $ bx login $ bx cs init --host https://us-south.containers.bluemix.net The following table summarized the command you may need | Command | Description | Comment| |---|---|---| | bx cs clusters | list the clusters from your org | return the name, ID, state... | | | Command to set the context of your interaction with the cluster | |---|---|---| The web application is dockerized and detail on the deployment is done in the related repository .","title":"Step1: Provision Kubernetes Cluster on IBM IBM Cloud"},{"location":"csmo/","text":"Hybrid Service Management -- UNDER CONSTRUCTION We are using a dedicated set of servers to support service management. You can read the following articles: https://www.ibm.com/blogs/bluemix/2018/01/dashboards-for-ibm-cloud-private/ https://www.ibm.com/cloud/garage/content/architecture/serviceManagementArchitecture/2_0 We will detail in close future all the configuration, settings to monitor brown compute. In this section we are detailing how to manage the deployed components of the hybrid solution, mixing ICP workloads and on-premise applications. For information about the service management reference architecture, read this We are using a dedicated set of servers to support service management for the different ICP platforms we are using in the different deployment environments. You can read the following articles: https://www.ibm.com/blogs/bluemix/2018/01/dashboards-for-ibm-cloud-private/ https://www.ibm.com/cloud/garage/content/architecture/serviceManagementArchitecture/2_0 Application Performance Management and IBM Cloud Private: https://www.ibm.com/support/knowledgecenter/SSHLNR_8.1.4/com.ibm.pm.doc/install/integ_cloudprivate_intro.htm https://www.ibm.com/blogs/bluemix/2017/12/monitoring-ibm-cloud-applications/ Operating System monitoring Agents From on-premise to ICP workloads Liberty on-premise Watson conversation monitoring Secure gateway monitoring Agile service management Tracing and logging In this section we are addressing the how to trace a user interaction from chatbot conversation to service implemented in as message flow, microservice, transaction and Database.","title":"CSMO"},{"location":"csmo/#hybrid-service-management","text":"-- UNDER CONSTRUCTION We are using a dedicated set of servers to support service management. You can read the following articles: https://www.ibm.com/blogs/bluemix/2018/01/dashboards-for-ibm-cloud-private/ https://www.ibm.com/cloud/garage/content/architecture/serviceManagementArchitecture/2_0 We will detail in close future all the configuration, settings to monitor brown compute. In this section we are detailing how to manage the deployed components of the hybrid solution, mixing ICP workloads and on-premise applications. For information about the service management reference architecture, read this We are using a dedicated set of servers to support service management for the different ICP platforms we are using in the different deployment environments. You can read the following articles: https://www.ibm.com/blogs/bluemix/2018/01/dashboards-for-ibm-cloud-private/ https://www.ibm.com/cloud/garage/content/architecture/serviceManagementArchitecture/2_0 Application Performance Management and IBM Cloud Private: https://www.ibm.com/support/knowledgecenter/SSHLNR_8.1.4/com.ibm.pm.doc/install/integ_cloudprivate_intro.htm https://www.ibm.com/blogs/bluemix/2017/12/monitoring-ibm-cloud-applications/","title":"Hybrid Service Management"},{"location":"csmo/#operating-system-monitoring","text":"","title":"Operating System monitoring"},{"location":"csmo/#agents","text":"From on-premise to ICP workloads","title":"Agents"},{"location":"csmo/#liberty-on-premise","text":"","title":"Liberty on-premise"},{"location":"csmo/#watson-conversation-monitoring","text":"","title":"Watson conversation monitoring"},{"location":"csmo/#secure-gateway-monitoring","text":"","title":"Secure gateway monitoring"},{"location":"csmo/#agile-service-management","text":"","title":"Agile service management"},{"location":"csmo/#tracing-and-logging","text":"In this section we are addressing the how to trace a user interaction from chatbot conversation to service implemented in as message flow, microservice, transaction and Database.","title":"Tracing and logging"},{"location":"devops/","text":"DevOps With all the components involved in this solution we want to enable CI/CD to deploy artifacts to on-premise servers and to the BM Cloud Private kubernetes cluster. Continuous integration with Jenkins For continuous integration and deployment we are using a IBM Cloud Private with Jenkins release deployed on it. With Jenkins we can do continuous build, execute non regression tests, integration tests and deployment of each components to different target servers. Table of content Installation Jenkins on IBM Cloud Private (ICP) Helm Add Public Helm Repository Create a Persistence Volume Claim Install Jenkins Helm Chart Accessing Jenkins Retrieve the Jenkins Admin Password Configure Docker Repository Optional: Create Service Account Optional: Updating Jenkins Plugins Jenkins on-premise server Get the jenkins binary and install For java you need jdk install docker on the build server Add dependencies for node / angular... Pipeline Creating Pipeline Projects build Installation There are multiple ways to install Jenkins server: using a dedicated VM server by installing the binary or running as docker container, or deploy the jenkins helm release to IBM Cloud Private. Jenkins on IBM Cloud Private (ICP) NOTE: ICP v2.1.0.2 was used at the time of writing this section. See also this jenkins pipeline tutorial . The Jenkins architecture while deployed to kubernetes cluster looks like the diagram below: The installation process deploys a Jenkins master responsible to execute build jobs. Each build jobs is in fact a slave pod , so another docker images that executes a jenkins file as defined in the pipeline definition. As application built with this approach are docker containers and helm releases, there is a need to use a docker registry, could be public or private to ICP. To support dynamic pod creation, there is a jenkins plugin for kubernetes that needs to be installed. This plugin helps to define specific elements in the Jenkins file to execute the build using docker containers. The installation should install the jenkins master as a helm release, configure some parameters to access private docker repository, and add necessary jenkins plugins. We detail those steps: Helm Chart In order to install the Jenkins Chart, you need to have the Helm client configured in your CLI. To install and configure helm, follow the IBM Cloud Private instructions listed here . Add Public Helm Repository Add the public kubernetes repositories, using the ICP admin console > Manage > Helm Repositories and use the URL: https://github.com/kubernetes/charts/tree/master/stable Create a Persistence Volume Claim You can skip this section if you already setup a default storage class for dynamic provisioning , as explained here . If you don't have dynamic provisioning setup, we recommend you read this document , which goes into detail on how to create a Persistent Volume Claim (PVC) , which we can pass on to the helm install command for Jenkins installation. For this example, the PVC has the following requirements: Needs to be at least 8GB It should be called jenkins-home Install Jenkins Helm Chart To Install the Jenkins Chart and Provision a PVC dynamically , use the following command: $ helm install --namespace browncompute --name jenkins --version 0 .14.4 \\ --set Master.ImageTag = 2 .117 \\ --set Master.ServiceType = NodePort \\ --set Master.InstallPlugins.0 = kubernetes:1.5.2 \\ --set Master.InstallPlugins.1 = workflow-aggregator:2.5 \\ --set Master.InstallPlugins.2 = workflow-job:2.19 \\ --set Master.InstallPlugins.3 = credentials-binding:1.16 \\ --set Master.InstallPlugins.4 = git:3.8.0 \\ --set Agent.ImageTag = 3 .19-1 \\ --set rbac.install = true \\ stable/jenkins --tls ; To Install the Jenkins Chart and Pass an Existing PVC , use the following command: $ helm install --namespace browncompute --name jenkins --version 0 .14.4 \\ --set Master.ImageTag = 2 .117 \\ --set Master.ServiceType = NodePort \\ --set Master.InstallPlugins.0 = kubernetes:1.5.2 \\ --set Master.InstallPlugins.1 = workflow-aggregator:2.5 \\ --set Master.InstallPlugins.2 = workflow-job:2.19 \\ --set Master.InstallPlugins.3 = credentials-binding:1.16 \\ --set Master.InstallPlugins.4 = git:3.8.0 \\ --set Persistence.ExistingClaim = jenkins-home \\ --set Agent.ImageTag = 3 .19-1 \\ --set rbac.install = true \\ stable/jenkins --tls ; To Install the Jenkins Chart without a PVC , use the following command: $ helm install --namespace browncompute --name jenkins --version 0 .14.4 \\ --set Master.ImageTag = 2 .117 \\ --set Master.ServiceType = NodePort \\ --set Master.InstallPlugins.0 = kubernetes:1.5.2 \\ --set Master.InstallPlugins.1 = workflow-aggregator:2.5 \\ --set Master.InstallPlugins.2 = workflow-job:2.19 \\ --set Master.InstallPlugins.3 = credentials-binding:1.16 \\ --set Master.InstallPlugins.4 = git:3.8.0 \\ --set Persistence.Enabled = false \\ --set Agent.ImageTag = 3 .19-1 \\ --set rbac.install = true \\ stable/jenkins --tls ; Once the pod is started, the service exposes a NodePort as you can see in the figure below: Accessing Jenkins The helm install command should have printed out instructions to get the Jenkins URL similar to this: NOTES: ... 2. Get the Jenkins URL to visit by running these commands in the same shell: export NODE_PORT=$(kubectl get --namespace browncompute -o jsonpath=\"{.spec.ports[0].nodePort}\" services jenkins) export NODE_IP=$(kubectl get nodes --namespace browncompute -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT/login Now open a new browser window and enter the URL from above to access Jenkins UI. Retrieve the Jenkins Admin Password The helm install command should have printed out instructions to get the Jenkins Admin Password similar to this:: NOTES: 1 . Get your 'admin' user password by running: printf $( kubectl get secret --namespace browncompute jenkins -o jsonpath = \"{.data.jenkins-admin-password}\" | base64 --decode ) ; echo As an alternate solution, try to remote connect to the pods and then navigating to the /var/jenkins_home/secrets path to see the password persisted in a file named initialAdminPassword : $ kubectl exec -it jenkins-jenkins-7995cd796f-btdjz /bin/bash $ cd /var/jenkins_home/secrets $ cat initialAdminPassword Configure Docker Repository In order for the pipelines to know where to push newly built docker images, you need to configure what docker registry to use. To do so, you need to create 2 files: A ConfigMap that specifies the registry location. + In this case, we are going to use ICP's internal docker registry. A Secret that has the registry login credentials. + In this case, the credentials are the ones used to log into ICP's console. To create the docker registry configmap, feel free to enter the registry location in the registry field in the registry/configmap.yaml file, then create the ConfigMap as follows: $ kubectl --namespace default browncompute -f registry/configmap.yaml For the docker registry secret, we need to base64-encode the registry's username and password. The registry/secret.yaml file contains base64-encoded credentials for username admin and password admin . Assuming your credentials are different, you need to replace those values with your base64-encoded credentials and save the file. You can use the online encoder at: https://www.base64encode.org/. To create the Secret, use the following command: $ kubectl --namespace default browncompute -f registry/secret.yaml You now have a fully Configured Jenkins Server inside your ICP cluster! Optional: Create Service Account If using a non-default namespace, or if Jenkins is unable to create slave pods, you may need to tell Jenkins to use the service account in the namespace in order to create new pods. To do so, follow these instructions: Create a Jenkins credential of type Kubernetes service account with service account name provided in the helm status output. Under configure Jenkins -- Update the credentials config in the cloud section to use the service account credential you created in the step above. Optional: Updating Jenkins Plugins Though we specified the versions of the Jenkins plugin needed to run pipelines in the helm install command, if new plugin updates come out Jenkins may ask you to upgrade the plugins in order to keep pipelines running. To do so, go to the Plugin Manager in http://JENKINS_IP:PORT/pluginManager/ menu and update the plugins. Jenkins on-premise server The installation is following a non-docker install approach as described here , specially the following steps should be done: Get the jenkins binary and install $ wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add - $ sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' $ sudo apt-get update $ sudo apt-get install jenkins doing a ps -ef | grep jenkins we can see jenkins server is up and running as a java program. Pointing a web browser to http://localhost:8080 goes to the administration wizard to complete the configuration. You need to get the created password using: sudo cat /var/lib/jenkins/secrets/initialAdminPassword . The wizard is also asking for a user, so we used admin/admin01 and to install the standard plugins. For java you need jdk As some of the codes of the solution are Java based, we add the JDK (to get javac tool and other system jars): $ sudo apt-get install openjdk-8-jdk To automate interaction script like ssh and scp , we are using expect tool so password and other input can be injected to any command $ sudo apt-get install expect Install docker on the build server As some of the build steps are to run docker build , we need to install docker on build server and authorize jenkins to be a docker user: $ sudo usermod -aG docker Jenkins # if jenkins is running restart the server using the web browser to ( jenkins_url ) /restart Add dependencies for node / angular... Finally you need to be sure that any dependencies for each project are met. Like for the nodes and angularjs, be sure to have the compatible angular/cli, npm, nodejs (version 6). The following set of commands were used: $ npm cache clean -f # install nodejs 6 $ curl -sL https://deb.nodesource.com/sertup_6.x | sudo -E bash - $ sudo apt-get install -y nodejs $ sudo mpm i -g @angular/cli Pipeline Pipelines are made up of multiple steps that allow you to build, test and deploy applications. Creating Pipeline Once the Jenkins server is started we need to create a pipeline. To setup a pipeline, open Jenkins in the browser (example of ICP URL: http://172.16.40.223:32277/) Note: Recall that the Jenkins URL is defined in the NodePort of the Jenkins master deployment configuration: $ kubectl get services -l component = jenkins-jenkins-master -n browncompute NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE jenkins NodePort 10 .10.10.4 <none> 8080 :32277/TCP 15d and follow the steps below: 1. Create a Sample Job 2. Select Pipeline Type 3. Setup the pipeline configuration Then select the Pipeline configuration. This should bring a page with multiple tabs. The following options were selected: General: + Discard old builds after 2 days and 2 maximum builds + Do not allow concurrent builds as there are some dependencies between projects Build Triggers + Select 'GitHub hook Trigger..' Pipeline + Pipeline script from SCM: as each project has a Jenkinsfile* + SCM is git and then specify the project to build. - http//github.com/ibm-cloud-architecture/refarch-caseinc-app. OR: - http//github.com/ibm-cloud-architecture/refarch-integration-inventory-dal/ Run the Pipeline To run the pipeline, open Jenkins in the browser and follow the steps below: 1. Launch Pipeline Build 2. Open Pipeline Console Output 3. Monitor Console Output If the pipeline finishes successfully, then Congratulations! You have successfully setup a fully working CICD pipeline. You should get the results like: Projects build Each project has its build process that we try to homogenize with the same jenkinsfile approach. Web app build Inventory Data access layer CI/CD Customer Microservice CI/CD Mediation flow on IIB API inventory product Integration tests","title":"Apply CI/CD for integration solution"},{"location":"devops/#devops","text":"With all the components involved in this solution we want to enable CI/CD to deploy artifacts to on-premise servers and to the BM Cloud Private kubernetes cluster.","title":"DevOps"},{"location":"devops/#continuous-integration-with-jenkins","text":"For continuous integration and deployment we are using a IBM Cloud Private with Jenkins release deployed on it. With Jenkins we can do continuous build, execute non regression tests, integration tests and deployment of each components to different target servers.","title":"Continuous integration with Jenkins"},{"location":"devops/#table-of-content","text":"Installation Jenkins on IBM Cloud Private (ICP) Helm Add Public Helm Repository Create a Persistence Volume Claim Install Jenkins Helm Chart Accessing Jenkins Retrieve the Jenkins Admin Password Configure Docker Repository Optional: Create Service Account Optional: Updating Jenkins Plugins Jenkins on-premise server Get the jenkins binary and install For java you need jdk install docker on the build server Add dependencies for node / angular... Pipeline Creating Pipeline Projects build","title":"Table of content"},{"location":"devops/#installation","text":"There are multiple ways to install Jenkins server: using a dedicated VM server by installing the binary or running as docker container, or deploy the jenkins helm release to IBM Cloud Private.","title":"Installation"},{"location":"devops/#jenkins-on-ibm-cloud-private-icp","text":"NOTE: ICP v2.1.0.2 was used at the time of writing this section. See also this jenkins pipeline tutorial . The Jenkins architecture while deployed to kubernetes cluster looks like the diagram below: The installation process deploys a Jenkins master responsible to execute build jobs. Each build jobs is in fact a slave pod , so another docker images that executes a jenkins file as defined in the pipeline definition. As application built with this approach are docker containers and helm releases, there is a need to use a docker registry, could be public or private to ICP. To support dynamic pod creation, there is a jenkins plugin for kubernetes that needs to be installed. This plugin helps to define specific elements in the Jenkins file to execute the build using docker containers. The installation should install the jenkins master as a helm release, configure some parameters to access private docker repository, and add necessary jenkins plugins. We detail those steps:","title":"Jenkins on IBM Cloud Private (ICP)"},{"location":"devops/#helm-chart","text":"In order to install the Jenkins Chart, you need to have the Helm client configured in your CLI. To install and configure helm, follow the IBM Cloud Private instructions listed here .","title":"Helm Chart"},{"location":"devops/#add-public-helm-repository","text":"Add the public kubernetes repositories, using the ICP admin console > Manage > Helm Repositories and use the URL: https://github.com/kubernetes/charts/tree/master/stable","title":"Add Public Helm Repository"},{"location":"devops/#create-a-persistence-volume-claim","text":"You can skip this section if you already setup a default storage class for dynamic provisioning , as explained here . If you don't have dynamic provisioning setup, we recommend you read this document , which goes into detail on how to create a Persistent Volume Claim (PVC) , which we can pass on to the helm install command for Jenkins installation. For this example, the PVC has the following requirements: Needs to be at least 8GB It should be called jenkins-home","title":"Create a Persistence Volume Claim"},{"location":"devops/#install-jenkins-helm-chart","text":"To Install the Jenkins Chart and Provision a PVC dynamically , use the following command: $ helm install --namespace browncompute --name jenkins --version 0 .14.4 \\ --set Master.ImageTag = 2 .117 \\ --set Master.ServiceType = NodePort \\ --set Master.InstallPlugins.0 = kubernetes:1.5.2 \\ --set Master.InstallPlugins.1 = workflow-aggregator:2.5 \\ --set Master.InstallPlugins.2 = workflow-job:2.19 \\ --set Master.InstallPlugins.3 = credentials-binding:1.16 \\ --set Master.InstallPlugins.4 = git:3.8.0 \\ --set Agent.ImageTag = 3 .19-1 \\ --set rbac.install = true \\ stable/jenkins --tls ; To Install the Jenkins Chart and Pass an Existing PVC , use the following command: $ helm install --namespace browncompute --name jenkins --version 0 .14.4 \\ --set Master.ImageTag = 2 .117 \\ --set Master.ServiceType = NodePort \\ --set Master.InstallPlugins.0 = kubernetes:1.5.2 \\ --set Master.InstallPlugins.1 = workflow-aggregator:2.5 \\ --set Master.InstallPlugins.2 = workflow-job:2.19 \\ --set Master.InstallPlugins.3 = credentials-binding:1.16 \\ --set Master.InstallPlugins.4 = git:3.8.0 \\ --set Persistence.ExistingClaim = jenkins-home \\ --set Agent.ImageTag = 3 .19-1 \\ --set rbac.install = true \\ stable/jenkins --tls ; To Install the Jenkins Chart without a PVC , use the following command: $ helm install --namespace browncompute --name jenkins --version 0 .14.4 \\ --set Master.ImageTag = 2 .117 \\ --set Master.ServiceType = NodePort \\ --set Master.InstallPlugins.0 = kubernetes:1.5.2 \\ --set Master.InstallPlugins.1 = workflow-aggregator:2.5 \\ --set Master.InstallPlugins.2 = workflow-job:2.19 \\ --set Master.InstallPlugins.3 = credentials-binding:1.16 \\ --set Master.InstallPlugins.4 = git:3.8.0 \\ --set Persistence.Enabled = false \\ --set Agent.ImageTag = 3 .19-1 \\ --set rbac.install = true \\ stable/jenkins --tls ; Once the pod is started, the service exposes a NodePort as you can see in the figure below:","title":"Install Jenkins Helm Chart"},{"location":"devops/#accessing-jenkins","text":"The helm install command should have printed out instructions to get the Jenkins URL similar to this: NOTES: ... 2. Get the Jenkins URL to visit by running these commands in the same shell: export NODE_PORT=$(kubectl get --namespace browncompute -o jsonpath=\"{.spec.ports[0].nodePort}\" services jenkins) export NODE_IP=$(kubectl get nodes --namespace browncompute -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT/login Now open a new browser window and enter the URL from above to access Jenkins UI.","title":"Accessing Jenkins"},{"location":"devops/#retrieve-the-jenkins-admin-password","text":"The helm install command should have printed out instructions to get the Jenkins Admin Password similar to this:: NOTES: 1 . Get your 'admin' user password by running: printf $( kubectl get secret --namespace browncompute jenkins -o jsonpath = \"{.data.jenkins-admin-password}\" | base64 --decode ) ; echo As an alternate solution, try to remote connect to the pods and then navigating to the /var/jenkins_home/secrets path to see the password persisted in a file named initialAdminPassword : $ kubectl exec -it jenkins-jenkins-7995cd796f-btdjz /bin/bash $ cd /var/jenkins_home/secrets $ cat initialAdminPassword","title":"Retrieve the Jenkins Admin Password"},{"location":"devops/#configure-docker-repository","text":"In order for the pipelines to know where to push newly built docker images, you need to configure what docker registry to use. To do so, you need to create 2 files: A ConfigMap that specifies the registry location. + In this case, we are going to use ICP's internal docker registry. A Secret that has the registry login credentials. + In this case, the credentials are the ones used to log into ICP's console. To create the docker registry configmap, feel free to enter the registry location in the registry field in the registry/configmap.yaml file, then create the ConfigMap as follows: $ kubectl --namespace default browncompute -f registry/configmap.yaml For the docker registry secret, we need to base64-encode the registry's username and password. The registry/secret.yaml file contains base64-encoded credentials for username admin and password admin . Assuming your credentials are different, you need to replace those values with your base64-encoded credentials and save the file. You can use the online encoder at: https://www.base64encode.org/. To create the Secret, use the following command: $ kubectl --namespace default browncompute -f registry/secret.yaml You now have a fully Configured Jenkins Server inside your ICP cluster!","title":"Configure Docker Repository"},{"location":"devops/#optional-create-service-account","text":"If using a non-default namespace, or if Jenkins is unable to create slave pods, you may need to tell Jenkins to use the service account in the namespace in order to create new pods. To do so, follow these instructions: Create a Jenkins credential of type Kubernetes service account with service account name provided in the helm status output. Under configure Jenkins -- Update the credentials config in the cloud section to use the service account credential you created in the step above.","title":"Optional: Create Service Account"},{"location":"devops/#optional-updating-jenkins-plugins","text":"Though we specified the versions of the Jenkins plugin needed to run pipelines in the helm install command, if new plugin updates come out Jenkins may ask you to upgrade the plugins in order to keep pipelines running. To do so, go to the Plugin Manager in http://JENKINS_IP:PORT/pluginManager/ menu and update the plugins.","title":"Optional: Updating Jenkins Plugins"},{"location":"devops/#jenkins-on-premise-server","text":"The installation is following a non-docker install approach as described here , specially the following steps should be done:","title":"Jenkins on-premise server"},{"location":"devops/#get-the-jenkins-binary-and-install","text":"$ wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add - $ sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' $ sudo apt-get update $ sudo apt-get install jenkins doing a ps -ef | grep jenkins we can see jenkins server is up and running as a java program. Pointing a web browser to http://localhost:8080 goes to the administration wizard to complete the configuration. You need to get the created password using: sudo cat /var/lib/jenkins/secrets/initialAdminPassword . The wizard is also asking for a user, so we used admin/admin01 and to install the standard plugins.","title":"Get the jenkins binary and install"},{"location":"devops/#for-java-you-need-jdk","text":"As some of the codes of the solution are Java based, we add the JDK (to get javac tool and other system jars): $ sudo apt-get install openjdk-8-jdk To automate interaction script like ssh and scp , we are using expect tool so password and other input can be injected to any command $ sudo apt-get install expect","title":"For java you need jdk"},{"location":"devops/#install-docker-on-the-build-server","text":"As some of the build steps are to run docker build , we need to install docker on build server and authorize jenkins to be a docker user: $ sudo usermod -aG docker Jenkins # if jenkins is running restart the server using the web browser to ( jenkins_url ) /restart","title":"Install docker on the build server"},{"location":"devops/#add-dependencies-for-node-angular","text":"Finally you need to be sure that any dependencies for each project are met. Like for the nodes and angularjs, be sure to have the compatible angular/cli, npm, nodejs (version 6). The following set of commands were used: $ npm cache clean -f # install nodejs 6 $ curl -sL https://deb.nodesource.com/sertup_6.x | sudo -E bash - $ sudo apt-get install -y nodejs $ sudo mpm i -g @angular/cli","title":"Add dependencies for node / angular..."},{"location":"devops/#pipeline","text":"Pipelines are made up of multiple steps that allow you to build, test and deploy applications.","title":"Pipeline"},{"location":"devops/#creating-pipeline","text":"Once the Jenkins server is started we need to create a pipeline. To setup a pipeline, open Jenkins in the browser (example of ICP URL: http://172.16.40.223:32277/) Note: Recall that the Jenkins URL is defined in the NodePort of the Jenkins master deployment configuration: $ kubectl get services -l component = jenkins-jenkins-master -n browncompute NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE jenkins NodePort 10 .10.10.4 <none> 8080 :32277/TCP 15d and follow the steps below:","title":"Creating Pipeline"},{"location":"devops/#1-create-a-sample-job","text":"","title":"1. Create a Sample Job"},{"location":"devops/#2-select-pipeline-type","text":"","title":"2. Select Pipeline Type"},{"location":"devops/#3-setup-the-pipeline-configuration","text":"Then select the Pipeline configuration. This should bring a page with multiple tabs. The following options were selected: General: + Discard old builds after 2 days and 2 maximum builds + Do not allow concurrent builds as there are some dependencies between projects Build Triggers + Select 'GitHub hook Trigger..' Pipeline + Pipeline script from SCM: as each project has a Jenkinsfile* + SCM is git and then specify the project to build. - http//github.com/ibm-cloud-architecture/refarch-caseinc-app. OR: - http//github.com/ibm-cloud-architecture/refarch-integration-inventory-dal/","title":"3. Setup the pipeline configuration"},{"location":"devops/#run-the-pipeline","text":"To run the pipeline, open Jenkins in the browser and follow the steps below:","title":"Run the Pipeline"},{"location":"devops/#1-launch-pipeline-build","text":"","title":"1. Launch Pipeline Build"},{"location":"devops/#2-open-pipeline-console-output","text":"","title":"2. Open Pipeline Console Output"},{"location":"devops/#3-monitor-console-output","text":"If the pipeline finishes successfully, then Congratulations! You have successfully setup a fully working CICD pipeline. You should get the results like:","title":"3. Monitor Console Output"},{"location":"devops/#projects-build","text":"Each project has its build process that we try to homogenize with the same jenkinsfile approach. Web app build Inventory Data access layer CI/CD Customer Microservice CI/CD Mediation flow on IIB API inventory product Integration tests","title":"Projects build"},{"location":"dsi/","text":"ODM Decision Service Insight Solution Implementation. The entities are: Application has a status and a name Application has one to many services A service has a status and a name. A user is uniquely identified by his email address. When one of the service is degraded the application is degraded. Events: service degraded: with unique service name service is back on line with unique service name * user y is impacted by application x","title":"ODM Decision Service Insight Solution Implementation."},{"location":"dsi/#odm-decision-service-insight-solution-implementation","text":"The entities are: Application has a status and a name Application has one to many services A service has a status and a name. A user is uniquely identified by his email address. When one of the service is degraded the application is degraded. Events: service degraded: with unique service name service is back on line with unique service name * user y is impacted by application x","title":"ODM Decision Service Insight Solution Implementation."},{"location":"icp/","text":"IBM Cloud Private Knowledge Sharing In this section we group assets related to ICP in the CASE organization. Table of Contents Value Proposition Kubernetes personal summary Community Edition installation (for your own development environment) Enterprise Edition Installation Deployment tutorials ISTIO service mesh FAQ CLI command summary Troubleshooting ICP Further Readings Get an exhaustive view of the things you need to read. Value propositions Value proposition for container Just to recall the value of using container for the cognitive application are the following: Docker ensures consistent environments from development to production. Docker containers are configured to maintain all configurations and dependencies internally. Docker containers allows you to commit changes to your Docker images and version control them. It is very easy to rollback to a previous version of your Docker image. This whole process can be tested in a few minutes. Docker is fast, allowing you to quickly make replications and achieve redundancy. Isolation: Docker makes sure each container has its own resources that are isolated from other containers Removing an app/ container is easy and won\u2019t leave any temporary or configuration files on your host OS. Docker ensures that applications that are running on containers are completely segregated and isolated from each other, granting you complete control over traffic flow and management Value proposition for Kubernetes Kubernetes is an open source system for automating the deployment, scaling, and management of containerized apps. high availability 24/7 Deploy new version multiple times a day Standard use of container for apps and business services Allocates resources and tools when applications need them to work Single-tenant Kubernetes clusters with compute, network and storage infrastructure isolation Automatic scaling of apps Use the cluster dashboard to quickly see and manage the health of your cluster, worker nodes, and container deployments. Automatic re-creation of containers in case of failures Value proposition for IBM Cloud Private The goal is to match the power of public cloud platform with the security and control of your firewall. Based on Kubernetes it offers the same benefits of kubernetes and adds more services and integration with on-premise data sources and services. Most of IBM leading middleware products can run on ICP. ICP helps developers and operations team to optimize legacy application with cloud-enabled middleware, open the data center to work with cloud services using hybrid integration, and create new cloud-native applications using devops tools and polyglot programming languages. See the IBM ICP product page Deployments Deploy the hybrid cloud integration ( e.g. 'browncompute') solution Deploying a CASE Portal web app developed with Angular and package with its BFF server side Deploying a Customer churn web app developed with Angular and package with its BFF server side Deploy the data access layer app done with JAXRS and packaged with WebSphere Liberty on ICP Deploy IBM Integration Bus on ICP Deploy API Connect 2018.* on ICP Deploy Operational Decision Management Deploy Kafka on ICP Deploy IBM Event Stream (Based on Kafka) on ICP Deploy Cassandra on ICP Deploy Data Science Experience on ICP Deploy Db2 Warehouse Developer Edition to IBM Cloud Private Deploy Mongo DB on ICP Deploy Asset management microservice deployment Deploy Asset Kafka consumer and injector to Cassandra Deploy Dashboard backend for frontend ICP Further Readings IBM ICP product page Kubernetes tutorial Storage best practice ICP backup","title":"Main summary"},{"location":"icp/#ibm-cloud-private-knowledge-sharing","text":"In this section we group assets related to ICP in the CASE organization.","title":"IBM Cloud Private Knowledge Sharing"},{"location":"icp/#table-of-contents","text":"Value Proposition Kubernetes personal summary Community Edition installation (for your own development environment) Enterprise Edition Installation Deployment tutorials ISTIO service mesh FAQ CLI command summary Troubleshooting ICP Further Readings Get an exhaustive view of the things you need to read.","title":"Table of Contents"},{"location":"icp/#value-propositions","text":"","title":"Value propositions"},{"location":"icp/#value-proposition-for-container","text":"Just to recall the value of using container for the cognitive application are the following: Docker ensures consistent environments from development to production. Docker containers are configured to maintain all configurations and dependencies internally. Docker containers allows you to commit changes to your Docker images and version control them. It is very easy to rollback to a previous version of your Docker image. This whole process can be tested in a few minutes. Docker is fast, allowing you to quickly make replications and achieve redundancy. Isolation: Docker makes sure each container has its own resources that are isolated from other containers Removing an app/ container is easy and won\u2019t leave any temporary or configuration files on your host OS. Docker ensures that applications that are running on containers are completely segregated and isolated from each other, granting you complete control over traffic flow and management","title":"Value proposition for container"},{"location":"icp/#value-proposition-for-kubernetes","text":"Kubernetes is an open source system for automating the deployment, scaling, and management of containerized apps. high availability 24/7 Deploy new version multiple times a day Standard use of container for apps and business services Allocates resources and tools when applications need them to work Single-tenant Kubernetes clusters with compute, network and storage infrastructure isolation Automatic scaling of apps Use the cluster dashboard to quickly see and manage the health of your cluster, worker nodes, and container deployments. Automatic re-creation of containers in case of failures","title":"Value proposition for Kubernetes"},{"location":"icp/#value-proposition-for-ibm-cloud-private","text":"The goal is to match the power of public cloud platform with the security and control of your firewall. Based on Kubernetes it offers the same benefits of kubernetes and adds more services and integration with on-premise data sources and services. Most of IBM leading middleware products can run on ICP. ICP helps developers and operations team to optimize legacy application with cloud-enabled middleware, open the data center to work with cloud services using hybrid integration, and create new cloud-native applications using devops tools and polyglot programming languages. See the IBM ICP product page","title":"Value proposition for IBM Cloud Private"},{"location":"icp/#deployments","text":"Deploy the hybrid cloud integration ( e.g. 'browncompute') solution Deploying a CASE Portal web app developed with Angular and package with its BFF server side Deploying a Customer churn web app developed with Angular and package with its BFF server side Deploy the data access layer app done with JAXRS and packaged with WebSphere Liberty on ICP Deploy IBM Integration Bus on ICP Deploy API Connect 2018.* on ICP Deploy Operational Decision Management Deploy Kafka on ICP Deploy IBM Event Stream (Based on Kafka) on ICP Deploy Cassandra on ICP Deploy Data Science Experience on ICP Deploy Db2 Warehouse Developer Edition to IBM Cloud Private Deploy Mongo DB on ICP Deploy Asset management microservice deployment Deploy Asset Kafka consumer and injector to Cassandra Deploy Dashboard backend for frontend","title":"Deployments"},{"location":"icp/#icp-further-readings","text":"IBM ICP product page Kubernetes tutorial Storage best practice ICP backup","title":"ICP Further Readings"},{"location":"icp/build-helm-rep/","text":"Build your own helm repository for ICP Github can be used to persist the helm charts and then reference it as source for the ICP catalog. We are presenting in this note how we did it. Cloud Native chart The webapp in the 'case' portal is packaged as helm charts . The commands used were: $ helm init casewebportal Then modified the values.yaml and template to reflect the expected outcomes, and all were described in this note . The command build the chart named $ helm package casewebportal It should create a zip file with the content of the casewebportal folder. You will use the zip file if you want to get the package visible inside the Catalog. You can copy the .tgz file to a folder to be used as repository. In this solution we use the charts folder in this project. Build a chart repository A chart repository is an HTTP server that houses an index.yaml file and optionally some packaged charts. A chart repository consists of packaged charts and a special file called index.yaml which contains an index of all of the charts in the repository. So first you need a index.yaml file which declares the app you want to be visible in the ICP catalog and second reference the repository from your ICP install. index.yaml The structure may look like this. It can be created once you have copy all the charts to the same folder using the commands helm index apiVersion : v1 entries : casewebportal : - apiVersion : v1 created : 2017-10-23T17:25:09.954656969-07:00 description : Webapp used as portal to demonstrate hybrid integration digest : 2a8f11e4163c4c496280fc5441facf85648e0bcbee200345b7b8cae78d294ec2 name : casewebportal home : https://github.com/ibm-cloud-architecture/refarch-caseinc-app keywords : - Hybrid - IBM - browncompute maintainers : - email : boyerje@us.ibm.com name : Jerome Boyer name : ibm-case-brown-web tillerVersion : '>=2.4.0' urls : - https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-integration/charts/casewebportal-0.0.3.tgz version : 1.0.3 Reference the repository You can do that using the web interface of the ICP admin console: in the upper-left corner, click the menu and expand the Admin section. Click Repositories to specify a new Helm chart repository: and add the repository using the url to the docs/charts folder As an alternate using the following command will have the same result $ kubectl repo add ibmcase-hybrid https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-integration/master/docs/charts Once done accessing the catalog and searching for caseweb you should see the application in the catalog: You can then install it from the catalog using the configure button or use the command: $ helm install --name casewebapp ibmcase-hybrid/casewebportal","title":"Build your own helm repository"},{"location":"icp/build-helm-rep/#build-your-own-helm-repository-for-icp","text":"Github can be used to persist the helm charts and then reference it as source for the ICP catalog. We are presenting in this note how we did it.","title":"Build your own helm repository for ICP"},{"location":"icp/build-helm-rep/#cloud-native-chart","text":"The webapp in the 'case' portal is packaged as helm charts . The commands used were: $ helm init casewebportal Then modified the values.yaml and template to reflect the expected outcomes, and all were described in this note . The command build the chart named $ helm package casewebportal It should create a zip file with the content of the casewebportal folder. You will use the zip file if you want to get the package visible inside the Catalog. You can copy the .tgz file to a folder to be used as repository. In this solution we use the charts folder in this project.","title":"Cloud Native chart"},{"location":"icp/build-helm-rep/#build-a-chart-repository","text":"A chart repository is an HTTP server that houses an index.yaml file and optionally some packaged charts. A chart repository consists of packaged charts and a special file called index.yaml which contains an index of all of the charts in the repository. So first you need a index.yaml file which declares the app you want to be visible in the ICP catalog and second reference the repository from your ICP install.","title":"Build a chart repository"},{"location":"icp/build-helm-rep/#indexyaml","text":"The structure may look like this. It can be created once you have copy all the charts to the same folder using the commands helm index apiVersion : v1 entries : casewebportal : - apiVersion : v1 created : 2017-10-23T17:25:09.954656969-07:00 description : Webapp used as portal to demonstrate hybrid integration digest : 2a8f11e4163c4c496280fc5441facf85648e0bcbee200345b7b8cae78d294ec2 name : casewebportal home : https://github.com/ibm-cloud-architecture/refarch-caseinc-app keywords : - Hybrid - IBM - browncompute maintainers : - email : boyerje@us.ibm.com name : Jerome Boyer name : ibm-case-brown-web tillerVersion : '>=2.4.0' urls : - https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-integration/charts/casewebportal-0.0.3.tgz version : 1.0.3","title":"index.yaml"},{"location":"icp/build-helm-rep/#reference-the-repository","text":"You can do that using the web interface of the ICP admin console: in the upper-left corner, click the menu and expand the Admin section. Click Repositories to specify a new Helm chart repository: and add the repository using the url to the docs/charts folder As an alternate using the following command will have the same result $ kubectl repo add ibmcase-hybrid https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-integration/master/docs/charts Once done accessing the catalog and searching for caseweb you should see the application in the catalog: You can then install it from the catalog using the configure button or use the command: $ helm install --name casewebapp ibmcase-hybrid/casewebportal","title":"Reference the repository"},{"location":"icp/dev-env-install/","text":"Install a development environment with IBM Cloud Private 2.1 For staging purpose, we are using a ICP EE deployment in a five virtual machine cluster. In this note we are presenting a single virtual machine configuration based on Ubuntu 64 bits v16.10 and how to install ICP 2.1.0.2 on it. Updated 04/23/2018 The developer environment may look like the following diagram, for a developer on Mac and a VM ubuntu image (Windows will look similar) There is a nice alternate solution to get an ICP single VM up and running in fe minutes by using a Vagrant file: Clone this github and do a vagrant up , 15 minutes later you have your environment with one proxy-master node and 3 worker nodes. Excellent! If you need to access the dockerhub IBM public image, use docker hub explorer and search for ibmcom Preparing your guest machine Install ubuntu Follow your VM player instruction to create a virtual machine and access an ubuntu 16.10 .iso file The expected minimum resource are: CPUs: 2 Memory: 16GB Disk: 100GB (Thin Provisioned) Install ubuntu following the step by step wizard, create a user with admin privilege. The ICP installation need root access and ssh connection to the different host nodes that are part of the topology. In the case of one VM you still need to do some security settings and make the system be passwordless with security key. Login as the newly create user Change root user password $ sudo su - $ passwd Update the ubuntu OS with last package references apt-get update Install Linux image extra packages apt-get install -y linux-image-extra-$(uname -r) linux-image-extra-virtual Install python $ apt-get install -y python-setuptools $ easy_install pip $ pip install docker-py Disable firewall if enabled $ ufw status $ sudo ufw disable Boot and log as root user Install NTP to keep time sync apt-get install -y ntp sytemctl restart ntp # test it ntpq -p Configure ssh for remote access Install openssh , and authorize remote access sudo apt-get install openssh-server systemctl restart ssh Create ssh keys for your user and authorize ssh # create rsa keys with no passphrase $ ssh-keygen -b 4096 -t rsa -P '' be sure the following parameters are enabled $ vi /etc/ssh/sshd_config PermitRootLogin yes PubkeyAuthentication yes PasswordAuthentication yes Then restart ssh daemon: $ systemctl restart ssh Copy the public key to the root user .ssh folder $ ssh-copy-id -i .ssh/id_rsa root@ubuntu Then you should be able to ssh root user to the guest machine. Install docker The developer's machine and VM need both to have docker or at least the VM needs it. To access the cluster environment you need kubectl * command line interface and hosts configuration to match the configuration defined during the ICP install. If you do not have docker install on your development machine, we will not describe it again ;-). See docker download . Install docker on the ubuntu machine Install docker repository $ apt-get install -y apt-transport-https ca-certificates curl software-properties-common Get the GPG key $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - apt-key fingerprint 0EBFCD88 Setup docker stable repository $ add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb \\_ release -cs ) stable\u201d $ apt-get update $ apt-get install -y docker-ce Validate it runs docker run hello-world Add user to docker group # Verify docker group is defined $ cat /etc/group # add user $ usermod -G docker -a usermod # re-login the user to get the group assignment at the session level When using a build server, you also need docker to be able to build the different docker images of our solution. The figure below illustrates what need to be done: A Jenkins server implements different pipeline to pull the different project from github, executes each jenkins file to build the different elements: compiled code, docker image, helm charts, and then push images and helm charts to ICP. Install Kubectl You need to have kubectl on your development computer and on any build server. * Install kubectl from IBM image. docker run -e LICENSE=accept --net=host -v /usr/local/bin:/data ibmcom/kubernetes:v1.7.3 cp /kubectl /data the --net=host means to use the host network stack and not the one coming from the container. -v is for mounting volume: the local /usr/local/bin is mounted to the /data in the container, so the command to cp /kubectl directory inside the container, to /data inside the container will in fact modify the host /usr/local/bin with kubectl CLI. (complex for a simple tar -xvf... but this is our new life...) see Docker volume notes Verify kubectl runs locally. We will get the cluster information to remote connect to it once ICP is installed. Install helm IBM Cloud Private contains integration with Helm that allows you to install the application and all of its components in a few steps. This can be done as an administrator using the following steps: 1. Click on the user icon on the top right corner and then click on Configure client . 2. Copy the displayed kubectl configuration, paste it in your terminal, and press Enter on your keyboard. 3. Initialize helm in your cluster. Use these instructions to install and initialize helm . Install IBM Cloud Private CE on the ubuntu machine Verify the public docker hub images available: go to docker hub explorer and search for ibmcom/icp-inception Get the ICP installer docker image using the following command ``` $ sudo su - $ docker pull ibmcom/icp-inception:2.1.0 Digest: sha256:f6513... Status: Downloaded newer image for ibmcom/icp-inception:2.1.0 $ $ mkdir /opt/ibm-cloud-private-ce-2.1.0 $ cd /opt/ibm-cloud-private-ce-2.1.0 The following command extracts configuration file under the *cluster* folder by mounting local folder to /data inside the container: $ docker run -e LICENSE=accept \\ -v \"$(pwd)\":/data ibmcom/icp-inception:2.1.0 cp -r cluster /data ``` In the cluster folder there are multiple files to modify: config.yaml, hosts, and ssh-keys hosts: As we run in a single VM, the master, proxy and worker node will have the same IP address. So get your VM's IP address using the command: $ ip address Modify the hosts file [master] 172.16.251.133 [worker] 172.16.251.133 [proxy] 172.16.251.133 Modify the config.yaml file using administrator privilege (so sudo) by specifying a domain name and cluster name. Also set the loopback dns flag so the server will run in single VM without error. loopback_dns : true cluster_name : mycluster cluster_domain : mycluster . domain Copy security keys you have created when configuring your linux environment to the ssh_key file $ cp ~/.ssh/id_rsa ./ssh_key Deploy the environment now # from the cluster folder $ sudo docker run -e LICENSE=accept --net=host --rm -t -v \"$(pwd)\":/installer/cluster ibmcom/icp-inception:2.1.0 install Verify access to ICP console using http://ipaddress:8443 admin/admin You should see the dashboard as in figure below: Image Repositories To deploy application you need to package as docker image and push them within a docker repository, and then define kubernetes deployment configuration using Helm charts and install those charts to a helm Tiller server. Docker repository You have two choices: using the private image repository deployed in ICP or create one private docker image repository somewhere and declare it inside ICP. Access to ICP private docker repository You need the public ssh keys of the master-node: * connect to the VM where the master node runs, get the ip address, and the ca.crt with commands like below: # on client machine $ cd /etc/docker # create a directory that matches the name of the cluster as defined in the config.yaml of ICP. 8500 is the port number. $ mkdir certs.d/myclyster.icp:8500 $ cd certs.d/myclyster.icp:8500 $ scp root@masternodevmipaddress:/etc/docker/certs.d/mycluster.icp:8500/ca.crts . So you copied the public key. An administrator could have sent it to you too. Add an entry for mycluster.icp in your /etc/hosts 172.16.5.xxx master.cfc On your computer you need to restart systemd and docker systemctl daemon-reload service ssh restart service docker restart Normally you should be able to login to remote docker with a userid known to the master node VM: admin is the default user. docker login mycluster.icp:8500 User: admin Password: Define a remote helm repository ICP supports referencing remote helm charts repository, but it also includes internal repository as illustrated in following figure: The hybrid integration set of projects has each component creating its own helm packaging (tgz files) and persisted into the current project charts folder. An index.yaml file defines the components part of this repository. Connect kubectl to remote cluster-master Access the ICP kubernetes cluster information from the ICP Console. From the Client configuration menu under your userid on the top right of the main console panel to access the configuration. Then copy and paste in a script or in a terminal to execute those commands: kubectl cluster-info returns the cluster information within ICP. Kubernetes master is running at https://192.168.27.100:8001 catalog-ui is running at https://192.168.27.100:8001/api/v1/proxy/namespaces/kube-system/services/catalog-ui Heapster is running at https://192.168.27.100:8001/api/v1/proxy/namespaces/kube-system/services/heapster image-manager is running at https://192.168.27.100:8001/api/v1/proxy/namespaces/kube-system/services/image-manager KubeDNS is running at https://192.168.27.100:8001/api/v1/proxy/namespaces/kube-system/services/kube-dns platform-ui is running at https://192.168.27.100:8001/api/v1/proxy/namespaces/kube-system/services/platform-ui To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Verify Helm is connected to Tiller server running on ICP If you get the kubectl connected to ICP cluster (as presented in previous figure), then the following command should give you the version of the Tiller server running in ICP. $ helm version If you have configured kubeclt to connect to the ICP master server, the following command should give you the version of the Tiller server running in ICP. Client: &version.Version{SemVer:\"v2.5.0\", GitCommit:\"012cb0ac1a1b2f888144ef5a67b8dab6c2d45be6\", GitTreeState:\"clean\"} Server: &version.Version{SemVer:\"v2.5.0\", GitCommit:\"012cb0ac1a1b2f888144ef5a67b8dab6c2d45be6\", GitTreeState:\"clean\"}","title":"Install a dev cluster"},{"location":"icp/dev-env-install/#install-a-development-environment-with-ibm-cloud-private-21","text":"For staging purpose, we are using a ICP EE deployment in a five virtual machine cluster. In this note we are presenting a single virtual machine configuration based on Ubuntu 64 bits v16.10 and how to install ICP 2.1.0.2 on it. Updated 04/23/2018 The developer environment may look like the following diagram, for a developer on Mac and a VM ubuntu image (Windows will look similar) There is a nice alternate solution to get an ICP single VM up and running in fe minutes by using a Vagrant file: Clone this github and do a vagrant up , 15 minutes later you have your environment with one proxy-master node and 3 worker nodes. Excellent! If you need to access the dockerhub IBM public image, use docker hub explorer and search for ibmcom","title":"Install a development environment with IBM Cloud Private 2.1"},{"location":"icp/dev-env-install/#preparing-your-guest-machine","text":"","title":"Preparing your guest machine"},{"location":"icp/dev-env-install/#install-ubuntu","text":"Follow your VM player instruction to create a virtual machine and access an ubuntu 16.10 .iso file The expected minimum resource are: CPUs: 2 Memory: 16GB Disk: 100GB (Thin Provisioned) Install ubuntu following the step by step wizard, create a user with admin privilege. The ICP installation need root access and ssh connection to the different host nodes that are part of the topology. In the case of one VM you still need to do some security settings and make the system be passwordless with security key. Login as the newly create user Change root user password $ sudo su - $ passwd Update the ubuntu OS with last package references apt-get update Install Linux image extra packages apt-get install -y linux-image-extra-$(uname -r) linux-image-extra-virtual Install python $ apt-get install -y python-setuptools $ easy_install pip $ pip install docker-py Disable firewall if enabled $ ufw status $ sudo ufw disable Boot and log as root user Install NTP to keep time sync apt-get install -y ntp sytemctl restart ntp # test it ntpq -p","title":"Install ubuntu"},{"location":"icp/dev-env-install/#configure-ssh-for-remote-access","text":"Install openssh , and authorize remote access sudo apt-get install openssh-server systemctl restart ssh Create ssh keys for your user and authorize ssh # create rsa keys with no passphrase $ ssh-keygen -b 4096 -t rsa -P '' be sure the following parameters are enabled $ vi /etc/ssh/sshd_config PermitRootLogin yes PubkeyAuthentication yes PasswordAuthentication yes Then restart ssh daemon: $ systemctl restart ssh Copy the public key to the root user .ssh folder $ ssh-copy-id -i .ssh/id_rsa root@ubuntu Then you should be able to ssh root user to the guest machine.","title":"Configure ssh for remote access"},{"location":"icp/dev-env-install/#install-docker","text":"The developer's machine and VM need both to have docker or at least the VM needs it. To access the cluster environment you need kubectl * command line interface and hosts configuration to match the configuration defined during the ICP install. If you do not have docker install on your development machine, we will not describe it again ;-). See docker download . Install docker on the ubuntu machine Install docker repository $ apt-get install -y apt-transport-https ca-certificates curl software-properties-common Get the GPG key $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - apt-key fingerprint 0EBFCD88 Setup docker stable repository $ add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $( lsb \\_ release -cs ) stable\u201d $ apt-get update $ apt-get install -y docker-ce Validate it runs docker run hello-world Add user to docker group # Verify docker group is defined $ cat /etc/group # add user $ usermod -G docker -a usermod # re-login the user to get the group assignment at the session level When using a build server, you also need docker to be able to build the different docker images of our solution. The figure below illustrates what need to be done: A Jenkins server implements different pipeline to pull the different project from github, executes each jenkins file to build the different elements: compiled code, docker image, helm charts, and then push images and helm charts to ICP.","title":"Install docker"},{"location":"icp/dev-env-install/#install-kubectl","text":"You need to have kubectl on your development computer and on any build server. * Install kubectl from IBM image. docker run -e LICENSE=accept --net=host -v /usr/local/bin:/data ibmcom/kubernetes:v1.7.3 cp /kubectl /data the --net=host means to use the host network stack and not the one coming from the container. -v is for mounting volume: the local /usr/local/bin is mounted to the /data in the container, so the command to cp /kubectl directory inside the container, to /data inside the container will in fact modify the host /usr/local/bin with kubectl CLI. (complex for a simple tar -xvf... but this is our new life...) see Docker volume notes Verify kubectl runs locally. We will get the cluster information to remote connect to it once ICP is installed.","title":"Install Kubectl"},{"location":"icp/dev-env-install/#install-helm","text":"IBM Cloud Private contains integration with Helm that allows you to install the application and all of its components in a few steps. This can be done as an administrator using the following steps: 1. Click on the user icon on the top right corner and then click on Configure client . 2. Copy the displayed kubectl configuration, paste it in your terminal, and press Enter on your keyboard. 3. Initialize helm in your cluster. Use these instructions to install and initialize helm .","title":"Install helm"},{"location":"icp/dev-env-install/#install-ibm-cloud-private-ce-on-the-ubuntu-machine","text":"Verify the public docker hub images available: go to docker hub explorer and search for ibmcom/icp-inception Get the ICP installer docker image using the following command ``` $ sudo su - $ docker pull ibmcom/icp-inception:2.1.0 Digest: sha256:f6513... Status: Downloaded newer image for ibmcom/icp-inception:2.1.0 $ $ mkdir /opt/ibm-cloud-private-ce-2.1.0 $ cd /opt/ibm-cloud-private-ce-2.1.0 The following command extracts configuration file under the *cluster* folder by mounting local folder to /data inside the container: $ docker run -e LICENSE=accept \\ -v \"$(pwd)\":/data ibmcom/icp-inception:2.1.0 cp -r cluster /data ``` In the cluster folder there are multiple files to modify: config.yaml, hosts, and ssh-keys hosts: As we run in a single VM, the master, proxy and worker node will have the same IP address. So get your VM's IP address using the command: $ ip address Modify the hosts file [master] 172.16.251.133 [worker] 172.16.251.133 [proxy] 172.16.251.133 Modify the config.yaml file using administrator privilege (so sudo) by specifying a domain name and cluster name. Also set the loopback dns flag so the server will run in single VM without error. loopback_dns : true cluster_name : mycluster cluster_domain : mycluster . domain Copy security keys you have created when configuring your linux environment to the ssh_key file $ cp ~/.ssh/id_rsa ./ssh_key Deploy the environment now # from the cluster folder $ sudo docker run -e LICENSE=accept --net=host --rm -t -v \"$(pwd)\":/installer/cluster ibmcom/icp-inception:2.1.0 install Verify access to ICP console using http://ipaddress:8443 admin/admin You should see the dashboard as in figure below:","title":"Install IBM Cloud Private CE on the ubuntu machine"},{"location":"icp/dev-env-install/#image-repositories","text":"To deploy application you need to package as docker image and push them within a docker repository, and then define kubernetes deployment configuration using Helm charts and install those charts to a helm Tiller server.","title":"Image Repositories"},{"location":"icp/dev-env-install/#docker-repository","text":"You have two choices: using the private image repository deployed in ICP or create one private docker image repository somewhere and declare it inside ICP.","title":"Docker repository"},{"location":"icp/dev-env-install/#access-to-icp-private-docker-repository","text":"You need the public ssh keys of the master-node: * connect to the VM where the master node runs, get the ip address, and the ca.crt with commands like below: # on client machine $ cd /etc/docker # create a directory that matches the name of the cluster as defined in the config.yaml of ICP. 8500 is the port number. $ mkdir certs.d/myclyster.icp:8500 $ cd certs.d/myclyster.icp:8500 $ scp root@masternodevmipaddress:/etc/docker/certs.d/mycluster.icp:8500/ca.crts . So you copied the public key. An administrator could have sent it to you too. Add an entry for mycluster.icp in your /etc/hosts 172.16.5.xxx master.cfc On your computer you need to restart systemd and docker systemctl daemon-reload service ssh restart service docker restart Normally you should be able to login to remote docker with a userid known to the master node VM: admin is the default user. docker login mycluster.icp:8500 User: admin Password:","title":"Access to ICP private docker repository"},{"location":"icp/dev-env-install/#define-a-remote-helm-repository","text":"ICP supports referencing remote helm charts repository, but it also includes internal repository as illustrated in following figure: The hybrid integration set of projects has each component creating its own helm packaging (tgz files) and persisted into the current project charts folder. An index.yaml file defines the components part of this repository.","title":"Define a remote helm repository"},{"location":"icp/dev-env-install/#connect-kubectl-to-remote-cluster-master","text":"Access the ICP kubernetes cluster information from the ICP Console. From the Client configuration menu under your userid on the top right of the main console panel to access the configuration. Then copy and paste in a script or in a terminal to execute those commands: kubectl cluster-info returns the cluster information within ICP. Kubernetes master is running at https://192.168.27.100:8001 catalog-ui is running at https://192.168.27.100:8001/api/v1/proxy/namespaces/kube-system/services/catalog-ui Heapster is running at https://192.168.27.100:8001/api/v1/proxy/namespaces/kube-system/services/heapster image-manager is running at https://192.168.27.100:8001/api/v1/proxy/namespaces/kube-system/services/image-manager KubeDNS is running at https://192.168.27.100:8001/api/v1/proxy/namespaces/kube-system/services/kube-dns platform-ui is running at https://192.168.27.100:8001/api/v1/proxy/namespaces/kube-system/services/platform-ui To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.","title":"Connect kubectl to remote cluster-master"},{"location":"icp/dev-env-install/#verify-helm-is-connected-to-tiller-server-running-on-icp","text":"If you get the kubectl connected to ICP cluster (as presented in previous figure), then the following command should give you the version of the Tiller server running in ICP. $ helm version If you have configured kubeclt to connect to the ICP master server, the following command should give you the version of the Tiller server running in ICP. Client: &version.Version{SemVer:\"v2.5.0\", GitCommit:\"012cb0ac1a1b2f888144ef5a67b8dab6c2d45be6\", GitTreeState:\"clean\"} Server: &version.Version{SemVer:\"v2.5.0\", GitCommit:\"012cb0ac1a1b2f888144ef5a67b8dab6c2d45be6\", GitTreeState:\"clean\"}","title":"Verify Helm is connected to Tiller server running on ICP"},{"location":"icp/faq/","text":"ICP Frequent Asked Questions","title":"FAQ"},{"location":"icp/faq/#icp-frequent-asked-questions","text":"","title":"ICP Frequent Asked Questions"},{"location":"icp/icp-cli/","text":"ICP or Bluemix Container Service CLI Summary IBM Cloud (Bluemix CLI) $ bx login -a api.ng.bluemix.net # Review the locations that are available. $ bx cs locations # Choose a location and review the machine type $ bx cs machine-types dal10 # Assess if a public and private VLAN already exists in the Bluemix Infrastructure NEED a paid account $ bx cs vlans dal10 # When the provisioning of your cluster is completed, the status of your cluster changes to deployed $ bx cs clusters # Check the status of the worker nodes $ bx cs workers cyancomputecluster Install CLIs https://www.ibm.com/support/knowledgecenter/SSBS6K_2.1.0.2/manage_cluster/scripts/install_cli.html Summary of CLI commands Login to your cluster bx pr login -a https://<master_ip_address>:8443 --skip-ssl-validation -a <accountname> Assess cluster name and status bx pr clusters Configure the cluster to get cert.pem and key.pem certificates added to ~/.helm folder: bx pr cluster-config ext-demo Work on deployments Kubectl official cheatsheet ICP CLI formal doc kubectl playground # Get cluster context kubectl config set-cluster gr33n-cluster --server=https://gr33n-cluster:8001 --insecure-skip-tls-verify=true kubectl config set-cluster gr33n-cluster --server=https://gr33n-cluster:8001 --insecure-skip-tls-verify=true kubectl config set-context gr33n-cluster --cluster=gr33n-cluster --user=admin --insecure-skip-tls-verify=true kubectl config set-credentials admin --client-certificate=gr33n-cluster/cert.pem --client-key=gr33n-cluster/key.pem kubectl config use-context gr33n-cluster # get all deployments kubectl get deployments --all-namespaces # get specifics deployments kubectl get deployment jenkins -n browncompute # Add a deployment from a yaml file kubectl create -f deployment.yaml # get pod details kubectl get pods -l component=jenkins-jenkins-master -n browncompute See a specific config map kubectl get configMap <name-of-the-map> --namespace browncompute $ kubectl get pods $ kubectl describe pod podid $ export POD_NAME = $( kubectl get pods -o go-template = '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}\u2019) $ kubectl exec $POD_NAME env --namespace browncompute $ kubectl logs $POD_NAME # run an alpine shell connected to the container $ kubectl exec -ti $POD_NAME /bin/ash > ls $ kubectl get services $ export NODE_PORT=$(kubectl get services/casewdsbroker -o go-template=' {{( index .spec.ports 0 ) .nodePort }} ' ) $ kubectl describe deployment # Apply change to existing pod $ kubectl apply -f filename.yml # Access to a pod using node port: example for cassandra pod. $ k port-forward cassandra-0 9042 :9042 helm CLI # create a new helm chart: helm create <chartname> # Install a charts on a connected ICP helm install browncompute-dal/ --name browncompute-dal --namespace browncompute --tls # delete an existing release helm del --purge browncompute-dal --tls The --purge flag makes sure that the browncompute-dal release name is reusable for a fresh install if you decide to use the same release name again.","title":"CLI summary"},{"location":"icp/icp-cli/#icp-or-bluemix-container-service-cli-summary","text":"","title":"ICP or Bluemix Container Service CLI Summary"},{"location":"icp/icp-cli/#ibm-cloud-bluemix-cli","text":"$ bx login -a api.ng.bluemix.net # Review the locations that are available. $ bx cs locations # Choose a location and review the machine type $ bx cs machine-types dal10 # Assess if a public and private VLAN already exists in the Bluemix Infrastructure NEED a paid account $ bx cs vlans dal10 # When the provisioning of your cluster is completed, the status of your cluster changes to deployed $ bx cs clusters # Check the status of the worker nodes $ bx cs workers cyancomputecluster","title":"IBM Cloud (Bluemix CLI)"},{"location":"icp/icp-cli/#install-clis","text":"https://www.ibm.com/support/knowledgecenter/SSBS6K_2.1.0.2/manage_cluster/scripts/install_cli.html","title":"Install CLIs"},{"location":"icp/icp-cli/#summary-of-cli-commands","text":"Login to your cluster bx pr login -a https://<master_ip_address>:8443 --skip-ssl-validation -a <accountname> Assess cluster name and status bx pr clusters Configure the cluster to get cert.pem and key.pem certificates added to ~/.helm folder: bx pr cluster-config ext-demo","title":"Summary of CLI commands"},{"location":"icp/icp-cli/#work-on-deployments","text":"Kubectl official cheatsheet ICP CLI formal doc kubectl playground # Get cluster context kubectl config set-cluster gr33n-cluster --server=https://gr33n-cluster:8001 --insecure-skip-tls-verify=true kubectl config set-cluster gr33n-cluster --server=https://gr33n-cluster:8001 --insecure-skip-tls-verify=true kubectl config set-context gr33n-cluster --cluster=gr33n-cluster --user=admin --insecure-skip-tls-verify=true kubectl config set-credentials admin --client-certificate=gr33n-cluster/cert.pem --client-key=gr33n-cluster/key.pem kubectl config use-context gr33n-cluster # get all deployments kubectl get deployments --all-namespaces # get specifics deployments kubectl get deployment jenkins -n browncompute # Add a deployment from a yaml file kubectl create -f deployment.yaml # get pod details kubectl get pods -l component=jenkins-jenkins-master -n browncompute See a specific config map kubectl get configMap <name-of-the-map> --namespace browncompute $ kubectl get pods $ kubectl describe pod podid $ export POD_NAME = $( kubectl get pods -o go-template = '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}\u2019) $ kubectl exec $POD_NAME env --namespace browncompute $ kubectl logs $POD_NAME # run an alpine shell connected to the container $ kubectl exec -ti $POD_NAME /bin/ash > ls $ kubectl get services $ export NODE_PORT=$(kubectl get services/casewdsbroker -o go-template=' {{( index .spec.ports 0 ) .nodePort }} ' ) $ kubectl describe deployment # Apply change to existing pod $ kubectl apply -f filename.yml # Access to a pod using node port: example for cassandra pod. $ k port-forward cassandra-0 9042 :9042","title":"Work on deployments"},{"location":"icp/icp-cli/#helm-cli","text":"# create a new helm chart: helm create <chartname> # Install a charts on a connected ICP helm install browncompute-dal/ --name browncompute-dal --namespace browncompute --tls # delete an existing release helm del --purge browncompute-dal --tls The --purge flag makes sure that the browncompute-dal release name is reusable for a fresh install if you decide to use the same release name again.","title":"helm CLI"},{"location":"icp/icp-integration/","text":"Hybrid Integration on IBM Cloud Private Deployment In this section we are presenting how Hybrid integration solution implementation is deployed to IBM Cloud Private. We address different configurations as business and operation requirements may differ per data center and even per business applications. Each configuration describes how some of the components of the solution may be deployed to ICP or kept on-premise servers. Prerequisites The following points should be considered before going into more detail of the ICP deployment: A conceptual understanding of how Kubernetes works, see also personal summary here A high-level understanding of Helm and Kubernetes package management . A basic understanding of IBM Cloud Private cluster architecture . Understand the different ICP environment and sizing Understand image management : Access to an operational IBM Cloud Private cluster see installation note for the different approaches you could use. As a developer, you need to have the following components: Docker installed on user laptop Kubectl installed on user laptop Helm we have provided shell script to do those installation. Execute ../scripts/install_cli.sh ( or ./scripts/install_cli.bat for Windows) Add Helm Charts Repository * Add a browncompute namespace using ICP admin console, under Manage > Namespaces menu. We will use this namespace to deploy the hybrid integration components into ICP cluster. As an alternate you can use the command: kubectl create namespace browncompute . Deployment steps Helm charts are defined one time and should stay reasonably stable over time. Docker image change at each build. Remember that docker images that are added to the image registry are owned by namespaces. All the users within a namespace are owners of the images. An owner can remove or update the images from the cluster management console. Super administrators have full access to all images in the cluster. Owners can also update the scope of an image Here are the common steps to perform when deploying a component of the hybrid integration solution. Install Helm Chart + Option 1: Clone the Repo & Install the Chart + Option 2: Install from Helm Chart Repository + Option 3: Install from Helm Chart Repository using ICP Helm Charts Catalog Validate Helm Chart * Setup a CICD Pipeline for each project Update docker images You need to name the images with the owner, so the namespace used. Here are example of images scoped: $ kubectl get images -n browncompute NAME AGE browncompute-inventory-dal 1d customerms 10m and kubectl get images -n default NAME AGE nginx 29d And the command to push one of the image: docker push ext-demo.icp:8500/browncompute/customerms:latest Install Helm Chart We created Helm Charts for each project. For example the browncompute-inventory-dal chart packages all of the kubernetes resources required to deploy the browncompute-inventory-dal app and expose it to a public endpoint. For more in-depth details of the inner-workings of Helm Charts, please refer to the Helm Chart Documentation to learn more about charts. You have 3 options to install the chart: 1. Clone the Repo and Install the Chart. 2. Install the chart from our Helm Chart Repository , which is served here directly from GitHub. 3. Install the chart from our Helm Chart Repository using ICP's Charts Catalog. Option 1: Clone the Repo & Install the Chart To clone the repo & install the browncompute-inventory-dal chart from source, run the following commands: # Clone the repo $ git clone https://github.com/ibm-cloud-architecture/refarch-integration-inventory-dal.git # Change to repo directory $ cd refarch-integration-inventory-dal # Install the Chart $ helm install chart/browncompute-inventory-dal --name browncompute-dal --tls Option 2: Install from Helm Chart Repository As an example for the Data Access Layer project, we created a Helm Chart Repository (located here ) where we serve a packaged version of the browncompute-inventory-dal so that you can conveniently install it in your ICP Cluster. To install the chart from the Helm Chart Repository , run the following commands: # Add Local Reference to Helm Chart Repository $ helm repo add browncompute https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-integration-inventory-dal/master/docs/charts # Install the Chart $ helm install browncompute/browncompute-inventory-dal --name browncompute-dal --tls Option 3: Install from Helm Chart Repository using ICP Helm Charts Catalog Coming Soon Validate Helm Chart If you installed the chart successfuly, you should see a CLI output similar to the following: NAME : browncompute - dal LAST DEPLOYED : Tue Nov 14 22 : 23 : 39 2017 NAMESPACE : browncompute STATUS : DEPLOYED RESOURCES : ==> v1 / Service NAME CLUSTER - IP EXTERNAL - IP PORT ( S ) AGE inventorydalsvc 10.101 . 0.176 < none > 9080 /TCP,9443/ TCP 1 s ==> v1beta1 / Deployment NAME DESIRED CURRENT UP - TO - DATE AVAILABLE AGE browncompute - dal - browncompute - dal 2 2 2 0 1 s ==> v1beta1 / Ingress NAME HOSTS ADDRESS PORTS AGE browncompute - dal - browncompute - dal dal . brown . case 80 1 s Configurations As an hybrid solution each component of the solution may run on existing on-premise servers or within IBM Cloud Private cluster. The deployment decision will be driven by the business requirements and the availability of underlying IBM middleware product as docker image and helm chart. For each component of the 'hybrid integration' solution the following needs may be done: build the docker image tag the image with information about the target repository server, namespace, tag and version push the image to the remote docker repository (most likely the one inside ICP master node) build the helm package from the chart definition install the chart to ICP cluster using helm install or upgrade command line interface access the URL end point Configuration 1: ICP deployment and SOA services on-premise Our target deployment is illustrated in the figure below: The light blue area represents on-premise servers, while the green area represents the IBM Cloud Private, kubernetes cluster. On the left side, the Portal cloud native webapp was first developed as Angular / nodejs app, using cloud foundry deployment model. The new approach was to externalize the configuration outside of cloud foundry and package the application as docker container. This approach helps to deploy the application on any environment in a control manner. For the web app deployment follow the step by step tutorial . API Gateway on ICP Inventory data access layer on ICP Decision for product recommendation with Operational Decision Management on ICP The second Datapower gateway is used to present 'System' APIs. (see this redbook \"A practical Guide for IBM Hybrid Integration Platform\" for detail about this clear APIs separation) The gateway flow , deployed on IIB, is doing the REST to SOAP interface mapping: this configuration illustrates deep adoption of the ESB pattern leveraging existing high end deployments, scaling both horizontally and vertically. In this model the concept of operation for mediation and integration logic development and deployment are kept. If you want to review each on-premise component, their descriptions are below: API Connect - Inventory product Gateway flow in integration broker SOAP service for data access Layer running on premise Inventory database on DB2 server Configuration 2: Only Cloud native application on ICP This is the simplest deployment where only the cloud native web application ( the 'case' portal ) is deployed. It still accesses the back end services via API Connect running on-premise. All other components run on-premise. The figure below illustrates this deployment: Configuration 3: API runtime on ICP The goal for this configuration is to deploy Data power gateway to IBM cloud private and deploy the interaction API products on it. The API product definition is split into interaction APIs and system APIs. The steps are: 1. Modify the webapp configuration to use a different URL for the gateway flow: The settings is done in the values.yaml in the chart folder of the case portal app 1. Deploy webapp to ICP following the tutorial as seen in configuration 1 1. Deploy API Connect gateway from ICP Catalog using this tutorial 1. Deploy the api product to the new gateway. Configuration 4: Integration Bus as micro flow running in ICP This configuration is using integration components on premise and the other more lightweight components on ICP, and add a micro-service for integration as introduced in this article using a message flow deployed on IIB runninig in ICP. This approach leverages existing investment and IIB concept of operation, and IBM Datapower for security and API gateway. This approach has an impact on the way to manage application inside IIB. Instead of deploying multiple applications inside one instance of IIB, we are packaging the app and IIB runtime into a unique container to deploy in pods and facilitate horizontal scaling. The vertical scaling delivered out of the box in IIB is not leveraged. Deploy webapp to ICP follows this tutorial Deploy Java application running on Liberty read this tutorial ) Deploy IBM Integration Bus read this tutorial Deploy API Connect gateway from ICP Catalog using this tutorial Deploy the api product to the new gateway. Use ICP Catalog All component of the solution we can deploy to ICP are packaged as helm chart, and centralized in the docs/charts folder of this repository. Using the ICP admin console you can get the list of repositories using the Manage > Helm Repositories menu: You can add a new repository, and with the Sync repositories button the Helm catalog is modified with the new images. When you add new application / package you need to update your private catalog index.yaml file. The index.yaml file describes how your applications is listed in the ICP Application Center: $ curl get -k https://9.19.34.107:8443/helm-repo/charts/index.yaml $ helm repo index --merge index.yaml","title":"Integration on ICP"},{"location":"icp/icp-integration/#hybrid-integration-on-ibm-cloud-private-deployment","text":"In this section we are presenting how Hybrid integration solution implementation is deployed to IBM Cloud Private. We address different configurations as business and operation requirements may differ per data center and even per business applications. Each configuration describes how some of the components of the solution may be deployed to ICP or kept on-premise servers.","title":"Hybrid Integration on IBM Cloud Private Deployment"},{"location":"icp/icp-integration/#prerequisites","text":"The following points should be considered before going into more detail of the ICP deployment: A conceptual understanding of how Kubernetes works, see also personal summary here A high-level understanding of Helm and Kubernetes package management . A basic understanding of IBM Cloud Private cluster architecture . Understand the different ICP environment and sizing Understand image management : Access to an operational IBM Cloud Private cluster see installation note for the different approaches you could use. As a developer, you need to have the following components: Docker installed on user laptop Kubectl installed on user laptop Helm we have provided shell script to do those installation. Execute ../scripts/install_cli.sh ( or ./scripts/install_cli.bat for Windows) Add Helm Charts Repository * Add a browncompute namespace using ICP admin console, under Manage > Namespaces menu. We will use this namespace to deploy the hybrid integration components into ICP cluster. As an alternate you can use the command: kubectl create namespace browncompute .","title":"Prerequisites"},{"location":"icp/icp-integration/#deployment-steps","text":"Helm charts are defined one time and should stay reasonably stable over time. Docker image change at each build. Remember that docker images that are added to the image registry are owned by namespaces. All the users within a namespace are owners of the images. An owner can remove or update the images from the cluster management console. Super administrators have full access to all images in the cluster. Owners can also update the scope of an image Here are the common steps to perform when deploying a component of the hybrid integration solution. Install Helm Chart + Option 1: Clone the Repo & Install the Chart + Option 2: Install from Helm Chart Repository + Option 3: Install from Helm Chart Repository using ICP Helm Charts Catalog Validate Helm Chart * Setup a CICD Pipeline for each project","title":"Deployment steps"},{"location":"icp/icp-integration/#update-docker-images","text":"You need to name the images with the owner, so the namespace used. Here are example of images scoped: $ kubectl get images -n browncompute NAME AGE browncompute-inventory-dal 1d customerms 10m and kubectl get images -n default NAME AGE nginx 29d And the command to push one of the image: docker push ext-demo.icp:8500/browncompute/customerms:latest","title":"Update docker images"},{"location":"icp/icp-integration/#install-helm-chart","text":"We created Helm Charts for each project. For example the browncompute-inventory-dal chart packages all of the kubernetes resources required to deploy the browncompute-inventory-dal app and expose it to a public endpoint. For more in-depth details of the inner-workings of Helm Charts, please refer to the Helm Chart Documentation to learn more about charts. You have 3 options to install the chart: 1. Clone the Repo and Install the Chart. 2. Install the chart from our Helm Chart Repository , which is served here directly from GitHub. 3. Install the chart from our Helm Chart Repository using ICP's Charts Catalog.","title":"Install Helm Chart"},{"location":"icp/icp-integration/#option-1-clone-the-repo-install-the-chart","text":"To clone the repo & install the browncompute-inventory-dal chart from source, run the following commands: # Clone the repo $ git clone https://github.com/ibm-cloud-architecture/refarch-integration-inventory-dal.git # Change to repo directory $ cd refarch-integration-inventory-dal # Install the Chart $ helm install chart/browncompute-inventory-dal --name browncompute-dal --tls","title":"Option 1: Clone the Repo &amp; Install the Chart"},{"location":"icp/icp-integration/#option-2-install-from-helm-chart-repository","text":"As an example for the Data Access Layer project, we created a Helm Chart Repository (located here ) where we serve a packaged version of the browncompute-inventory-dal so that you can conveniently install it in your ICP Cluster. To install the chart from the Helm Chart Repository , run the following commands: # Add Local Reference to Helm Chart Repository $ helm repo add browncompute https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-integration-inventory-dal/master/docs/charts # Install the Chart $ helm install browncompute/browncompute-inventory-dal --name browncompute-dal --tls","title":"Option 2: Install from Helm Chart Repository"},{"location":"icp/icp-integration/#option-3-install-from-helm-chart-repository-using-icp-helm-charts-catalog","text":"Coming Soon","title":"Option 3: Install from Helm Chart Repository using ICP Helm Charts Catalog"},{"location":"icp/icp-integration/#validate-helm-chart","text":"If you installed the chart successfuly, you should see a CLI output similar to the following: NAME : browncompute - dal LAST DEPLOYED : Tue Nov 14 22 : 23 : 39 2017 NAMESPACE : browncompute STATUS : DEPLOYED RESOURCES : ==> v1 / Service NAME CLUSTER - IP EXTERNAL - IP PORT ( S ) AGE inventorydalsvc 10.101 . 0.176 < none > 9080 /TCP,9443/ TCP 1 s ==> v1beta1 / Deployment NAME DESIRED CURRENT UP - TO - DATE AVAILABLE AGE browncompute - dal - browncompute - dal 2 2 2 0 1 s ==> v1beta1 / Ingress NAME HOSTS ADDRESS PORTS AGE browncompute - dal - browncompute - dal dal . brown . case 80 1 s","title":"Validate Helm Chart"},{"location":"icp/icp-integration/#configurations","text":"As an hybrid solution each component of the solution may run on existing on-premise servers or within IBM Cloud Private cluster. The deployment decision will be driven by the business requirements and the availability of underlying IBM middleware product as docker image and helm chart. For each component of the 'hybrid integration' solution the following needs may be done: build the docker image tag the image with information about the target repository server, namespace, tag and version push the image to the remote docker repository (most likely the one inside ICP master node) build the helm package from the chart definition install the chart to ICP cluster using helm install or upgrade command line interface access the URL end point","title":"Configurations"},{"location":"icp/icp-integration/#configuration-1-icp-deployment-and-soa-services-on-premise","text":"Our target deployment is illustrated in the figure below: The light blue area represents on-premise servers, while the green area represents the IBM Cloud Private, kubernetes cluster. On the left side, the Portal cloud native webapp was first developed as Angular / nodejs app, using cloud foundry deployment model. The new approach was to externalize the configuration outside of cloud foundry and package the application as docker container. This approach helps to deploy the application on any environment in a control manner. For the web app deployment follow the step by step tutorial . API Gateway on ICP Inventory data access layer on ICP Decision for product recommendation with Operational Decision Management on ICP The second Datapower gateway is used to present 'System' APIs. (see this redbook \"A practical Guide for IBM Hybrid Integration Platform\" for detail about this clear APIs separation) The gateway flow , deployed on IIB, is doing the REST to SOAP interface mapping: this configuration illustrates deep adoption of the ESB pattern leveraging existing high end deployments, scaling both horizontally and vertically. In this model the concept of operation for mediation and integration logic development and deployment are kept. If you want to review each on-premise component, their descriptions are below: API Connect - Inventory product Gateway flow in integration broker SOAP service for data access Layer running on premise Inventory database on DB2 server","title":"Configuration 1: ICP deployment and SOA services on-premise"},{"location":"icp/icp-integration/#configuration-2-only-cloud-native-application-on-icp","text":"This is the simplest deployment where only the cloud native web application ( the 'case' portal ) is deployed. It still accesses the back end services via API Connect running on-premise. All other components run on-premise. The figure below illustrates this deployment:","title":"Configuration 2: Only Cloud native application on ICP"},{"location":"icp/icp-integration/#configuration-3-api-runtime-on-icp","text":"The goal for this configuration is to deploy Data power gateway to IBM cloud private and deploy the interaction API products on it. The API product definition is split into interaction APIs and system APIs. The steps are: 1. Modify the webapp configuration to use a different URL for the gateway flow: The settings is done in the values.yaml in the chart folder of the case portal app 1. Deploy webapp to ICP following the tutorial as seen in configuration 1 1. Deploy API Connect gateway from ICP Catalog using this tutorial 1. Deploy the api product to the new gateway.","title":"Configuration 3: API runtime on ICP"},{"location":"icp/icp-integration/#configuration-4-integration-bus-as-micro-flow-running-in-icp","text":"This configuration is using integration components on premise and the other more lightweight components on ICP, and add a micro-service for integration as introduced in this article using a message flow deployed on IIB runninig in ICP. This approach leverages existing investment and IIB concept of operation, and IBM Datapower for security and API gateway. This approach has an impact on the way to manage application inside IIB. Instead of deploying multiple applications inside one instance of IIB, we are packaging the app and IIB runtime into a unique container to deploy in pods and facilitate horizontal scaling. The vertical scaling delivered out of the box in IIB is not leveraged. Deploy webapp to ICP follows this tutorial Deploy Java application running on Liberty read this tutorial ) Deploy IBM Integration Bus read this tutorial Deploy API Connect gateway from ICP Catalog using this tutorial Deploy the api product to the new gateway.","title":"Configuration 4: Integration Bus as micro flow running in ICP"},{"location":"icp/icp-integration/#use-icp-catalog","text":"All component of the solution we can deploy to ICP are packaged as helm chart, and centralized in the docs/charts folder of this repository. Using the ICP admin console you can get the list of repositories using the Manage > Helm Repositories menu: You can add a new repository, and with the Sync repositories button the Helm catalog is modified with the new images. When you add new application / package you need to update your private catalog index.yaml file. The index.yaml file describes how your applications is listed in the ICP Application Center: $ curl get -k https://9.19.34.107:8443/helm-repo/charts/index.yaml $ helm repo index --merge index.yaml","title":"Use ICP Catalog"},{"location":"icp/troubleshooting/","text":"Troubleshouting in ICP This note regroups a set of things we have met during our work on kubernetes and ICP. The Kubernetes official troubleshooting docs Table of contents Installation specifics Access during development Deployment Investigation Installation specifics The installation of the ICP 2.1.0.x is here Error: hostname not resolved fatal: [...] => Failed to connect to the host via ssh: ssh: Could not resolve hostname ...: Name or service not known * verify the hostname match the ip address in /etc/hosts * be sure to start the installation from the folder with the hosts file. It should be cluster or modify $(pwd) to $(pwd)/cluster ssh connect failure fatal: [192.168.1.147] => Failed to connect to the host via ssh: Permission denied (publickey, password). This is a problem of accessing root user during the installation. Be sure to authorize root login, (ssh_config file), that the ssh_key is in the root user home/.ssh. See above While login from a developer's laptop. $ docker login cluster.icp:8500 > Error response from daemon: Get https://cluster.icp:8500/v2/: net/http: request canceled while waiting for connection ( Client.Timeout exceeded while awaiting headers ) Be sure the cluster.icp hostname is mapped to the host's IP address in the local /etc/hosts Issues during upgrade to 2.1.0.1 The following issues are errors that may occur during the upgrade to ICP 2.1.0.1. Etcd Fails to Start During Upgrade While running the Kubernetes upgrade on the master node, Etcd fails to start. This is due to swap being enabled on the OS. Resolution is to disable swap. Error Message TASK [upgrade-master : Waiting for Etcd to start] ****************************** fatal: [172.16.40.130]: FAILED! => {\"changed\": false, \"elapsed\": 600, \"failed\": true, \"msg\": \"The Etcd component failed to start. For more details, see https://ibm.biz/etcd-fails.\"} Problem Determination & Resolution Check Kubelet Logs to determine why node has not started. $ journalctl -u kubelet & > kl.log Open kl.log and check the error. Jan 18 11:42:32 green-icp-proxy hyperkube[31106]: Error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename Disable swap. $ swapoff -a $ sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab Kubelet Install Fails on Nodes During the Kubernetes upgrade step, the installer attempts to install Kubelet on each of the nodes. The installation fails because the ibmcom/kubernetes:v1.8.3-ee image is not on the nodes. Resolution is to put the image on the nodes manually. Error Message TASK [upgrade-kubelet : Ensuring kubelet install dir exists] **************************************************************************************************************************************** ok: [172.16.40.135] FAILED - RETRYING: Copying hyperkube onto operating system (3 retries left). FAILED - RETRYING: Copying hyperkube onto operating system (2 retries left). FAILED - RETRYING: Copying hyperkube onto operating system (1 retries left). TASK [upgrade-kubelet : Copying hyperkube onto operating system] ************************************************************************************************************************************ fatal: [172.16.40.135]: FAILED! => {\"attempts\": 3, \"changed\": true, \"cmd\": \"docker run --rm -v /opt/kubernetes/:/data ibmcom/kubernetes:v1.8.3-ee sh -c 'cp -f /hyperkube /data/'\", \"delta\": \"0:00:00.574937\", \"end\": \"2018-01-18 10:03:52.881064\", \"failed\": true, \"rc\": 125, \"start\": \"2018-01-18 10:03:52.306127\", \"stderr\": \"Unable to find image 'ibmcom/kubernetes:v1.8.3-ee' locally\\ndocker: Error response from daemon: manifest for ibmcom/kubernetes:v1.8.3-ee not found.\\nSee 'docker run --help'.\", \"stderr_lines\": [\"Unable to find image 'ibmcom/kubernetes:v1.8.3-ee' locally\", \"docker: Error response from daemon: manifest for ibmcom/kubernetes:v1.8.3-ee not found.\", \"See 'docker run --help'.\"], \"stdout\": \"\", \"stdout_lines\": []} Problem Determination & Resolution Log onto node and checked the local images. Notice that the ibmcom/kubernetes:v1.8.3-ee image is absent. $ docker image ls Copy ibm-cloud-private-x86_64-2.1.0.1.tar.gz package to the node. Extract the images and load into Docker $ tar xf ibm-cloud-private-x86_64-2.1.0.1.tar.gz -O | sudo docker load Access See official faq on login Unknown certificate authority $ docker login mycluster.icp:8500 Error response from daemon: Get https://mycluster.icp:8500/v2/: x509: certificate signed by unknown authority Go to your docker engine configuration and add the remote registry as an insecure one. On MAC you select the docker > preferences > Daemons> Advanced menu and then add the remote master name { \"debug\" : true , \"experimental\" : true , \"insecure-registries\" : [ \"jbcluster.icp:8500\" , \"mycluster.icp:8500\" , \"cpscluster.icp:8500\" ] } You can also verify the certificates are in the logged user ~/.docker folder. This folder should have a certs.d folder and one folder per remote server, you need to access. So the mycluster.icp:8500/ca.crt file needs to be copied there too. Not able to login to docker repository running on master node Different type of messages: Unknown authority `Error response from daemon: Get https://greencluster.icp:8500/v2/: x509: certificate signed by unknown authority. You need to configure your local docker to accept to connect to insecure registries by adding an entry about the target host. On MACOS the Preferences> Daemon > Advanced See also the note about accessing ICP private repository here and how to copy SSL certificate to your local host. x509 certificate not valid for a specific hostname Be sure the hostname you are using is in your /etc/hosts and you docker login to the good host. Could not connect to a backend service. Try again later. (E0004) While trying to get cluster configuration with command like bx pr cluster-config green2-cluster got this message. The problem may come from a lack of disk space on / on the host OS of the active master node. To add space for the virtual machine with Ubuntu OS, you need to do the following: Using the VM management tool like VMWare vsphere, add a new virtual disk log as root user to host OS, and list the device with ls /dev/sd* . You may have a new sdc or sdb device. We assume sdc for now. * Stop docker and kubelet (it can take some time for docker to stop): ```bash systemctl stop docker systemctl stop kubelet * See existing disks with `fdisk -l` and add a new disk with `fdisk /dev/sdc`. It should add a DOS partition and use the w option to write the changes. * Create different tables for the filesystem using `mkfs.ext4 /dev/sdc` * mount the newly created filesystem to a new folder: mkdir /mnt/disk mount /dev/sdc /mnt/disk * Move ICP install file to the new disk : ` mv / opt / ibm / cfc /* /mnt/disk` * unmount but update the boot setting to get the disk back on reboot: ``` umount /mnt/disk vi /etc/fstab /dev/sdc /opt/ibm/cfc ext4 default 1 3 ``` * restart docker and kubelet: ``` systemctl start docker systemctl start kubelet ``` See also [this note](https://kb.vmware.com/s/article/1003940) ## 503 on a deployed app with ingress rule The following message \"503 Service Temporarily Unavailable\" may appear when accessing a pod via virtual hostname defined in Ingress rules. Be sure to understand the ingress role To investigate do the following: * Display the helm release, and verify the Ingress is specified and the hosts is specified, the IP address matches the proxy IP address in the cluster. In the service verify the type is ClusterIP and the ports map the exposed port in the docker image. ![](chart-view.png) * In the ingress verify the service name, the app selector are matching with the deployment parameters using Service > Ingress -> Edit: ![](edit-ingress.png) ## Helm connection Issue: tls: bad certificate For the *Error: remote error: tls: bad certificate*: you need to be logged into the cluster and get the cluster config. The commands are: bx pr login -a https://ext-demo.icp:8443 -u admin --skip-ssl-validation bx pr cluster-config mycluster ## Helm version not able to connect to Tiller. Error: cannot connect to Tiller With version 2.1.0.2, TLS is enforced to communicate with the server. So to get the version the command is `helm version --tls`. You need also to get the certificates for the cluster. The command ` bx pr cluster-config <clustername>` will add those certificate into `~/.helm`. ## Helm incompatible version The error message may look like: `Error: incompatible versions client[v2.9.1] server[v2.7.3+icp]` Use the command to upgrade: `helm init --upgrade` ### For using SSL between Tiller and Helm See [this note from github helm account](https://github.com/helm/helm/blob/master/docs/tiller_ssl.md) # Deployment ## helm install command: User is not authorized to install release Be sure to enter the good namespace name in the install command. ## Pod not getting the image from docker private repository Looking at the Events report from the pod view you got a message like: Failed to pull image \u201cgreencluster.icp:8500/greencompute/customerms:v0.0.7\u201d: rpc error: code = Unknown desc = Error response from daemon: Get https://greencluster.icp:8500/v2/greencompute/customerms/manifests/v0.0.7: unauthorized: authentication required The new version of k8s enforces the use of secret to access the docker private repository. So you need to add a secret, named for example regsecret, for the docker registry object. $ kubectl create secret docker-registry regsecret --docker-server=172.16.40.130 --docker-username=admin --docker-password=<> --docker-email= --namespace=greencompute $ kubectl get secret regsecret --output=yaml --namespace=greencompute Then modify the deployment.yaml to reference this secret so the pod can access the repo during deployment: ``` spec: containers: - name: {{ .Chart.Name }} image: \" {{ .Values.image.repository }} : {{ .Values.image.tag }} \" .... imagePullSecrets: - name: regsecret Verify deployment When you deploy a helm chart you can assess how the deployment went using the ICP admin console or the kubectl CLI. For the user interface, go to the Workloads > Deployments menu to access the list of current deployments. Select the deployment and then the pod list. In the pod view select the events to assess how the pod deployment performed and the log file in Logs menu Using kublectl to get the status of a deployment $ kubectl get deployments --namespace browncompute > NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE casewebportal-casewebportal 1 1 1 1 2d or a specific detail view: kubectl describe deployment browncompute-dal-browncompute-dal Get the logs and events $ export POD_NAME = $( kubectl get pods --namespace browncompute -l \"app=casewebportal-casewebportal\" -o jsonpath = \"{.items[0].metadata.name}\" ) $ kubectl logs $POD_NAME --namespace browncompute $ kubectl get events --namespace browncompute --sort-by = '.metadata.creationTimestamp' Accessing an app expose with Ingress: 503 Service Temporarily Unavailable This could be due to ingress rule configuration issue. We need to assess if the pod is up and running Get the pod name: k get pods --namespace greencompute Verify no issue in the container itself: k logs bc-inventory-dal-browncompute-dal-6bfb4f85b8-tdzjt --namespace greencompute if the service and ingress are well defined Go to the service view or kubectl get svc --namespace greencompute -o wide verify the app-selector and service Type Verify the service definition: k describe svc bc-inventory-dal-browncompute-dal --namespace greencompute * Get the ingress with kubectl describe ingress bc-inventory-dal-browncompute-dal -n greencompute Name : bc - inventory - dal - browncompute - dal Namespace : greencompute Address : 172.16 . 40.131 Default backend : default - http - backend : 80 ( < none > ) Rules : Host Path Backends ---- ---- -------- dal . brown . case / bc - inventory - dal - browncompute - dal : http ( < none > ) Annotations : Events : < none > this configuration looks good, but not... so what is happening? We need to look at the ingress-controller config: Get the list of ingress controllers: kubectl get pods -n kube-system Get the controller logs and verify if it receives the configuration when we deployed the service: kubectl logs nginx-ingress-controller-gvcj5 -n kube-system . Search for message with \"Event ... Kind: Ingress...\" Get NGINX configuration: kubectl exec -it -n kube-system nginx-ingress-controller-gvcj5 cat /etc/nginx/nginx.conf Here is an extract # start server dal.brown.case server { server_name dal.brown.case ; listen 80; listen [::]:80; set $proxy_upstream_name \"-\"; location / { log_by_lua_block { } port_in_redirect off; set $proxy_upstream_name \"\"; set $namespace \"greencompute\"; set $ingress_name \"bc-inventory-dal-browncompute-dal\"; set $service_name \"bc-inventory-dal-browncompute-dal\"; ... # No endpoints available for the request return 503; As we can see the $proxy_upstream_name is not set and the configuration is enforcing returning 503. The ingress rule has an issue... the port is not a port number but a name (\"bc-inventory-dal-browncompute-dal:http\")... So editing the rule and change to port 9080 makes it work: kubectl edit ing -n greencompute bc-inventory-dal-browncompute-dal Error while getting cluster info Try to do kubectl cluster-info : failed: error: you must be logged in to the server (the server has asked for the client to provide credentials): Be sure to have use the settings from the 'configure client'. Be sure the cluster name / IP address are mapped in /etc/hosts Be sure to have a ca.crt into ~/.ssh folder Use the bx pr login -a https://<ipaddress>:8443 -u admin command to login to the cluster Default backend - 404 This error can occur if the ingress rules are not working well. Assess if ingress is well defined: virtual hostname, proxy address and status/age of running ``` kubectl get ing --namespace browncompute NAME HOSTS ADDRESS PORTS AGE browncompute-dal-browncompute-dal dal.brown.case 172.16.40.31 80 59m casewebportal-casewebportal portal.brown.case 172.16.40.31 80 10d ``` Get the detail of ingress rules, and its mapping to the expected service, the path and host mapping. kubectl describe ingress browncompute - dal - browncompute - dal -- namespace browncompute Name : browncompute - dal - browncompute - dal Namespace : browncompute Address : 172.16 . 40.31 Default backend : default - http - backend : 80 ( 10.100 . 221.196 : 8080 ) Rules : Host Path Backends ---- ---- -------- dal . brown . case / inventorydalsvc : 9080 ( < none > ) Annotations : No events . If ingress rules are correct for your release, check to see if there are other releases sharing the same host rule. This can happen if the release was deleted without the ingress rule being removed. kubectl get ing --all-namespaces=true NAMESPACE NAME HOSTS ADDRESS PORTS AGE greencompute greencompute-green-customerapp-green-customerapp greenapp.green.case 172.16.40.131 80 1h greencompute greencustomerapp-green-customerapp greenapp.green.case 172.16.40.131 80 1h Delete the conflicting ingress. kubectl delete ing greencompute-green-customerapp-green-customerapp ingress \"greencompute-green-customerapp-green-customerapp\" deleted ICP Cluster is not accessible via admin console After restart of the ICP master node, the ICP cluster is inaccessible remotely. Log into master node via SSH and check kube-system pods $ kubectl -s 127 .0.0.1:8888 -n kube-system get pods Check if any pods are in a bad state such as CrashLoopBackOff ... k8s-master-172.16.40.130 2/3 CrashLoopBackOff 393 40s ... Check the containers for that pod $ kubectl \u2013s 127 .0.0.1:8888 \u2013n kube-system describe pods k8s-master-172.16.40.130 To view the logs for a specific container, use the following command. For example, the controller-manager in the k8s-master-172.16.40.130 pod: $ kubectl \u2013s 127 .0.0.1:8888 \u2013n kube-system logs k8s-master-172.16.40.130 \u2013p controller-manager Investigation When something is going wrong you can do the following: assess the node status with kubectl get nodes -o wide assess the state of the pods and where they are deployed: kubectl get pods -o wide look at what is deployed within a node: kubectl describe <podname> assess the storage state with kubectl get pv and kubectl get pvc Access logs of a pod: 'kubectl logs ' Exec a shell in the running pod and then use traditional network tools to investigate: kubectl exec -tin <namespace> <podname> sh * For a CrashLoopBackoffs error: CrashLoopBackoff encapsulates a large set of errors that are all hidden behind the same error condition. Some potential debugging steps * kubectl describe pod <podname> * ssh to the host","title":"Troubleshouting"},{"location":"icp/troubleshooting/#troubleshouting-in-icp","text":"This note regroups a set of things we have met during our work on kubernetes and ICP. The Kubernetes official troubleshooting docs","title":"Troubleshouting in ICP"},{"location":"icp/troubleshooting/#table-of-contents","text":"Installation specifics Access during development Deployment Investigation","title":"Table of contents"},{"location":"icp/troubleshooting/#installation-specifics","text":"The installation of the ICP 2.1.0.x is here","title":"Installation specifics"},{"location":"icp/troubleshooting/#error-hostname-not-resolved","text":"fatal: [...] => Failed to connect to the host via ssh: ssh: Could not resolve hostname ...: Name or service not known * verify the hostname match the ip address in /etc/hosts * be sure to start the installation from the folder with the hosts file. It should be cluster or modify $(pwd) to $(pwd)/cluster","title":"Error: hostname not resolved"},{"location":"icp/troubleshooting/#ssh-connect-failure","text":"fatal: [192.168.1.147] => Failed to connect to the host via ssh: Permission denied (publickey, password). This is a problem of accessing root user during the installation. Be sure to authorize root login, (ssh_config file), that the ssh_key is in the root user home/.ssh. See above While login from a developer's laptop. $ docker login cluster.icp:8500 > Error response from daemon: Get https://cluster.icp:8500/v2/: net/http: request canceled while waiting for connection ( Client.Timeout exceeded while awaiting headers ) Be sure the cluster.icp hostname is mapped to the host's IP address in the local /etc/hosts","title":"ssh connect failure"},{"location":"icp/troubleshooting/#issues-during-upgrade-to-2101","text":"The following issues are errors that may occur during the upgrade to ICP 2.1.0.1.","title":"Issues during upgrade to 2.1.0.1"},{"location":"icp/troubleshooting/#etcd-fails-to-start-during-upgrade","text":"While running the Kubernetes upgrade on the master node, Etcd fails to start. This is due to swap being enabled on the OS. Resolution is to disable swap.","title":"Etcd Fails to Start During Upgrade"},{"location":"icp/troubleshooting/#error-message","text":"TASK [upgrade-master : Waiting for Etcd to start] ****************************** fatal: [172.16.40.130]: FAILED! => {\"changed\": false, \"elapsed\": 600, \"failed\": true, \"msg\": \"The Etcd component failed to start. For more details, see https://ibm.biz/etcd-fails.\"}","title":"Error Message"},{"location":"icp/troubleshooting/#problem-determination-resolution","text":"Check Kubelet Logs to determine why node has not started. $ journalctl -u kubelet & > kl.log Open kl.log and check the error. Jan 18 11:42:32 green-icp-proxy hyperkube[31106]: Error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename Disable swap. $ swapoff -a $ sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab","title":"Problem Determination &amp; Resolution"},{"location":"icp/troubleshooting/#kubelet-install-fails-on-nodes","text":"During the Kubernetes upgrade step, the installer attempts to install Kubelet on each of the nodes. The installation fails because the ibmcom/kubernetes:v1.8.3-ee image is not on the nodes. Resolution is to put the image on the nodes manually.","title":"Kubelet Install Fails on Nodes"},{"location":"icp/troubleshooting/#error-message_1","text":"TASK [upgrade-kubelet : Ensuring kubelet install dir exists] **************************************************************************************************************************************** ok: [172.16.40.135] FAILED - RETRYING: Copying hyperkube onto operating system (3 retries left). FAILED - RETRYING: Copying hyperkube onto operating system (2 retries left). FAILED - RETRYING: Copying hyperkube onto operating system (1 retries left). TASK [upgrade-kubelet : Copying hyperkube onto operating system] ************************************************************************************************************************************ fatal: [172.16.40.135]: FAILED! => {\"attempts\": 3, \"changed\": true, \"cmd\": \"docker run --rm -v /opt/kubernetes/:/data ibmcom/kubernetes:v1.8.3-ee sh -c 'cp -f /hyperkube /data/'\", \"delta\": \"0:00:00.574937\", \"end\": \"2018-01-18 10:03:52.881064\", \"failed\": true, \"rc\": 125, \"start\": \"2018-01-18 10:03:52.306127\", \"stderr\": \"Unable to find image 'ibmcom/kubernetes:v1.8.3-ee' locally\\ndocker: Error response from daemon: manifest for ibmcom/kubernetes:v1.8.3-ee not found.\\nSee 'docker run --help'.\", \"stderr_lines\": [\"Unable to find image 'ibmcom/kubernetes:v1.8.3-ee' locally\", \"docker: Error response from daemon: manifest for ibmcom/kubernetes:v1.8.3-ee not found.\", \"See 'docker run --help'.\"], \"stdout\": \"\", \"stdout_lines\": []}","title":"Error Message"},{"location":"icp/troubleshooting/#problem-determination-resolution_1","text":"Log onto node and checked the local images. Notice that the ibmcom/kubernetes:v1.8.3-ee image is absent. $ docker image ls Copy ibm-cloud-private-x86_64-2.1.0.1.tar.gz package to the node. Extract the images and load into Docker $ tar xf ibm-cloud-private-x86_64-2.1.0.1.tar.gz -O | sudo docker load","title":"Problem Determination &amp; Resolution"},{"location":"icp/troubleshooting/#access","text":"See official faq on login","title":"Access"},{"location":"icp/troubleshooting/#unknown-certificate-authority","text":"$ docker login mycluster.icp:8500 Error response from daemon: Get https://mycluster.icp:8500/v2/: x509: certificate signed by unknown authority Go to your docker engine configuration and add the remote registry as an insecure one. On MAC you select the docker > preferences > Daemons> Advanced menu and then add the remote master name { \"debug\" : true , \"experimental\" : true , \"insecure-registries\" : [ \"jbcluster.icp:8500\" , \"mycluster.icp:8500\" , \"cpscluster.icp:8500\" ] } You can also verify the certificates are in the logged user ~/.docker folder. This folder should have a certs.d folder and one folder per remote server, you need to access. So the mycluster.icp:8500/ca.crt file needs to be copied there too.","title":"Unknown certificate authority"},{"location":"icp/troubleshooting/#not-able-to-login-to-docker-repository-running-on-master-node","text":"Different type of messages:","title":"Not able to login to docker repository running on master node"},{"location":"icp/troubleshooting/#unknown-authority","text":"`Error response from daemon: Get https://greencluster.icp:8500/v2/: x509: certificate signed by unknown authority. You need to configure your local docker to accept to connect to insecure registries by adding an entry about the target host. On MACOS the Preferences> Daemon > Advanced See also the note about accessing ICP private repository here and how to copy SSL certificate to your local host.","title":"Unknown authority"},{"location":"icp/troubleshooting/#x509-certificate-not-valid-for-a-specific-hostname","text":"Be sure the hostname you are using is in your /etc/hosts and you docker login to the good host.","title":"x509 certificate not valid for a specific hostname"},{"location":"icp/troubleshooting/#could-not-connect-to-a-backend-service-try-again-later-e0004","text":"While trying to get cluster configuration with command like bx pr cluster-config green2-cluster got this message. The problem may come from a lack of disk space on / on the host OS of the active master node. To add space for the virtual machine with Ubuntu OS, you need to do the following: Using the VM management tool like VMWare vsphere, add a new virtual disk log as root user to host OS, and list the device with ls /dev/sd* . You may have a new sdc or sdb device. We assume sdc for now. * Stop docker and kubelet (it can take some time for docker to stop): ```bash systemctl stop docker systemctl stop kubelet * See existing disks with `fdisk -l` and add a new disk with `fdisk /dev/sdc`. It should add a DOS partition and use the w option to write the changes. * Create different tables for the filesystem using `mkfs.ext4 /dev/sdc` * mount the newly created filesystem to a new folder: mkdir /mnt/disk mount /dev/sdc /mnt/disk * Move ICP install file to the new disk : ` mv / opt / ibm / cfc /* /mnt/disk` * unmount but update the boot setting to get the disk back on reboot: ``` umount /mnt/disk vi /etc/fstab /dev/sdc /opt/ibm/cfc ext4 default 1 3 ``` * restart docker and kubelet: ``` systemctl start docker systemctl start kubelet ``` See also [this note](https://kb.vmware.com/s/article/1003940) ## 503 on a deployed app with ingress rule The following message \"503 Service Temporarily Unavailable\" may appear when accessing a pod via virtual hostname defined in Ingress rules. Be sure to understand the ingress role To investigate do the following: * Display the helm release, and verify the Ingress is specified and the hosts is specified, the IP address matches the proxy IP address in the cluster. In the service verify the type is ClusterIP and the ports map the exposed port in the docker image. ![](chart-view.png) * In the ingress verify the service name, the app selector are matching with the deployment parameters using Service > Ingress -> Edit: ![](edit-ingress.png) ## Helm connection Issue: tls: bad certificate For the *Error: remote error: tls: bad certificate*: you need to be logged into the cluster and get the cluster config. The commands are: bx pr login -a https://ext-demo.icp:8443 -u admin --skip-ssl-validation bx pr cluster-config mycluster ## Helm version not able to connect to Tiller. Error: cannot connect to Tiller With version 2.1.0.2, TLS is enforced to communicate with the server. So to get the version the command is `helm version --tls`. You need also to get the certificates for the cluster. The command ` bx pr cluster-config <clustername>` will add those certificate into `~/.helm`. ## Helm incompatible version The error message may look like: `Error: incompatible versions client[v2.9.1] server[v2.7.3+icp]` Use the command to upgrade: `helm init --upgrade` ### For using SSL between Tiller and Helm See [this note from github helm account](https://github.com/helm/helm/blob/master/docs/tiller_ssl.md) # Deployment ## helm install command: User is not authorized to install release Be sure to enter the good namespace name in the install command. ## Pod not getting the image from docker private repository Looking at the Events report from the pod view you got a message like: Failed to pull image \u201cgreencluster.icp:8500/greencompute/customerms:v0.0.7\u201d: rpc error: code = Unknown desc = Error response from daemon: Get https://greencluster.icp:8500/v2/greencompute/customerms/manifests/v0.0.7: unauthorized: authentication required The new version of k8s enforces the use of secret to access the docker private repository. So you need to add a secret, named for example regsecret, for the docker registry object. $ kubectl create secret docker-registry regsecret --docker-server=172.16.40.130 --docker-username=admin --docker-password=<> --docker-email= --namespace=greencompute $ kubectl get secret regsecret --output=yaml --namespace=greencompute Then modify the deployment.yaml to reference this secret so the pod can access the repo during deployment: ``` spec: containers: - name: {{ .Chart.Name }} image: \" {{ .Values.image.repository }} : {{ .Values.image.tag }} \" .... imagePullSecrets: - name: regsecret","title":"Could not connect to a backend service. Try again later.   (E0004)"},{"location":"icp/troubleshooting/#verify-deployment","text":"When you deploy a helm chart you can assess how the deployment went using the ICP admin console or the kubectl CLI. For the user interface, go to the Workloads > Deployments menu to access the list of current deployments. Select the deployment and then the pod list. In the pod view select the events to assess how the pod deployment performed and the log file in Logs menu Using kublectl to get the status of a deployment $ kubectl get deployments --namespace browncompute > NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE casewebportal-casewebportal 1 1 1 1 2d or a specific detail view: kubectl describe deployment browncompute-dal-browncompute-dal Get the logs and events $ export POD_NAME = $( kubectl get pods --namespace browncompute -l \"app=casewebportal-casewebportal\" -o jsonpath = \"{.items[0].metadata.name}\" ) $ kubectl logs $POD_NAME --namespace browncompute $ kubectl get events --namespace browncompute --sort-by = '.metadata.creationTimestamp'","title":"Verify deployment"},{"location":"icp/troubleshooting/#accessing-an-app-expose-with-ingress-503-service-temporarily-unavailable","text":"This could be due to ingress rule configuration issue. We need to assess if the pod is up and running Get the pod name: k get pods --namespace greencompute Verify no issue in the container itself: k logs bc-inventory-dal-browncompute-dal-6bfb4f85b8-tdzjt --namespace greencompute if the service and ingress are well defined Go to the service view or kubectl get svc --namespace greencompute -o wide verify the app-selector and service Type Verify the service definition: k describe svc bc-inventory-dal-browncompute-dal --namespace greencompute * Get the ingress with kubectl describe ingress bc-inventory-dal-browncompute-dal -n greencompute Name : bc - inventory - dal - browncompute - dal Namespace : greencompute Address : 172.16 . 40.131 Default backend : default - http - backend : 80 ( < none > ) Rules : Host Path Backends ---- ---- -------- dal . brown . case / bc - inventory - dal - browncompute - dal : http ( < none > ) Annotations : Events : < none > this configuration looks good, but not... so what is happening? We need to look at the ingress-controller config: Get the list of ingress controllers: kubectl get pods -n kube-system Get the controller logs and verify if it receives the configuration when we deployed the service: kubectl logs nginx-ingress-controller-gvcj5 -n kube-system . Search for message with \"Event ... Kind: Ingress...\" Get NGINX configuration: kubectl exec -it -n kube-system nginx-ingress-controller-gvcj5 cat /etc/nginx/nginx.conf Here is an extract # start server dal.brown.case server { server_name dal.brown.case ; listen 80; listen [::]:80; set $proxy_upstream_name \"-\"; location / { log_by_lua_block { } port_in_redirect off; set $proxy_upstream_name \"\"; set $namespace \"greencompute\"; set $ingress_name \"bc-inventory-dal-browncompute-dal\"; set $service_name \"bc-inventory-dal-browncompute-dal\"; ... # No endpoints available for the request return 503; As we can see the $proxy_upstream_name is not set and the configuration is enforcing returning 503. The ingress rule has an issue... the port is not a port number but a name (\"bc-inventory-dal-browncompute-dal:http\")... So editing the rule and change to port 9080 makes it work: kubectl edit ing -n greencompute bc-inventory-dal-browncompute-dal","title":"Accessing an app expose with Ingress: 503 Service Temporarily Unavailable"},{"location":"icp/troubleshooting/#error-while-getting-cluster-info","text":"Try to do kubectl cluster-info : failed: error: you must be logged in to the server (the server has asked for the client to provide credentials): Be sure to have use the settings from the 'configure client'. Be sure the cluster name / IP address are mapped in /etc/hosts Be sure to have a ca.crt into ~/.ssh folder Use the bx pr login -a https://<ipaddress>:8443 -u admin command to login to the cluster","title":"Error while getting cluster info"},{"location":"icp/troubleshooting/#default-backend-404","text":"This error can occur if the ingress rules are not working well. Assess if ingress is well defined: virtual hostname, proxy address and status/age of running ``` kubectl get ing --namespace browncompute NAME HOSTS ADDRESS PORTS AGE browncompute-dal-browncompute-dal dal.brown.case 172.16.40.31 80 59m casewebportal-casewebportal portal.brown.case 172.16.40.31 80 10d ``` Get the detail of ingress rules, and its mapping to the expected service, the path and host mapping. kubectl describe ingress browncompute - dal - browncompute - dal -- namespace browncompute Name : browncompute - dal - browncompute - dal Namespace : browncompute Address : 172.16 . 40.31 Default backend : default - http - backend : 80 ( 10.100 . 221.196 : 8080 ) Rules : Host Path Backends ---- ---- -------- dal . brown . case / inventorydalsvc : 9080 ( < none > ) Annotations : No events . If ingress rules are correct for your release, check to see if there are other releases sharing the same host rule. This can happen if the release was deleted without the ingress rule being removed. kubectl get ing --all-namespaces=true NAMESPACE NAME HOSTS ADDRESS PORTS AGE greencompute greencompute-green-customerapp-green-customerapp greenapp.green.case 172.16.40.131 80 1h greencompute greencustomerapp-green-customerapp greenapp.green.case 172.16.40.131 80 1h Delete the conflicting ingress. kubectl delete ing greencompute-green-customerapp-green-customerapp ingress \"greencompute-green-customerapp-green-customerapp\" deleted","title":"Default backend - 404"},{"location":"icp/troubleshooting/#icp-cluster-is-not-accessible-via-admin-console","text":"After restart of the ICP master node, the ICP cluster is inaccessible remotely. Log into master node via SSH and check kube-system pods $ kubectl -s 127 .0.0.1:8888 -n kube-system get pods Check if any pods are in a bad state such as CrashLoopBackOff ... k8s-master-172.16.40.130 2/3 CrashLoopBackOff 393 40s ... Check the containers for that pod $ kubectl \u2013s 127 .0.0.1:8888 \u2013n kube-system describe pods k8s-master-172.16.40.130 To view the logs for a specific container, use the following command. For example, the controller-manager in the k8s-master-172.16.40.130 pod: $ kubectl \u2013s 127 .0.0.1:8888 \u2013n kube-system logs k8s-master-172.16.40.130 \u2013p controller-manager","title":"ICP Cluster is not accessible via admin console"},{"location":"icp/troubleshooting/#investigation","text":"When something is going wrong you can do the following: assess the node status with kubectl get nodes -o wide assess the state of the pods and where they are deployed: kubectl get pods -o wide look at what is deployed within a node: kubectl describe <podname> assess the storage state with kubectl get pv and kubectl get pvc Access logs of a pod: 'kubectl logs ' Exec a shell in the running pod and then use traditional network tools to investigate: kubectl exec -tin <namespace> <podname> sh * For a CrashLoopBackoffs error: CrashLoopBackoff encapsulates a large set of errors that are all hidden behind the same error condition. Some potential debugging steps * kubectl describe pod <podname> * ssh to the host","title":"Investigation"},{"location":"istio/readme/","text":"Use ISTIO for service mesh In this article we are covering a quick summary of istio as deployed inside ICP and how to leverage it for supporting different operational use cases for the Asset predictive maintenance solution. Summary The concepts are presented in the istio main page . From the istio architecture the control plane includes Pilot, Mixer and Citadel , and is responsible for managing and configuring proxies to route traffic, and configuring Mixers to enforce policies and collect telemetry. The following is an example of pod assignment within ICP. Egress gateway and servicegraph run on a proxy, while the other components run in the worker nodes. The command used to get this assignment are: $ kubectl get nodes $ kubectl describe node <ipaddress> To get the pods: kubectl get pods -n istio-system . The component roles: Component Role Envoy Proxy to mediate all inbound and outbound traffic for all services in the service mesh. It is deployed as a sidecar container inside the same pod as a service. Mixer Enforces access control and usage policies across the service mesh and collecting telemetry data from the Envoy proxy. Pilot Supports service discovery, traffic management, resiliency. Citadel Used for service-to-service and end-user authentication. Enforce security policy based on service identity. FAQ Communication Circuit Breaker \u2013 Need clarification on this. Does it depend on K8\u2019s liveliness check? Circuit breaking is specific to the Istio sidecar and does not depend on K8s health checks. More information https://istio.io/docs/tasks/traffic-management/circuit-breaking/ Security Service Registry \u2013 Do we have to deploy a service registry (like Eureka) or Istio will use K8s service information? Documentation says Istio assumes presence of service registry. Istio will automatically use the service information from K8s, there is no need to deploy another. Security - Do we have to run all services with K8 service account to use Istio? Workloads in K8s always run under a service account. If you don\u2019t explicitly specify one, the default service account within the namespace will be used. Security: Auth policy - How does application-generated OAuth token (in HTTP request) work with Istio? Need some examples on \u201cauth.claims\u201d and impact of Istio HTTP filters The OAuth token is parsed by Istio and its claims (including both standard claims, such as issuer, expiration, etc. and application specific claims) are sent to the Mixer. Mixer policies can refer to the claims in making policy decisions. There is some discussion in https://preliminary.istio.io/docs/concepts/security/ (e.g., can map claims to role in RBAC) but we don't have a concrete example. Note that you can still define policies that match on attributes and since request.auth.claims are strings, they can be processed by the Mixer language. Security: Auth policy - How do you use Role\u2019s permissions? A role's permissions are evaluated by a mixer policy. Request attributes, such as user, path and method are evaluated against RBAC rules to allow/deny access. More information in https://preliminary.istio.io/docs/concepts/security/#role-based-access-control-rbac Security: Auth policy - How to use application defined RBAC within Istio? Does it work only with K8S RBAC? Istio's RBAC model follows that of Kubernetes, but is implemented independently. RBAC is documented along with example policies in https://istio.io/docs/concepts/security/rbac/ RBAC policies for the BookInfo sample can be found in https://istio.io/docs/tasks/security/role-based-access-control/ The RBAC is very close to the one for k8s, RBAC defined in k8s are to control the k8s API. ISTIO control RBAC controls microservice, so the rules are not on the same objects. Security: Mutual TLS - Is there a way to use Customer provided Certificate Authority? Istio supports setting external CA as the root of trust. See https://istio.io/docs/tasks/security/plugin-ca-cert Security: Mutual TLS - Is there a process to approve/sign CSRs? From doc it seems like they are automatically approved/signed. Question Clarification [Rakesh]: Regarding CSR, it was specific to mTLS as specified under that bulleted item. Is there a CSR process for \u201csomething else\u201d as well? Citadel watches k8s service accounts creation and deletion and generates certificates (and keys) for that service account. Since Citadel is the CA, there is no formal CSR process involved. Currently it is automatic. Could you please provide more information regarding what controls, if any, do you think are needed for CSR approval? Security: the network policy (how the rules) can be configured in Istio along with calico (good example) We believe Istio and Calico are complementary. Calico can be used to set policies that are applied preDNAT on the nodes which is useful for setting standard firewall rules on all of the nodes. Istio policies apply to the communication between the services (both internal and external to the mesh). Kubernetes Network Policies (implemented via Calico) control communication between pods. These policies are applied regardless if the pods belong to the Istio mesh or not. Note, K8s network policies are basic source and destination rules whereas istio has more advanced support for evaluating policies between services within the mesh. API ingress controller configuration (that can work on ICP) The most straightforward approach for ingress is to use the Istio ingress gateway (not a K8s ingress controller) exposed as a load balancer service. Is there a specific issue of concern regarding ICP deployment? Telemetry (e.g latency tracing) strategy/configuration, rate limit rules/configuration, canary strategy, mixer Distributed tracing helps identify and debug issues when they arise. Requires minimal change in code - copying trace headers from incoming to outgoing requests. Traffic metrics are automatically generated and collected for display (preconfigured dashboards available). Rate limiting requires the mixer and you typically want to set the limit to have clients fail fast (e.g., ~80% of actual peak rate). Could you please clarify what guidance is being sought in relation to canary strategy? Oauth token istio expects oauth to come, it does not create ones. istio validates token and return 401 is not valid or present token needs to be signed but not encrypted istio extracts attributes at the proxy, and claims and send them to the mixer. at the mixer you can implement your rules. Use regular expression. 1.0 version there are some examples. https://istio-releases.github.io/v0.1/docs/concepts/network-and-auth/auth.html What Entry point ISTIO gateway is the entry point. We have an ingress gateway as an alternative to k8s ingress controller. expose a unique IP to a load balancer Support to websocket Was not supported before 0.8. And it is supported now. Stomp support ISTIO will not block Stomp Each service account has an unique identity, certificate is at the identity External CA for certificate management. component in istio is called 'citadel' watch for the creation of k8s for a service account, and then bind it to a certificate by creating a secret. The proxy is injected with the certification. when an envoy sends to another envoy they exchange certificate via mutual MTLS. The route of trust is kept inside the cluster. if you need to externalize the CA, you need to add 'citadel' to istio.io/. https://istio.io/docs/tasks/security/plugin-ca-cert/ Service Registry Component responsible for data plan connectivity is Pilot. It hooks up to the k8s registry. And it will get all the services deployed to k8s. Communication between microservice, there is no need to get ingress gateway, each pod has its own envoy sidecar. This sidecar automatically connects to pilot, to get the envoy for the target microservice. Service A needs to reference the service name. Be sure to use HTTP so envoy can decode the traffic. With encrypted traffic is not able to uncript. ingress gateway is for communication at the border of the cluster. Rate limiting Cassandra is using TCP, so for istio is TCP end point, it can be limited by the rate in byte at the network level. Mixer is where you can specify rate limiting policy and are applied globally.","title":"Istio introduction"},{"location":"istio/readme/#use-istio-for-service-mesh","text":"In this article we are covering a quick summary of istio as deployed inside ICP and how to leverage it for supporting different operational use cases for the Asset predictive maintenance solution.","title":"Use ISTIO for service mesh"},{"location":"istio/readme/#summary","text":"The concepts are presented in the istio main page . From the istio architecture the control plane includes Pilot, Mixer and Citadel , and is responsible for managing and configuring proxies to route traffic, and configuring Mixers to enforce policies and collect telemetry. The following is an example of pod assignment within ICP. Egress gateway and servicegraph run on a proxy, while the other components run in the worker nodes. The command used to get this assignment are: $ kubectl get nodes $ kubectl describe node <ipaddress> To get the pods: kubectl get pods -n istio-system . The component roles: Component Role Envoy Proxy to mediate all inbound and outbound traffic for all services in the service mesh. It is deployed as a sidecar container inside the same pod as a service. Mixer Enforces access control and usage policies across the service mesh and collecting telemetry data from the Envoy proxy. Pilot Supports service discovery, traffic management, resiliency. Citadel Used for service-to-service and end-user authentication. Enforce security policy based on service identity.","title":"Summary"},{"location":"istio/readme/#faq","text":"","title":"FAQ"},{"location":"istio/readme/#communication","text":"","title":"Communication"},{"location":"istio/readme/#circuit-breaker-need-clarification-on-this-does-it-depend-on-k8s-liveliness-check","text":"Circuit breaking is specific to the Istio sidecar and does not depend on K8s health checks. More information https://istio.io/docs/tasks/traffic-management/circuit-breaking/","title":"Circuit Breaker \u2013 Need clarification on this. Does it depend on K8\u2019s liveliness check?"},{"location":"istio/readme/#security","text":"","title":"Security"},{"location":"istio/readme/#service-registry-do-we-have-to-deploy-a-service-registry-like-eureka-or-istio-will-use-k8s-service-information","text":"Documentation says Istio assumes presence of service registry. Istio will automatically use the service information from K8s, there is no need to deploy another.","title":"Service Registry \u2013 Do we have to deploy a service registry (like Eureka) or Istio will use K8s service information?"},{"location":"istio/readme/#security-do-we-have-to-run-all-services-with-k8-service-account-to-use-istio","text":"Workloads in K8s always run under a service account. If you don\u2019t explicitly specify one, the default service account within the namespace will be used.","title":"Security - Do we have to run all services with K8 service account to use Istio?"},{"location":"istio/readme/#security-auth-policy-how-does-application-generated-oauth-token-in-http-request-work-with-istio-need-some-examples-on-authclaims-and-impact-of-istio-http-filters","text":"The OAuth token is parsed by Istio and its claims (including both standard claims, such as issuer, expiration, etc. and application specific claims) are sent to the Mixer. Mixer policies can refer to the claims in making policy decisions. There is some discussion in https://preliminary.istio.io/docs/concepts/security/ (e.g., can map claims to role in RBAC) but we don't have a concrete example. Note that you can still define policies that match on attributes and since request.auth.claims are strings, they can be processed by the Mixer language.","title":"Security: Auth policy - How does application-generated OAuth token (in HTTP request) work with Istio? Need some examples on \u201cauth.claims\u201d and impact of Istio HTTP filters"},{"location":"istio/readme/#security-auth-policy-how-do-you-use-roles-permissions","text":"A role's permissions are evaluated by a mixer policy. Request attributes, such as user, path and method are evaluated against RBAC rules to allow/deny access. More information in https://preliminary.istio.io/docs/concepts/security/#role-based-access-control-rbac","title":"Security: Auth policy - How do you use Role\u2019s permissions?"},{"location":"istio/readme/#security-auth-policy-how-to-use-application-defined-rbac-within-istio-does-it-work-only-with-k8s-rbac","text":"Istio's RBAC model follows that of Kubernetes, but is implemented independently. RBAC is documented along with example policies in https://istio.io/docs/concepts/security/rbac/ RBAC policies for the BookInfo sample can be found in https://istio.io/docs/tasks/security/role-based-access-control/ The RBAC is very close to the one for k8s, RBAC defined in k8s are to control the k8s API. ISTIO control RBAC controls microservice, so the rules are not on the same objects.","title":"Security: Auth policy - How to use application defined RBAC within Istio? Does it work only with K8S RBAC?"},{"location":"istio/readme/#security-mutual-tls-is-there-a-way-to-use-customer-provided-certificate-authority","text":"Istio supports setting external CA as the root of trust. See https://istio.io/docs/tasks/security/plugin-ca-cert","title":"Security: Mutual TLS - Is there a way to use Customer provided Certificate Authority?"},{"location":"istio/readme/#security-mutual-tls-is-there-a-process-to-approvesign-csrs-from-doc-it-seems-like-they-are-automatically-approvedsigned","text":"Question Clarification [Rakesh]: Regarding CSR, it was specific to mTLS as specified under that bulleted item. Is there a CSR process for \u201csomething else\u201d as well? Citadel watches k8s service accounts creation and deletion and generates certificates (and keys) for that service account. Since Citadel is the CA, there is no formal CSR process involved. Currently it is automatic. Could you please provide more information regarding what controls, if any, do you think are needed for CSR approval?","title":"Security: Mutual TLS - Is there a process to approve/sign CSRs? From doc it seems like they are automatically approved/signed."},{"location":"istio/readme/#security-the-network-policy-how-the-rules-can-be-configured-in-istio-along-with-calico-good-example","text":"We believe Istio and Calico are complementary. Calico can be used to set policies that are applied preDNAT on the nodes which is useful for setting standard firewall rules on all of the nodes. Istio policies apply to the communication between the services (both internal and external to the mesh). Kubernetes Network Policies (implemented via Calico) control communication between pods. These policies are applied regardless if the pods belong to the Istio mesh or not. Note, K8s network policies are basic source and destination rules whereas istio has more advanced support for evaluating policies between services within the mesh. API ingress controller configuration (that can work on ICP) The most straightforward approach for ingress is to use the Istio ingress gateway (not a K8s ingress controller) exposed as a load balancer service. Is there a specific issue of concern regarding ICP deployment? Telemetry (e.g latency tracing) strategy/configuration, rate limit rules/configuration, canary strategy, mixer Distributed tracing helps identify and debug issues when they arise. Requires minimal change in code - copying trace headers from incoming to outgoing requests. Traffic metrics are automatically generated and collected for display (preconfigured dashboards available). Rate limiting requires the mixer and you typically want to set the limit to have clients fail fast (e.g., ~80% of actual peak rate). Could you please clarify what guidance is being sought in relation to canary strategy?","title":"Security: the network policy (how the rules) can be configured in Istio along with calico (good example)"},{"location":"istio/readme/#oauth-token","text":"istio expects oauth to come, it does not create ones. istio validates token and return 401 is not valid or present token needs to be signed but not encrypted istio extracts attributes at the proxy, and claims and send them to the mixer. at the mixer you can implement your rules. Use regular expression. 1.0 version there are some examples. https://istio-releases.github.io/v0.1/docs/concepts/network-and-auth/auth.html","title":"Oauth token"},{"location":"istio/readme/#what-entry-point","text":"ISTIO gateway is the entry point. We have an ingress gateway as an alternative to k8s ingress controller. expose a unique IP to a load balancer","title":"What Entry point"},{"location":"istio/readme/#support-to-websocket","text":"Was not supported before 0.8. And it is supported now.","title":"Support to websocket"},{"location":"istio/readme/#stomp-support","text":"ISTIO will not block Stomp Each service account has an unique identity, certificate is at the identity External CA for certificate management. component in istio is called 'citadel' watch for the creation of k8s for a service account, and then bind it to a certificate by creating a secret. The proxy is injected with the certification. when an envoy sends to another envoy they exchange certificate via mutual MTLS. The route of trust is kept inside the cluster. if you need to externalize the CA, you need to add 'citadel' to istio.io/. https://istio.io/docs/tasks/security/plugin-ca-cert/","title":"Stomp support"},{"location":"istio/readme/#service-registry","text":"Component responsible for data plan connectivity is Pilot. It hooks up to the k8s registry. And it will get all the services deployed to k8s. Communication between microservice, there is no need to get ingress gateway, each pod has its own envoy sidecar. This sidecar automatically connects to pilot, to get the envoy for the target microservice. Service A needs to reference the service name. Be sure to use HTTP so envoy can decode the traffic. With encrypted traffic is not able to uncript. ingress gateway is for communication at the border of the cluster.","title":"Service Registry"},{"location":"istio/readme/#rate-limiting","text":"Cassandra is using TCP, so for istio is TCP end point, it can be limited by the rate in byte at the network level. Mixer is where you can specify rate limiting policy and are applied globally.","title":"Rate limiting"},{"location":"journey/","text":"Journey to the cloud This article aims to present how to adopt the cloud leveraging the existing materials develop by IBM Cloud Architecture and Solution engineering team. Why Cloud? The most important value points: Rapid deployment, scalability, ease of use, and elasticity to adapt to demand Predictable cost, optimized for workload demand Enable DevOps, increase developers productivity Private cloud adds: Knowledge of where data resides Apply own enterprise own security and governance policies Simplify integration to on-premise business functions IBM Public Cloud value proposition: Easy migration Adopt cloud native development and operations AI Ready Hybrid integration Secure: continuous security scanning for apps and data Easily integrate and manage all your data across vendors and clouds \u2014 on or off premises Cloud is the technology for innovation and transformation. AI, blockchain, multi-cloud, SaaS integration, single page app are drivers for cloud adoption. The new application landscape integrate existing data centers, private cloud within corporate firewall and SaaS, Public cloud provider, IoT, traditional B2B... The new landscape will be multi-cluster & multi-cloud. IBM Cloud public offers a set of added value services to manage data, app development, devops, networking access, servers, security, AI, blockchain and more.... The IBM Cloud service catalog is continuously updated IBM Private Cloud reference architecture IBM Cloud Private brings cloud innovation within your datacenter. It is a Kubernetes platform with optimized scheduling, with most of the IBM Middleware products moving to it and it: supports better cluster management, security capabilities, image repositories, routing services, and microservices mesh authorizes infrastructure automation with scripts (Terraform, IBM Multi Cloud Manager ) provides monitoring for container-based applications for logging, dashboards, and automation. supports network and storage policy-based controls for application isolation and security, and automated application health checking and recovery from failures For product introduction see ICP Product page here. With the ICP catalog you can install a lot of IBM middleware products and some open sources and your own helm charts in few seconds. Challenges to solve We recognize that not every organization is ready to move everything they have to a public cloud environment, and there are lots of reasons for that. IBM offers the richest range of deployment options \u2013 from Private to Public and Dedicated. Still, enterprises will face new challenges in broadening the adoption of Cloud to critical applications. We can group those challenges into different categories and we will address in next sections the best practices and other product informations we have developed for you: Application ARCHITECTURE and DEVELOPMENT practices: What does it mean to adopt Microservice pattern Lift and shift existing applications to cloud Refactoring existing applications Adopt polyglot with new Languages & runtimes Engage with APIs management and API standard DevOps, continuous delivery Application PORTABILITY Regulation and multi regions deployment Cloud provider availability Cost and quality of services INTEGRATION APIs definition & Management Integrating existing Applications and SOA services Support transactions Leverage and coexist with existing ESB Agility for new integration needs DATA MOVEMENT & GOVERNANCE New Analytics & AI Services Data Privacy & Risk Data Gravity & Performance Network Cost Data Gravity & Lock-in SERVICE MANAGEMENT Monitoring/SRE SLAs Problem Diagnosis HA/DR Scale Backup and restore SECURITY & COMPLIANCE Identity & Authorization Audit Shared Responsibility Models Regulatory Compliance Operation lead (Todd), responsible for infrastructure management, security and environment availability and maintenance has different concerns than developer (Jane) who is responsible to develop new application but also maintain existing application. A journey... Architecture and Development Microservice reference architecture Microservices is an application architectural style in which an application is composed of many discrete, network-connected components Microservices in the world of integration Public cloud for dev and test Stock trader app to ICP and Public Blue compute on kubernetes: a microservice reference implementation Innovate quickly with cloud native development Leverage the following tutorials and articles Tutorial: Deploy a cloud-native application in Kubernetes Microservices with Kubernetes An Angular 6 SPA with nodejs bff full app Run the Springboot on a Kubernetes Cluster Making Microservices Resilient Use Microclimate to run an End-to-End DevOps environment on IBM Cloud Private Refactoring app to microservices Business wants to improve the application to increase client adoption and satisfaction. Monolithic applications are more difficult to change and moving to microservice architecture will bring velocity to the development team. Refactoring application code to microservices , this article addresses the why and how to refactor an existing Java application to microservice architecture. They also cover data model refactoring The process to start from an existing JEE to split into microservices is documented in this repository . 10-15% of existing WebSphere workloads can be moved as-is to cloud. WebSphere on the Cloud: Application Modernization Polyglot applications Use one the available boiler plates, starting code from IBM Cloud like Nodejs, Java, Python, GoLang, Swift... and deploy them on ICP or IBM Container services. Below is a list of how to guides for Java and nodejs app but also decision and event streaming: Develop a SOAP Jaxws app deployed in OpenLiberty and docker Apply a test driven development for Angular 6 app with nodejs-expressjs BFF service Event messaging with Apache Kafka in kubernetes Develop a REST API with JAXRS How to develop a REST API which integrates with a SOAP backend Implement decisions with Operational Decision Management and deploy ODM on ICP Lift and shift A need to shift from IaaS (VM, network, storage, security) to container and CaaS (kubernetes) and PaaS (cloud foundry). Java based lift and shift A Traditional JEE app running on WebSphere Application server can be lift and shift to WAS on IBM Cloud. The Inventory Data Access Layer is a JaxWS application exposing a SOAP APIs. The figure below shows the deployed app in the IBM console . The application is accessing a DB2 via JDBC. Deployment explanation on tWAS We are presenting how to move an integration solution to IBM Cloud in this article: Lift and shift of an integration solution . With a deep dive Java EE migration path in this repository A migration strategy tool to support your migration scoping, with the WebSphere Application Server V9 Total Cost of Ownership Calculator . The Transformation Advisor application deployable on ICP helps to quickly evaluate your on-premise applications for rapid deployment on WebSphere Application Server and Liberty on Public and/or Private Cloud environments. The Migration Toolkit for Application Binaries provides a command line tool that quickly evaluates application binaries for rapid deployment on newer versions of WebSphere Application Server traditional or Liberty. Finally the source migration toolkit is an Eclipse-based Migration Toolkit provides a rich set of tools that help you migrate applications from third-party application servers, between versions of WebSphere Application Server, to Liberty, and to cloud platforms. MQ lift and shift We are presenting some simple implementation using MQ on premise with Message Driven Bean deployed on WAS and a lift and shift path to MQ on IBM Cloud in this note The benefits to run MQ on cloud is that you keep your skill set but use cloud speed to: Create queue manager in minute. Get product upgrade and patch done by IBM. Pay as you use. Integrate with MQ manager on premise. DB2 lift and shift We are presenting different approaches to migrate DB2 database to DB2 on cloud. Message Broker / IBM Integration Bus How an IBM Integration Bus runtime can be deployed on premise or on IBM Cloud Private, running gateway flows to expose REST api from SOAP back end services Deploying IIB Application using Docker locally Build IBM Integration Bus Helm Chart suitable for IBM Cloud Private and deploy it to ICP API management How we define an API product with IBM API Connect to integrate an existing SOA service Deploy API Connect 2018.* on ICP Devops DevOps for Cloud Native Reference Application Hybrid integration solution CI/CD Tutorial to install and configure a Jenkins server that uses a persistent storage volume on IBM Cloud Private Use Jenkins in a Kubernetes cluster to continuously integrate and deliver on IBM Cloud Private Devops for API deployment Portability Private cloud reference architecture Backup and restore on IBM cloud Private A guidance to backup and recovery procedures to best meet your resilience requirements, in the context of kubernetes cluster using ETCD. Using Terraform to deploy ICP on different Cloud providers Federating ICP on-premise clusters Integration Leveraging existing investments and in-production services with new cloud native and mobile applications. Transforming SOAP and other interface to RESTful API. The reference architecture for hybrid cloud Enables cloud applications and services to have a tighter coupling with specific on-premises enterprise system components. Hybrid integration solution implementation presents a deep dive implementation using API Connect, MQ, WAS application, Cloud native web application, DB2, ODM, BPM, and IIB... IBM Cloud private knowledge sharing How an IBM Integration Bus runtime can be deployed on premise or on IBM Cloud Private, running gateway flows to expose REST api from SOAP back end services Tutorial provides a guided walkthrough of the IBM MQ on Cloud service in IBM Cloud Data governance The IBM AI Ladder also begins with data. You get higher business value when you perform business-assisted functions such as analytics, machine learning, or artificial intelligence on top of the data... Data Analytics reference architecture Data Analytics solution implementation This project provides a reference implementation for building and running analytics application deployed on hybrid cloud environment. Two sub projects are under development to leverage ICP for Data Customer analysis with cognitive and analytics in hybrid cloud The goal of this implementation is to deliver a reference implementation for data management and service integration to consume structured and unstructured data to assess customer attrition. Manufacturing Asset Predictive Maintenance This set of projects presents an end to end solution to enable predictive maintenance capabilities on manufacturing assets Deploying Cassandra on kubernetes Service management Reference Architecture @ IBM Garage method Hybrid cloud management Hybrid, multicloud world is quickly becoming the new normal for enterprise. Monitoring in IBM Cloud Private A set of artifacts created by the IBM CSMO team to assist you with performance management of your ICP deployment. CSMO for cloud native application Leveraging Grafana, prometheus DevOps, specifically Cloud Service Management & Operations (CSMO), is important for Cloud Native Microservice style applications. This project is developed to demonstrate how to use tools and services available on IBM Cloud to implement CSMO for the BlueCompute reference application A Sample Tools Implementation of Incident Management Solution A set of tools to provide an end-to-end view of application. HA/DR Making Microservices Resilient Guidance on HA ICP Cluster Setup ICP Installation on Ubuntu Security IBM Cloud Private supports Identity and Access Management, based on OpenID Connect (OIDC). It permits access to private LDAP, the identity service authenticates users with the credentials in your enterprise directory. It provides Role Based Access Control: editor, reader, operator, administrator roles. You can use container security context to specify user and group used to run the container(s) within a pod and how to access filesystem. IBM Cloud Security for IBM Cloud Kubernetes Service IBP Cloud Private user management See kubernetes documentation about pod security context here Cloud Architecture Solution Engineering Assets list","title":"Skill Journey"},{"location":"journey/#journey-to-the-cloud","text":"This article aims to present how to adopt the cloud leveraging the existing materials develop by IBM Cloud Architecture and Solution engineering team.","title":"Journey to the cloud"},{"location":"journey/#why-cloud","text":"The most important value points: Rapid deployment, scalability, ease of use, and elasticity to adapt to demand Predictable cost, optimized for workload demand Enable DevOps, increase developers productivity Private cloud adds: Knowledge of where data resides Apply own enterprise own security and governance policies Simplify integration to on-premise business functions IBM Public Cloud value proposition: Easy migration Adopt cloud native development and operations AI Ready Hybrid integration Secure: continuous security scanning for apps and data Easily integrate and manage all your data across vendors and clouds \u2014 on or off premises Cloud is the technology for innovation and transformation. AI, blockchain, multi-cloud, SaaS integration, single page app are drivers for cloud adoption. The new application landscape integrate existing data centers, private cloud within corporate firewall and SaaS, Public cloud provider, IoT, traditional B2B... The new landscape will be multi-cluster & multi-cloud. IBM Cloud public offers a set of added value services to manage data, app development, devops, networking access, servers, security, AI, blockchain and more.... The IBM Cloud service catalog is continuously updated","title":"Why Cloud?"},{"location":"journey/#ibm-private-cloud-reference-architecture","text":"IBM Cloud Private brings cloud innovation within your datacenter. It is a Kubernetes platform with optimized scheduling, with most of the IBM Middleware products moving to it and it: supports better cluster management, security capabilities, image repositories, routing services, and microservices mesh authorizes infrastructure automation with scripts (Terraform, IBM Multi Cloud Manager ) provides monitoring for container-based applications for logging, dashboards, and automation. supports network and storage policy-based controls for application isolation and security, and automated application health checking and recovery from failures For product introduction see ICP Product page here. With the ICP catalog you can install a lot of IBM middleware products and some open sources and your own helm charts in few seconds.","title":"IBM Private Cloud reference architecture"},{"location":"journey/#challenges-to-solve","text":"We recognize that not every organization is ready to move everything they have to a public cloud environment, and there are lots of reasons for that. IBM offers the richest range of deployment options \u2013 from Private to Public and Dedicated. Still, enterprises will face new challenges in broadening the adoption of Cloud to critical applications. We can group those challenges into different categories and we will address in next sections the best practices and other product informations we have developed for you: Application ARCHITECTURE and DEVELOPMENT practices: What does it mean to adopt Microservice pattern Lift and shift existing applications to cloud Refactoring existing applications Adopt polyglot with new Languages & runtimes Engage with APIs management and API standard DevOps, continuous delivery Application PORTABILITY Regulation and multi regions deployment Cloud provider availability Cost and quality of services INTEGRATION APIs definition & Management Integrating existing Applications and SOA services Support transactions Leverage and coexist with existing ESB Agility for new integration needs DATA MOVEMENT & GOVERNANCE New Analytics & AI Services Data Privacy & Risk Data Gravity & Performance Network Cost Data Gravity & Lock-in SERVICE MANAGEMENT Monitoring/SRE SLAs Problem Diagnosis HA/DR Scale Backup and restore SECURITY & COMPLIANCE Identity & Authorization Audit Shared Responsibility Models Regulatory Compliance Operation lead (Todd), responsible for infrastructure management, security and environment availability and maintenance has different concerns than developer (Jane) who is responsible to develop new application but also maintain existing application.","title":"Challenges to solve"},{"location":"journey/#a-journey","text":"","title":"A journey..."},{"location":"journey/#architecture-and-development","text":"","title":"Architecture and Development"},{"location":"journey/#microservice-reference-architecture","text":"Microservices is an application architectural style in which an application is composed of many discrete, network-connected components Microservices in the world of integration Public cloud for dev and test Stock trader app to ICP and Public Blue compute on kubernetes: a microservice reference implementation","title":"Microservice reference architecture"},{"location":"journey/#innovate-quickly-with-cloud-native-development","text":"Leverage the following tutorials and articles Tutorial: Deploy a cloud-native application in Kubernetes Microservices with Kubernetes An Angular 6 SPA with nodejs bff full app Run the Springboot on a Kubernetes Cluster Making Microservices Resilient Use Microclimate to run an End-to-End DevOps environment on IBM Cloud Private","title":"Innovate quickly with cloud native development"},{"location":"journey/#refactoring-app-to-microservices","text":"Business wants to improve the application to increase client adoption and satisfaction. Monolithic applications are more difficult to change and moving to microservice architecture will bring velocity to the development team. Refactoring application code to microservices , this article addresses the why and how to refactor an existing Java application to microservice architecture. They also cover data model refactoring The process to start from an existing JEE to split into microservices is documented in this repository . 10-15% of existing WebSphere workloads can be moved as-is to cloud. WebSphere on the Cloud: Application Modernization","title":"Refactoring app to microservices"},{"location":"journey/#polyglot-applications","text":"Use one the available boiler plates, starting code from IBM Cloud like Nodejs, Java, Python, GoLang, Swift... and deploy them on ICP or IBM Container services. Below is a list of how to guides for Java and nodejs app but also decision and event streaming: Develop a SOAP Jaxws app deployed in OpenLiberty and docker Apply a test driven development for Angular 6 app with nodejs-expressjs BFF service Event messaging with Apache Kafka in kubernetes Develop a REST API with JAXRS How to develop a REST API which integrates with a SOAP backend Implement decisions with Operational Decision Management and deploy ODM on ICP","title":"Polyglot applications"},{"location":"journey/#lift-and-shift","text":"A need to shift from IaaS (VM, network, storage, security) to container and CaaS (kubernetes) and PaaS (cloud foundry). Java based lift and shift A Traditional JEE app running on WebSphere Application server can be lift and shift to WAS on IBM Cloud. The Inventory Data Access Layer is a JaxWS application exposing a SOAP APIs. The figure below shows the deployed app in the IBM console . The application is accessing a DB2 via JDBC. Deployment explanation on tWAS We are presenting how to move an integration solution to IBM Cloud in this article: Lift and shift of an integration solution . With a deep dive Java EE migration path in this repository A migration strategy tool to support your migration scoping, with the WebSphere Application Server V9 Total Cost of Ownership Calculator . The Transformation Advisor application deployable on ICP helps to quickly evaluate your on-premise applications for rapid deployment on WebSphere Application Server and Liberty on Public and/or Private Cloud environments. The Migration Toolkit for Application Binaries provides a command line tool that quickly evaluates application binaries for rapid deployment on newer versions of WebSphere Application Server traditional or Liberty. Finally the source migration toolkit is an Eclipse-based Migration Toolkit provides a rich set of tools that help you migrate applications from third-party application servers, between versions of WebSphere Application Server, to Liberty, and to cloud platforms. MQ lift and shift We are presenting some simple implementation using MQ on premise with Message Driven Bean deployed on WAS and a lift and shift path to MQ on IBM Cloud in this note The benefits to run MQ on cloud is that you keep your skill set but use cloud speed to: Create queue manager in minute. Get product upgrade and patch done by IBM. Pay as you use. Integrate with MQ manager on premise. DB2 lift and shift We are presenting different approaches to migrate DB2 database to DB2 on cloud. Message Broker / IBM Integration Bus How an IBM Integration Bus runtime can be deployed on premise or on IBM Cloud Private, running gateway flows to expose REST api from SOAP back end services Deploying IIB Application using Docker locally Build IBM Integration Bus Helm Chart suitable for IBM Cloud Private and deploy it to ICP","title":"Lift and shift"},{"location":"journey/#api-management","text":"How we define an API product with IBM API Connect to integrate an existing SOA service Deploy API Connect 2018.* on ICP","title":"API management"},{"location":"journey/#devops","text":"DevOps for Cloud Native Reference Application Hybrid integration solution CI/CD Tutorial to install and configure a Jenkins server that uses a persistent storage volume on IBM Cloud Private Use Jenkins in a Kubernetes cluster to continuously integrate and deliver on IBM Cloud Private Devops for API deployment","title":"Devops"},{"location":"journey/#portability","text":"","title":"Portability"},{"location":"journey/#private-cloud-reference-architecture","text":"","title":"Private cloud reference architecture"},{"location":"journey/#backup-and-restore-on-ibm-cloud-private","text":"A guidance to backup and recovery procedures to best meet your resilience requirements, in the context of kubernetes cluster using ETCD.","title":"Backup and restore on IBM cloud Private"},{"location":"journey/#using-terraform-to-deploy-icp-on-different-cloud-providers","text":"","title":"Using Terraform to deploy ICP on different Cloud providers"},{"location":"journey/#federating-icp-on-premise-clusters","text":"","title":"Federating ICP  on-premise clusters"},{"location":"journey/#integration","text":"Leveraging existing investments and in-production services with new cloud native and mobile applications. Transforming SOAP and other interface to RESTful API.","title":"Integration"},{"location":"journey/#the-reference-architecture-for-hybrid-cloud","text":"Enables cloud applications and services to have a tighter coupling with specific on-premises enterprise system components.","title":"The reference architecture for hybrid cloud"},{"location":"journey/#hybrid-integration-solution-implementation","text":"presents a deep dive implementation using API Connect, MQ, WAS application, Cloud native web application, DB2, ODM, BPM, and IIB...","title":"Hybrid integration solution implementation"},{"location":"journey/#ibm-cloud-private-knowledge-sharing","text":"","title":"IBM Cloud private knowledge sharing"},{"location":"journey/#how-an-ibm-integration-bus-runtime-can-be-deployed-on-premise-or-on-ibm-cloud-private-running-gateway-flows-to-expose-rest-api-from-soap-back-end-services","text":"","title":"How an IBM Integration Bus runtime can be deployed on premise or on IBM Cloud Private, running gateway flows to expose REST api from SOAP back end services"},{"location":"journey/#tutorial-provides-a-guided-walkthrough-of-the-ibm-mq-on-cloud-service-in-ibm-cloud","text":"","title":"Tutorial provides a guided walkthrough of the IBM MQ on Cloud service in IBM Cloud"},{"location":"journey/#data-governance","text":"The IBM AI Ladder also begins with data. You get higher business value when you perform business-assisted functions such as analytics, machine learning, or artificial intelligence on top of the data...","title":"Data governance"},{"location":"journey/#data-analytics-reference-architecture","text":"","title":"Data Analytics reference architecture"},{"location":"journey/#data-analytics-solution-implementation","text":"This project provides a reference implementation for building and running analytics application deployed on hybrid cloud environment. Two sub projects are under development to leverage ICP for Data Customer analysis with cognitive and analytics in hybrid cloud The goal of this implementation is to deliver a reference implementation for data management and service integration to consume structured and unstructured data to assess customer attrition. Manufacturing Asset Predictive Maintenance This set of projects presents an end to end solution to enable predictive maintenance capabilities on manufacturing assets","title":"Data Analytics solution implementation"},{"location":"journey/#deploying-cassandra-on-kubernetes","text":"","title":"Deploying Cassandra on kubernetes"},{"location":"journey/#service-management","text":"","title":"Service management"},{"location":"journey/#reference-architecture-ibm-garage-method","text":"","title":"Reference Architecture @ IBM Garage method"},{"location":"journey/#hybrid-cloud-management","text":"Hybrid, multicloud world is quickly becoming the new normal for enterprise.","title":"Hybrid cloud management"},{"location":"journey/#monitoring-in-ibm-cloud-private","text":"A set of artifacts created by the IBM CSMO team to assist you with performance management of your ICP deployment.","title":"Monitoring in IBM Cloud Private"},{"location":"journey/#csmo-for-cloud-native-application","text":"Leveraging Grafana, prometheus DevOps, specifically Cloud Service Management & Operations (CSMO), is important for Cloud Native Microservice style applications. This project is developed to demonstrate how to use tools and services available on IBM Cloud to implement CSMO for the BlueCompute reference application","title":"CSMO for cloud native application"},{"location":"journey/#a-sample-tools-implementation-of-incident-management-solution","text":"A set of tools to provide an end-to-end view of application.","title":"A Sample Tools Implementation of Incident Management Solution"},{"location":"journey/#hadr","text":"Making Microservices Resilient Guidance on HA ICP Cluster Setup","title":"HA/DR"},{"location":"journey/#icp-installation-on-ubuntu","text":"","title":"ICP Installation on Ubuntu"},{"location":"journey/#security","text":"IBM Cloud Private supports Identity and Access Management, based on OpenID Connect (OIDC). It permits access to private LDAP, the identity service authenticates users with the credentials in your enterprise directory. It provides Role Based Access Control: editor, reader, operator, administrator roles. You can use container security context to specify user and group used to run the container(s) within a pod and how to access filesystem. IBM Cloud Security for IBM Cloud Kubernetes Service IBP Cloud Private user management See kubernetes documentation about pod security context here","title":"Security"},{"location":"journey/#cloud-architecture-solution-engineering-assets-list","text":"","title":"Cloud Architecture Solution Engineering Assets list"},{"location":"service-mesh/readme/","text":"Microservice mesh In this note we are grouping the studies around microservice to microservice communication with Kubernetes deployment. We are addressing: how ingress controller helps inside Kubernetes how API gateway helps for API management and service integration how to expose service in hybrid cloud how to discover service All come back to the requirements, skill set and fit to purpose. Definitions Service meshes provide visibility, resiliency, traffic, and security control of distributed application services. They deliver policy-based networking for microservices in the contraints of virtual network and continuous topology updates. Externalizing, via declarations, the logic to support network potential issues, like resiliency, simplifies dramatically developers work. Some misconception to clarify around microservice and APIs: microservices are not fine grained web services APIs are not equivalent to microservices microservices are not implementation of APIs API is an interface, a way to make a request to get or change data in an application. In modern use API refers to REST web APIs using HTTP protocol, with JSON format (sometime XML is still used). Interface decouples the caller from the implementation. The caller has no idea how it is implemented. A microservice is in fact a component. micro refers to the granularity of the component not of the exposed interface. The following diagram illustrates all those concepts. We encourage you to go to read integration design and architecture series . Container orchestration like Kubernetes are mainly doing application scheduling, cluster management, resource provisioning, platform and workload monitoring and service discovery. When application solutions are growing in size and complexity, you need to addres the following items: visibility on how traffic is flowing between microservice, how routing is done between microservice based on requests contained or the origination point or the end point how to support resiliency by handling failure in a graceful manner how to ensure security with identity assertion how to enforce security policy which defined the requirements for service mesh. Service mesh architecture defines a data and control planes: Control plane: supports policy and configuration for services in the mesh, and provides aggregation for telemetry. It has API and CLI to centralize control to the services deployed. In Kubernetes control planes are deployed in a system namespace. Data plane: handles the actual inspection, transiting, and routing of network traffic. It is responsible for health checking, load balancing, authentication, authorization, inbound (ingress) and outbound (egress) cluster network traffic. Applications / microservices are unaware of data plane. Context Traditional modern architecture involves having different components exposing reusable APIs, addressing different channels (mobile, single page application, traditional server pages or B2B apps), consuming APIs (mobile APIs, back end for front end, shared common apis like authentication, authorization,...) and backend services addressing reusable business services: API management can be added via API gateway. This is a distributed application with cross related communication channels, where any changes to the service interface characteristics impact any of the components. Moving to microservices architecture style adds more communication challenges and devops complexity but provides a lot of business values such as: rapid deployment of new business capabilities, co-evolving in parallel of other services. focusing on business domain with clear ownership of the business function and feature roadmap better operation procedure, automated, and with easy rollout and continuous delivery. A/B testing to assess how new feature deployed improve business operations * Improve resiliency by deploying on multi language cluster As an example we can use the following predictive maintenance asset solution with the following capabilities to support: user authentication user management: add / delete new user user self registration, reset password user permission control user profile asset management risk assessment service Each could be grouped by business domain like the user management, asset management, and application access control. So domain separation can be a good microservice boundary. But if the number of user reach millions then we may need to optimize the runtime processing of reading user credential, and scale the service differently, leading to a service map like the diagram below, where runtime and management are separated services. All of these still does not address the fact that data are distributed and even more with microservices owning their data persistence. As developers and architects we still have to address the following data integrity problems: two phases commit compensating operation eventual data consistency: some microservice updating data may share those updates with other microservices. Data aggregation: adding new views on data, owned by a microservice, to support new aggregates. Examples are preparing data view for machine learning modeling, analytics, or business intelligence... From the previous microservice allocation we can see the needs to propagate data update between services. Adding or unsubscribing a user involves updating the asset the user own and the authentication runtime service: Adding a new application changes the authorization runtime service. We are now looking at the following questions: how does webapp access APIs for their main service, of back end for front end service. how does deployed microservice access other service: discover and access? How data consistency can be ensured? is there a simpler way to manage cross microservice dependency? The answers depend on the existing infrastructure and environment, and deployment needs. Service routing We have to dissociate intra-cluster communication versus inter clusters or cluster to external services. Without getting into too much detail of IP routing within Kubernetes some important elements of the cluster are important to remember: microservices are packaged as docker container and expose port. When deployed they run in a pod within a node (physical or virtual machine) containers can talk to other containers only if they are on the same machine, or when they have exposed port. * Kubernetes is configured with a large flat subnet (e.g. 172.30.0.0/16) which is used for internal application traffic inside of the cluster. Each worker node in the Kubernetes cluster is assigned one or more non-overlapping slices of this network, coordinated by the Kubernetes master node. When a container is created in the cluster, it gets assigned to a worker node and is given an IP address from the slice of the subnet for the worker node. Kube-proxy intercepts and controls where to forward the traffic, either to another worker node running your destination pod, or outside of the cluster Kube proxy watches the API Server on the Master Node for the addition and removal of Services endpoints. It configures the IPtable rules to capture the traffic for its ClusterIP and forwards it to one of the endpoints. Worker nodes have internal DNS service and load balancer Within Kubernetes, Ingress is a service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps. You use ingress when you need to support HTTP, HTTPS, TLS, load balancing, expose app outside of the cluster, and custom routing rules... One ingress resource is required by namespace. So if microservices are in the same namespace you can define a domain name for those services (e.g. assetmanagement.greencompute.ibmcase.com) and defined path for each service: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : assetmanagement spec : rules : - host : assetmanagement.greencompute.ibmcase.com http : paths : - path : /assetconsumer backend : serviceName : asset-consumer-svc servicePort : 8080 - path : /assetdashboard backend : serviceName : asset-dashboard-bff-svc servicePort : 8080 - path : /assetmgrms backend : serviceName : asset-mgr-ms-svc servicePort : 8080 The backend for front end component, the asset manager microservice and the asset consumer components are exposed in the same domain. The serviceName matches the service exposed for each components. The following diagram presents how an external application accesses deployed microservice within Kubernetes pod. The following diagram shows how Ingress directs communication from the internet to a deployed microservice: A user sends a request to your app by accessing your app's URL. Using DNS name abstracts the application from the underlying infrastructure. Inter clusters microservice to microservice should use the same approach A DNS system service resolves the hostname in the URL to the portable public IP address of the load balancer Based on the resolved IP address, the client sends the request to the load balancer service that exposes the Application Load Balancer (ALB) The ALB checks if a routing rule for the app path in the cluster exists. If a matching rule is found, the request is forwarded according to the rules that you defined in the Ingress resource to the pod where the app is deployed. If multiple app instances are deployed in the cluster, the ALB load balances the requests between the app pods. To also load balance incoming HTTPS connections, you can configure the ALB to you can use your own TLS certificate to decrypt the network traffic. Microservice to microservice can use this DNS name to communicate between service. Using Ingress, the global load balancer can support parallel, cross region, clusters. Service exposition There is an architecture style focusing on APIs which proposes to have different SLA and semantic for external, internet facing API versus internal back end APIs only exposed within intranet. This article presents using different API gateways to support this architecture. Backend data services are not exposed directly to internet. API Gateway provides a secure end point for external web app to access those business functions. So the decisions on how to expose service are linked to: do you need to do API management do you need to secure APIs do you need to expose to internet do you need to support other protocol then HTTP do you need to have multiple instance of the application When deploying a microservice to Kubernetes it is recommended to use Ingress rule as presented above.. The following yaml file exposes the BFF service using ClusterIP: apiVersion : v1 kind : Service metadata : name : asset-consumer-svc labels : chart : asset-consumer spec : type : ClusterIP ports : - port : 8080 targetPort : 8080 protocol : TCP name : asset-consumer-svc selector : app : asset-consumer Service discovery When deploying on Kubernetes cluster, microservices use the DNS lookup to discover deployed microservice. ISTIO ISTIO provides an easy way to create a network of deployed services with load balancing, service-to-service authentication, traffic flow management, monitoring, etc... By deploying a special sidecar proxy (called Envoy) throughout your environment, all network communication between microservices is intercepted and controlled by ISTIO control panel. The control plane manages the overall network infrastructure and enforces the policy and traffic rules. To deploy ISTIO to IBM cloud private you can access the ICP catalog and search for istio. But as it is the version 0.7 we recommend to do your own installation using istio.io download page to get the last release. ICP installation Here is a quick summary of the steps: #1- Download istio latest versions #2- Modify your PATH to get access to istioctl CLI tool. Verify with $ istioctl version #3- Connect to your ICP cluster #4- Install istio without TLS security $ kubectl apply -f install/kubernetes/istio-demo.yaml #5- Verify the deployed services $ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.0.0.170 <none> 3000/TCP 16m istio-citadel ClusterIP 10.0.0.123 <none> 8060/TCP,9093/TCP 16m istio-egressgateway ClusterIP 10.0.0.16 <none> 80/TCP,443/TCP 16m istio-galley ClusterIP 10.0.0.52 <none> 443/TCP 16m istio-grafana ClusterIP 10.0.0.71 <none> 3000/TCP 14d istio-ingress LoadBalancer 10.0.0.91 <pending> 80:31196/TCP,443:30664/TCP 14d istio-mixer ClusterIP 10.0.0.6 <none> 9091/TCP,15004/TCP,9093/TCP,9094/TCP,9102/TCP,9125/UDP,42422/TCP 14d istio-pilot ClusterIP 10.0.0.39 <none> 15003/TCP,15005/TCP,15007/TCP,15010/TCP,15011/TCP,8080/TCP,9093/TCP,443/TCP 14d istio-policy ClusterIP 10.0.0.17 <none> 9091/TCP,15004/TCP,9093/TCP 16m istio-security ClusterIP 10.0.0.162 <none> 8060/TCP 14d istio-servicegraph ClusterIP 10.0.0.65 <none> 8088/TCP 14d istio-sidecar-injector ClusterIP 10.0.0.195 <none> 443/TCP 14d istio-statsd-prom-bridge ClusterIP 10.0.0.37 <none> 9102/TCP,9125/UDP 16m istio-telemetry ClusterIP 10.0.0.27 <none> 9091/TCP,15004/TCP,9093/TCP,42422/TCP 16m istio-zipkin ClusterIP 10.0.0.96 <none> 9411/TCP 14d prometheus ClusterIP 10.0.0.118 <none> 9090/TCP 14d servicegraph ClusterIP 10.0.0.7 <none> 8088/TCP 16m tracing ClusterIP 10.0.0.66 <none> 80/TCP 16m zipkin ClusterIP 10.0.0.176 <none> 9411/TCP 16m The default installation configuration does not install sidecar-injector, Prometheus, Grafana, service-graph, zipkin but istio-proxy, Ingress, Mixer, Pilot. Installing your solution Be sure your application is using HTTP 1.1 or 2.0. To create a service mesh with Istio, you update the deployment of the pods to add the Istio Proxy (based on the Lyft Envoy Proxy) as a side car to each pod. With the deployment of istio-sidecar-injector this is done automatically for any container deployed within a namespace where istio is enabled. Here is a command to do so: kubectl label namespace greencompute istio-injection=enabled We are summarizing the support to Istio for a specific solution in this article . More reading Istio and Kubernetes Workshop Advanced traffic management with ISTIO Istio workshop for IBM Cloud Container service Our Istio FAQ Asynchronous loosely coupled solution using events If we change of paradigm and use a messaging approach or better an event approach of the data update requirements, we will implement a loosly coupled solution with a pub/sub communication protocol. We need to think about the activities that apply within each service and how they can be of interest to other components. Internal microservice tasks are becoming facts about something happened and those facts may be published as events for others to consume. The first level of refactoring may become: An event is a fact that happens in the past and carry all the data needed, and it becomes a source of record. It becoming consistent if it is played in a messaging backbone via topics. But the persistence of data can be externalized to consumer and then simplify the architecture: Then we can use the history of the persisted events to add features not address before, and outside of the direct scope of a microservice. For example to compute the number of users added last month, just a query on the users topic will get the answer: no or very limited coding needed. We recommend to go deeper in event driven architecture with this site","title":"Service mesh"},{"location":"service-mesh/readme/#microservice-mesh","text":"In this note we are grouping the studies around microservice to microservice communication with Kubernetes deployment. We are addressing: how ingress controller helps inside Kubernetes how API gateway helps for API management and service integration how to expose service in hybrid cloud how to discover service All come back to the requirements, skill set and fit to purpose.","title":"Microservice mesh"},{"location":"service-mesh/readme/#definitions","text":"Service meshes provide visibility, resiliency, traffic, and security control of distributed application services. They deliver policy-based networking for microservices in the contraints of virtual network and continuous topology updates. Externalizing, via declarations, the logic to support network potential issues, like resiliency, simplifies dramatically developers work. Some misconception to clarify around microservice and APIs: microservices are not fine grained web services APIs are not equivalent to microservices microservices are not implementation of APIs API is an interface, a way to make a request to get or change data in an application. In modern use API refers to REST web APIs using HTTP protocol, with JSON format (sometime XML is still used). Interface decouples the caller from the implementation. The caller has no idea how it is implemented. A microservice is in fact a component. micro refers to the granularity of the component not of the exposed interface. The following diagram illustrates all those concepts. We encourage you to go to read integration design and architecture series . Container orchestration like Kubernetes are mainly doing application scheduling, cluster management, resource provisioning, platform and workload monitoring and service discovery. When application solutions are growing in size and complexity, you need to addres the following items: visibility on how traffic is flowing between microservice, how routing is done between microservice based on requests contained or the origination point or the end point how to support resiliency by handling failure in a graceful manner how to ensure security with identity assertion how to enforce security policy which defined the requirements for service mesh. Service mesh architecture defines a data and control planes: Control plane: supports policy and configuration for services in the mesh, and provides aggregation for telemetry. It has API and CLI to centralize control to the services deployed. In Kubernetes control planes are deployed in a system namespace. Data plane: handles the actual inspection, transiting, and routing of network traffic. It is responsible for health checking, load balancing, authentication, authorization, inbound (ingress) and outbound (egress) cluster network traffic. Applications / microservices are unaware of data plane.","title":"Definitions"},{"location":"service-mesh/readme/#context","text":"Traditional modern architecture involves having different components exposing reusable APIs, addressing different channels (mobile, single page application, traditional server pages or B2B apps), consuming APIs (mobile APIs, back end for front end, shared common apis like authentication, authorization,...) and backend services addressing reusable business services: API management can be added via API gateway. This is a distributed application with cross related communication channels, where any changes to the service interface characteristics impact any of the components. Moving to microservices architecture style adds more communication challenges and devops complexity but provides a lot of business values such as: rapid deployment of new business capabilities, co-evolving in parallel of other services. focusing on business domain with clear ownership of the business function and feature roadmap better operation procedure, automated, and with easy rollout and continuous delivery. A/B testing to assess how new feature deployed improve business operations * Improve resiliency by deploying on multi language cluster As an example we can use the following predictive maintenance asset solution with the following capabilities to support: user authentication user management: add / delete new user user self registration, reset password user permission control user profile asset management risk assessment service Each could be grouped by business domain like the user management, asset management, and application access control. So domain separation can be a good microservice boundary. But if the number of user reach millions then we may need to optimize the runtime processing of reading user credential, and scale the service differently, leading to a service map like the diagram below, where runtime and management are separated services. All of these still does not address the fact that data are distributed and even more with microservices owning their data persistence. As developers and architects we still have to address the following data integrity problems: two phases commit compensating operation eventual data consistency: some microservice updating data may share those updates with other microservices. Data aggregation: adding new views on data, owned by a microservice, to support new aggregates. Examples are preparing data view for machine learning modeling, analytics, or business intelligence... From the previous microservice allocation we can see the needs to propagate data update between services. Adding or unsubscribing a user involves updating the asset the user own and the authentication runtime service: Adding a new application changes the authorization runtime service. We are now looking at the following questions: how does webapp access APIs for their main service, of back end for front end service. how does deployed microservice access other service: discover and access? How data consistency can be ensured? is there a simpler way to manage cross microservice dependency? The answers depend on the existing infrastructure and environment, and deployment needs.","title":"Context"},{"location":"service-mesh/readme/#service-routing","text":"We have to dissociate intra-cluster communication versus inter clusters or cluster to external services. Without getting into too much detail of IP routing within Kubernetes some important elements of the cluster are important to remember: microservices are packaged as docker container and expose port. When deployed they run in a pod within a node (physical or virtual machine) containers can talk to other containers only if they are on the same machine, or when they have exposed port. * Kubernetes is configured with a large flat subnet (e.g. 172.30.0.0/16) which is used for internal application traffic inside of the cluster. Each worker node in the Kubernetes cluster is assigned one or more non-overlapping slices of this network, coordinated by the Kubernetes master node. When a container is created in the cluster, it gets assigned to a worker node and is given an IP address from the slice of the subnet for the worker node. Kube-proxy intercepts and controls where to forward the traffic, either to another worker node running your destination pod, or outside of the cluster Kube proxy watches the API Server on the Master Node for the addition and removal of Services endpoints. It configures the IPtable rules to capture the traffic for its ClusterIP and forwards it to one of the endpoints. Worker nodes have internal DNS service and load balancer Within Kubernetes, Ingress is a service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps. You use ingress when you need to support HTTP, HTTPS, TLS, load balancing, expose app outside of the cluster, and custom routing rules... One ingress resource is required by namespace. So if microservices are in the same namespace you can define a domain name for those services (e.g. assetmanagement.greencompute.ibmcase.com) and defined path for each service: apiVersion : extensions/v1beta1 kind : Ingress metadata : name : assetmanagement spec : rules : - host : assetmanagement.greencompute.ibmcase.com http : paths : - path : /assetconsumer backend : serviceName : asset-consumer-svc servicePort : 8080 - path : /assetdashboard backend : serviceName : asset-dashboard-bff-svc servicePort : 8080 - path : /assetmgrms backend : serviceName : asset-mgr-ms-svc servicePort : 8080 The backend for front end component, the asset manager microservice and the asset consumer components are exposed in the same domain. The serviceName matches the service exposed for each components. The following diagram presents how an external application accesses deployed microservice within Kubernetes pod. The following diagram shows how Ingress directs communication from the internet to a deployed microservice: A user sends a request to your app by accessing your app's URL. Using DNS name abstracts the application from the underlying infrastructure. Inter clusters microservice to microservice should use the same approach A DNS system service resolves the hostname in the URL to the portable public IP address of the load balancer Based on the resolved IP address, the client sends the request to the load balancer service that exposes the Application Load Balancer (ALB) The ALB checks if a routing rule for the app path in the cluster exists. If a matching rule is found, the request is forwarded according to the rules that you defined in the Ingress resource to the pod where the app is deployed. If multiple app instances are deployed in the cluster, the ALB load balances the requests between the app pods. To also load balance incoming HTTPS connections, you can configure the ALB to you can use your own TLS certificate to decrypt the network traffic. Microservice to microservice can use this DNS name to communicate between service. Using Ingress, the global load balancer can support parallel, cross region, clusters.","title":"Service routing"},{"location":"service-mesh/readme/#service-exposition","text":"There is an architecture style focusing on APIs which proposes to have different SLA and semantic for external, internet facing API versus internal back end APIs only exposed within intranet. This article presents using different API gateways to support this architecture. Backend data services are not exposed directly to internet. API Gateway provides a secure end point for external web app to access those business functions. So the decisions on how to expose service are linked to: do you need to do API management do you need to secure APIs do you need to expose to internet do you need to support other protocol then HTTP do you need to have multiple instance of the application When deploying a microservice to Kubernetes it is recommended to use Ingress rule as presented above.. The following yaml file exposes the BFF service using ClusterIP: apiVersion : v1 kind : Service metadata : name : asset-consumer-svc labels : chart : asset-consumer spec : type : ClusterIP ports : - port : 8080 targetPort : 8080 protocol : TCP name : asset-consumer-svc selector : app : asset-consumer","title":"Service exposition"},{"location":"service-mesh/readme/#service-discovery","text":"When deploying on Kubernetes cluster, microservices use the DNS lookup to discover deployed microservice.","title":"Service discovery"},{"location":"service-mesh/readme/#istio","text":"ISTIO provides an easy way to create a network of deployed services with load balancing, service-to-service authentication, traffic flow management, monitoring, etc... By deploying a special sidecar proxy (called Envoy) throughout your environment, all network communication between microservices is intercepted and controlled by ISTIO control panel. The control plane manages the overall network infrastructure and enforces the policy and traffic rules. To deploy ISTIO to IBM cloud private you can access the ICP catalog and search for istio. But as it is the version 0.7 we recommend to do your own installation using istio.io download page to get the last release.","title":"ISTIO"},{"location":"service-mesh/readme/#icp-installation","text":"Here is a quick summary of the steps: #1- Download istio latest versions #2- Modify your PATH to get access to istioctl CLI tool. Verify with $ istioctl version #3- Connect to your ICP cluster #4- Install istio without TLS security $ kubectl apply -f install/kubernetes/istio-demo.yaml #5- Verify the deployed services $ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.0.0.170 <none> 3000/TCP 16m istio-citadel ClusterIP 10.0.0.123 <none> 8060/TCP,9093/TCP 16m istio-egressgateway ClusterIP 10.0.0.16 <none> 80/TCP,443/TCP 16m istio-galley ClusterIP 10.0.0.52 <none> 443/TCP 16m istio-grafana ClusterIP 10.0.0.71 <none> 3000/TCP 14d istio-ingress LoadBalancer 10.0.0.91 <pending> 80:31196/TCP,443:30664/TCP 14d istio-mixer ClusterIP 10.0.0.6 <none> 9091/TCP,15004/TCP,9093/TCP,9094/TCP,9102/TCP,9125/UDP,42422/TCP 14d istio-pilot ClusterIP 10.0.0.39 <none> 15003/TCP,15005/TCP,15007/TCP,15010/TCP,15011/TCP,8080/TCP,9093/TCP,443/TCP 14d istio-policy ClusterIP 10.0.0.17 <none> 9091/TCP,15004/TCP,9093/TCP 16m istio-security ClusterIP 10.0.0.162 <none> 8060/TCP 14d istio-servicegraph ClusterIP 10.0.0.65 <none> 8088/TCP 14d istio-sidecar-injector ClusterIP 10.0.0.195 <none> 443/TCP 14d istio-statsd-prom-bridge ClusterIP 10.0.0.37 <none> 9102/TCP,9125/UDP 16m istio-telemetry ClusterIP 10.0.0.27 <none> 9091/TCP,15004/TCP,9093/TCP,42422/TCP 16m istio-zipkin ClusterIP 10.0.0.96 <none> 9411/TCP 14d prometheus ClusterIP 10.0.0.118 <none> 9090/TCP 14d servicegraph ClusterIP 10.0.0.7 <none> 8088/TCP 16m tracing ClusterIP 10.0.0.66 <none> 80/TCP 16m zipkin ClusterIP 10.0.0.176 <none> 9411/TCP 16m The default installation configuration does not install sidecar-injector, Prometheus, Grafana, service-graph, zipkin but istio-proxy, Ingress, Mixer, Pilot.","title":"ICP installation"},{"location":"service-mesh/readme/#installing-your-solution","text":"Be sure your application is using HTTP 1.1 or 2.0. To create a service mesh with Istio, you update the deployment of the pods to add the Istio Proxy (based on the Lyft Envoy Proxy) as a side car to each pod. With the deployment of istio-sidecar-injector this is done automatically for any container deployed within a namespace where istio is enabled. Here is a command to do so: kubectl label namespace greencompute istio-injection=enabled We are summarizing the support to Istio for a specific solution in this article .","title":"Installing your solution"},{"location":"service-mesh/readme/#more-reading","text":"Istio and Kubernetes Workshop Advanced traffic management with ISTIO Istio workshop for IBM Cloud Container service Our Istio FAQ","title":"More reading"},{"location":"service-mesh/readme/#asynchronous-loosely-coupled-solution-using-events","text":"If we change of paradigm and use a messaging approach or better an event approach of the data update requirements, we will implement a loosly coupled solution with a pub/sub communication protocol. We need to think about the activities that apply within each service and how they can be of interest to other components. Internal microservice tasks are becoming facts about something happened and those facts may be published as events for others to consume. The first level of refactoring may become: An event is a fact that happens in the past and carry all the data needed, and it becomes a source of record. It becoming consistent if it is played in a messaging backbone via topics. But the persistence of data can be externalized to consumer and then simplify the architecture: Then we can use the history of the persisted events to add features not address before, and outside of the direct scope of a microservice. For example to compute the number of users added last month, just a query on the users topic will get the answer: no or very limited coding needed. We recommend to go deeper in event driven architecture with this site","title":"Asynchronous loosely coupled solution using events"},{"location":"toSaaS/readme/","text":"Lift and shift of an integration solution In this article and solution implementation we are detailing how to migrate part of an 'hybrid cloud reference architecture implementation solution' based on MQ, DB2 and traditional WAS (tWAS) to a pure IBM Cloud deployment. Updated (03/12/2018). The following diagram illustrates the starting environment: where the components are: User interface is the current cloud native portal application done with Angular 6 and BFF with nodejs that we can deploy as a cloud foundry app, or as container on IBM Container service. (See this project for detail ). Inventory data access layer is a JEE app used to expose SOAP interface implemented with JAXWS and JPA to support SOA services as data access layer to the inventory database. It is deployed to Traditional WebSphere Application Service 8.5.5 (tWAS). An inventory DB2 to persist items, suppliers and inventory data in DB2. Item event producer is a java application to simulate event created from a warehouse when an item is added to the inventory in the given warehouse. This is an event sent as a message to a Queue defined in the MQ manager. An event listener application implemented as a Message Driven Bean (MDB) deployed on tWAS. This application persists the date to the inventory database. The target environment will be using the same components but we move the DB, WAS apps and MQ to IBM Cloud. In the target environment the MQ layer is using one queue manager deployed on IBM Cloud. The MDB listens to queue defined on queue manager on Cloud and uses the Data Access Layer SOAP service to save the item it received from the message payload. Pre-requisites To be able to perform this lift and shift exercise by yourself you can use two approaches: Use at least three virtual machines with Linux OS (Ubuntu or RedHat) and install the IBM products on them. We did this approach at first to develop all the content. Use 3 docker images we have built to 'simulate' the on-premise environment. As the goal is really taking running workloads and move to IBM Cloud, the source environment, from a tutorial point of view, is less relevant. For production workload, then it will be important to assess the different configurations. Running on-premise using docker The DB2 and MQ projects have their Dockerfile for building custom images from the official IBM dockerhub images or use the public images we built and pushed to dockerhub. In this project the docker folder has scripts and docker compose file to start the 'on-premise' environment on your local machine. Lift And Shift We propose incremental move: 1- Db2 database migration We are addressing the lift and shift DB2 workload migration in this note where we use different approaches, one using dbmove tools and one lift tooling. 2- Traditional WAS App lift and shift We are using the same approach as detailed in this tutorial. but we adapt it for our purpose. The application to migrate is in the data access layer service for the inventory data base as defined in this repository . For refresh our memory or for beginner we are providing how to configure the resources to access DB2 from WAS in this note . Once resources are configured we can deploy the war file using the admin console or with script: see the note here for detail on how to deploy the war to WAS. Tooling There are a set of tools available to assess application for migration: WebSphere Application Server Migration Toolkit https://developer.ibm.com/wasdev/downloads/#asset/tools-WebSphere_Application_Server_Migration_Toolkit: The migration toolkit provides Eclipse-based tools for WebSphere migration scenarios including Cloud migration, WebSphere version to version migration including WAS Liberty, and migration from third-party application servers. Videos and demos around toolingHow to use migration toolkit for Discovery, assessment& binary scan : https://www-01.ibm.com/support/docview.wss?uid=swg27008724&aid=11 Getting started with WebSphere in the Cloud : https://developer.ibm.com/wasdev/docs/getting-started-websphere-cloud/ Moving WebSphere Worklods to public Cloud : https://www.ibm.com/cloud/garage/tutorials/was_lift_shift/Learn how to use the public cloud as either the upgrade development and test environment for your WebSphere infrastructure or as your new permanent environment. Migrating your WebSphere configurations to Cloud: https://ibm.ent.box.com/s/vja6fm8u3mktw9v26x1glvayvv5vlzrq WebSphere Configuration migration tool for IBM Cloud : https://developer.ibm.com/wasdev/docs/websphere-config-migration-cloud/ Moving applications to the cloud :https://developer.ibm.com/wasdev/docs/migration/ 3- MQ based messaging solution The implementation of the messaging solution is done in this repository and we are providing a step by step tutorial to do MQ workload lift and shift.","title":"Context"},{"location":"toSaaS/readme/#lift-and-shift-of-an-integration-solution","text":"In this article and solution implementation we are detailing how to migrate part of an 'hybrid cloud reference architecture implementation solution' based on MQ, DB2 and traditional WAS (tWAS) to a pure IBM Cloud deployment. Updated (03/12/2018). The following diagram illustrates the starting environment: where the components are: User interface is the current cloud native portal application done with Angular 6 and BFF with nodejs that we can deploy as a cloud foundry app, or as container on IBM Container service. (See this project for detail ). Inventory data access layer is a JEE app used to expose SOAP interface implemented with JAXWS and JPA to support SOA services as data access layer to the inventory database. It is deployed to Traditional WebSphere Application Service 8.5.5 (tWAS). An inventory DB2 to persist items, suppliers and inventory data in DB2. Item event producer is a java application to simulate event created from a warehouse when an item is added to the inventory in the given warehouse. This is an event sent as a message to a Queue defined in the MQ manager. An event listener application implemented as a Message Driven Bean (MDB) deployed on tWAS. This application persists the date to the inventory database. The target environment will be using the same components but we move the DB, WAS apps and MQ to IBM Cloud. In the target environment the MQ layer is using one queue manager deployed on IBM Cloud. The MDB listens to queue defined on queue manager on Cloud and uses the Data Access Layer SOAP service to save the item it received from the message payload.","title":"Lift and shift of an integration solution"},{"location":"toSaaS/readme/#pre-requisites","text":"To be able to perform this lift and shift exercise by yourself you can use two approaches: Use at least three virtual machines with Linux OS (Ubuntu or RedHat) and install the IBM products on them. We did this approach at first to develop all the content. Use 3 docker images we have built to 'simulate' the on-premise environment. As the goal is really taking running workloads and move to IBM Cloud, the source environment, from a tutorial point of view, is less relevant. For production workload, then it will be important to assess the different configurations.","title":"Pre-requisites"},{"location":"toSaaS/readme/#running-on-premise-using-docker","text":"The DB2 and MQ projects have their Dockerfile for building custom images from the official IBM dockerhub images or use the public images we built and pushed to dockerhub. In this project the docker folder has scripts and docker compose file to start the 'on-premise' environment on your local machine.","title":"Running on-premise using docker"},{"location":"toSaaS/readme/#lift-and-shift","text":"We propose incremental move:","title":"Lift And Shift"},{"location":"toSaaS/readme/#1-db2-database-migration","text":"We are addressing the lift and shift DB2 workload migration in this note where we use different approaches, one using dbmove tools and one lift tooling.","title":"1- Db2 database migration"},{"location":"toSaaS/readme/#2-traditional-was-app-lift-and-shift","text":"We are using the same approach as detailed in this tutorial. but we adapt it for our purpose. The application to migrate is in the data access layer service for the inventory data base as defined in this repository . For refresh our memory or for beginner we are providing how to configure the resources to access DB2 from WAS in this note . Once resources are configured we can deploy the war file using the admin console or with script: see the note here for detail on how to deploy the war to WAS.","title":"2- Traditional WAS App lift and shift"},{"location":"toSaaS/readme/#tooling","text":"There are a set of tools available to assess application for migration: WebSphere Application Server Migration Toolkit https://developer.ibm.com/wasdev/downloads/#asset/tools-WebSphere_Application_Server_Migration_Toolkit: The migration toolkit provides Eclipse-based tools for WebSphere migration scenarios including Cloud migration, WebSphere version to version migration including WAS Liberty, and migration from third-party application servers. Videos and demos around toolingHow to use migration toolkit for Discovery, assessment& binary scan : https://www-01.ibm.com/support/docview.wss?uid=swg27008724&aid=11 Getting started with WebSphere in the Cloud : https://developer.ibm.com/wasdev/docs/getting-started-websphere-cloud/ Moving WebSphere Worklods to public Cloud : https://www.ibm.com/cloud/garage/tutorials/was_lift_shift/Learn how to use the public cloud as either the upgrade development and test environment for your WebSphere infrastructure or as your new permanent environment. Migrating your WebSphere configurations to Cloud: https://ibm.ent.box.com/s/vja6fm8u3mktw9v26x1glvayvv5vlzrq WebSphere Configuration migration tool for IBM Cloud : https://developer.ibm.com/wasdev/docs/websphere-config-migration-cloud/ Moving applications to the cloud :https://developer.ibm.com/wasdev/docs/migration/","title":"Tooling"},{"location":"toSaaS/readme/#3-mq-based-messaging-solution","text":"The implementation of the messaging solution is done in this repository and we are providing a step by step tutorial to do MQ workload lift and shift.","title":"3- MQ based messaging solution"},{"location":"toSaaS/twas-res/","text":"Configure resource in WebSphere Application Server DB2 access for tWAS deployed on-premise To access DB2 from a JEE application we need to define: a DB provider which defines what jars to load for JDBC connection a user id and password credential as J2C a data source to reference the target database inside DB2. You need to be logged as an administrator user to the IBM Console. JDBC Provider You need to get your db2jcc jar file from DB2 server and copy it somewhere under the WAS install folder on the WAS server. Click on Resources -> JDBC -> JDBC Provider -> New You can select a scope at the Node or Cluster level depending of the deployment and high availability requirements. Complete the JDBC Provider Fields. Database type : DB2 Provider type: DB2 Using IBM JCC Driver Implementation type: you can select XA (for a two-phases commit) or connection pool data source for one-phase commit. Specify the class path to access the jdbc jar file and the jar name. User authentication Create JAAS \u2013 J2C authentication Data specifying DB2 username and password for Container Connection: Click on Security -> Global security -> Java Authentication and Authorization Service: Add a new entry for the DB2INST1 user: Data source The goal here is to create a data sources for the 'Items / inventory' database. Click on Resources -> JDBC -> Data sources, select the cell scope and new button. Enter name and JNDI name to be used. See the application descriptor for getting the JNDI name. Select the JNDC provider as defined in previous step: Enter database specific properties like database name, IP address or dns name for the DB2 server and the port number it listens to connection: Select the 'db2alias' you created previously for the container-managed authentication alias: Save the configuration and then Test connection . You should get a successful connection:","title":"Configure resource in WebSphere Application Server"},{"location":"toSaaS/twas-res/#configure-resource-in-websphere-application-server","text":"","title":"Configure resource in WebSphere Application Server"},{"location":"toSaaS/twas-res/#db2-access-for-twas-deployed-on-premise","text":"To access DB2 from a JEE application we need to define: a DB provider which defines what jars to load for JDBC connection a user id and password credential as J2C a data source to reference the target database inside DB2. You need to be logged as an administrator user to the IBM Console.","title":"DB2 access for tWAS deployed on-premise"},{"location":"toSaaS/twas-res/#jdbc-provider","text":"You need to get your db2jcc jar file from DB2 server and copy it somewhere under the WAS install folder on the WAS server. Click on Resources -> JDBC -> JDBC Provider -> New You can select a scope at the Node or Cluster level depending of the deployment and high availability requirements. Complete the JDBC Provider Fields. Database type : DB2 Provider type: DB2 Using IBM JCC Driver Implementation type: you can select XA (for a two-phases commit) or connection pool data source for one-phase commit. Specify the class path to access the jdbc jar file and the jar name.","title":"JDBC Provider"},{"location":"toSaaS/twas-res/#user-authentication","text":"Create JAAS \u2013 J2C authentication Data specifying DB2 username and password for Container Connection: Click on Security -> Global security -> Java Authentication and Authorization Service: Add a new entry for the DB2INST1 user:","title":"User authentication"},{"location":"toSaaS/twas-res/#data-source","text":"The goal here is to create a data sources for the 'Items / inventory' database. Click on Resources -> JDBC -> Data sources, select the cell scope and new button. Enter name and JNDI name to be used. See the application descriptor for getting the JNDI name. Select the JNDC provider as defined in previous step: Enter database specific properties like database name, IP address or dns name for the DB2 server and the port number it listens to connection: Select the 'db2alias' you created previously for the container-managed authentication alias: Save the configuration and then Test connection . You should get a successful connection:","title":"Data source"},{"location":"toSaaS/twas/readme/","text":"Lift and shift This tutorial will demonstrate how to lift and shift WebSphere workload running on-premises to IBM public cloud. When move to Cloud you will be on the latest version of the WebSphere platform available on Cloud. For this scenario, we are using an On-premise workload running on WebSphere version 8.5.5.13 and the Cloud WebSphere version is running on version 9.x. On completing this tutorial, you will learn the steps required to successfully migrate applications to cloud. Pre-requisites An IBM public cloud account Access to On-premise WebSphere environment Access to WebSphere Configuration Migration Tool Install WebSphere Configuration Migration Tool (WCMT) Download the WebSphere Configuration Migration Tool for IBM Cloud and install it by unzipping it to a directory of your choice. For this scenario we installed the configuration migration tool on the same system where the WebSphere workload is running. https://developer.ibm.com/wasdev/downloads/#asset/tools-WebSphere_Configuration_Migration_Tool_for_IBM_Cloud Note: Set the JAVA_HOME environment variable to a version of Java 7 or higher. Also pre-pend the $JAVA_HOME/bin directory to your system path. To run the tool, use the following command: java -jar CloudMigration.jar When you run the tool for the first time you will be prompted to set the language and then to accept the license agreement. Select to use one-time single sign on and select the link below to collect the one time password. Enter the passcode obtained from the link and click login On the successful login page click \u2018Next \u2018 Select organization and space and click \u2018New\u2019 Enter a new name for the services and select the environment. Click create service Click \u2018Next\u2019 Select the WebSphere Cell you want to migrate and click \u2018Next\u2019 Read the summary screen and click \u2018continue\u2019 Click the migrate button for the profiles to migrate to Cloud. For this scenario, we select both Deployment manager and node profile for migration. Provide the admin credentials to gather the profile configurations and click upload. You can click migrate button for all the profiles in parallel. Click the link in the \u201cComplete the migration screen\u201d to go to the provisioning window in Cloud. Also you can bookmark the provision the environment later. Click the \u2018Finish button\u2019 to close window. Select traditional cell GO down and select the size of your Deployment manager VM Select the number and size of the nodes Review summary and click provision Post Lift and Shift activities Open the service from the Cloud Dashboard and verify the information Configure the VPN connection Gather login credential to systems Open the ports on WAS servers to connect to internal services and external systems Verify the connection to third party systems such as Database server Logon to WebSphere console and verify the migrated configuration Reconfigure the data source to connect to Cloud Database and validate Review the binary scan report, make the updates recommended and redeploy the applications Validate the application Open the service from the Cloud Dashboard and verify the information Find the service (Brown-WASND) from the dashboard and open it by clicking on it. Verify the information Configure the VPN connection Click on your service \u2018Brown-WASND\u2019 service on your cloud dashboard. Follow the VPN instructions and download the VPN configuration to connect to the Cloud. Using \u2018Tunnelblick\u2019 on my desktop to connect to US-South Public Cloud. Verify the connection by pinging from your terminal window Gather the credentials required Next step is to capture the login credentials (root , WebSphere console login ,etc) from the dashboard for all WebSphere systems. Gather the ports WebSphere processes using from : /opt/IBM/WebSphere/Profiles/BrownDM01/logs/AboutThisMigratedProfile.txt SSH to the server as root user Eg : ssh root @ 169.55 . 3.25 Note down the ports for the deployment manager from /opt/IBM/WebSphere/Profiles/BrownDM01/logs/AboutThisMigratedProfile.txt Open the ports on WAS servers to connect to internal services and external systems Logon to the WAS servers: ssh root@ 169.55.3.25 Change directory to: /opt/IBM/WebSphere/AppServer/virtual/bin drwxrwxr-x 2 virtuser admingroup 4096 Aug 30 18:25 . drwxrwxr-x 3 virtuser admingroup 4096 Aug 30 18:25 .. -rwxrwxr-x 1 virtuser admingroup 1604 Aug 30 18:25 federate.sh -rwxrwxr-x 1 virtuser admingroup 1897 Aug 30 18:25 openFirewallPorts.sh -rwxrwxr-x 1 virtuser admingroup 1667 Aug 30 18:25 openWASPorts.sh Execute: ./openWASPorts.sh Execute: ./openFirewallPorts.sh to open ports to other systems like database or LDAP servers Eg: ./openFirewallPorts.sh \u2013ports 50000 \u2013persist true Verify the connection to third party systems such as the Cloud database instance For this lift and shift scenario we are using a Db2 on Cloud instance running on Bluemix Database details: hostname : dashdb - txn - sbox - yp - dal09 - 04 . services . dal . bluemix . net User Id : vvb46996 password : ******** Database : BLUDB port : 50000 Telnet to the DB server with the port to verify the connection. Logon to WebSphere console and verify the migrated configuration Logon to the WebSphere Console Verify the version, server and applications got migrated Reconfigure the data source to connect to Cloud Database and validate Change the existing data source configuration to point to the Cloud database instance. Check the JDBC driver Path and actual location of the jar files Test the Data Source connection Review the binary scan report, make the updates recommended and redeploy the applications Highlights from the binary scan report Apply all the changes recommended under the \u2018Severe Rules\u2019 section and redeploy the application.","title":"Tarditional WAS app"},{"location":"toSaaS/twas/readme/#lift-and-shift","text":"This tutorial will demonstrate how to lift and shift WebSphere workload running on-premises to IBM public cloud. When move to Cloud you will be on the latest version of the WebSphere platform available on Cloud. For this scenario, we are using an On-premise workload running on WebSphere version 8.5.5.13 and the Cloud WebSphere version is running on version 9.x. On completing this tutorial, you will learn the steps required to successfully migrate applications to cloud.","title":"Lift and shift"},{"location":"toSaaS/twas/readme/#pre-requisites","text":"An IBM public cloud account Access to On-premise WebSphere environment Access to WebSphere Configuration Migration Tool","title":"Pre-requisites"},{"location":"toSaaS/twas/readme/#install-websphere-configuration-migration-tool-wcmt","text":"Download the WebSphere Configuration Migration Tool for IBM Cloud and install it by unzipping it to a directory of your choice. For this scenario we installed the configuration migration tool on the same system where the WebSphere workload is running. https://developer.ibm.com/wasdev/downloads/#asset/tools-WebSphere_Configuration_Migration_Tool_for_IBM_Cloud Note: Set the JAVA_HOME environment variable to a version of Java 7 or higher. Also pre-pend the $JAVA_HOME/bin directory to your system path. To run the tool, use the following command: java -jar CloudMigration.jar When you run the tool for the first time you will be prompted to set the language and then to accept the license agreement. Select to use one-time single sign on and select the link below to collect the one time password. Enter the passcode obtained from the link and click login On the successful login page click \u2018Next \u2018 Select organization and space and click \u2018New\u2019 Enter a new name for the services and select the environment. Click create service Click \u2018Next\u2019 Select the WebSphere Cell you want to migrate and click \u2018Next\u2019 Read the summary screen and click \u2018continue\u2019 Click the migrate button for the profiles to migrate to Cloud. For this scenario, we select both Deployment manager and node profile for migration. Provide the admin credentials to gather the profile configurations and click upload. You can click migrate button for all the profiles in parallel. Click the link in the \u201cComplete the migration screen\u201d to go to the provisioning window in Cloud. Also you can bookmark the provision the environment later. Click the \u2018Finish button\u2019 to close window. Select traditional cell GO down and select the size of your Deployment manager VM Select the number and size of the nodes Review summary and click provision","title":"Install WebSphere Configuration Migration Tool (WCMT)"},{"location":"toSaaS/twas/readme/#post-lift-and-shift-activities","text":"Open the service from the Cloud Dashboard and verify the information Configure the VPN connection Gather login credential to systems Open the ports on WAS servers to connect to internal services and external systems Verify the connection to third party systems such as Database server Logon to WebSphere console and verify the migrated configuration Reconfigure the data source to connect to Cloud Database and validate Review the binary scan report, make the updates recommended and redeploy the applications Validate the application","title":"Post Lift and Shift activities"},{"location":"toSaaS/twas/readme/#open-the-service-from-the-cloud-dashboard-and-verify-the-information","text":"Find the service (Brown-WASND) from the dashboard and open it by clicking on it. Verify the information","title":"Open the service from the Cloud Dashboard and verify the information"},{"location":"toSaaS/twas/readme/#configure-the-vpn-connection","text":"Click on your service \u2018Brown-WASND\u2019 service on your cloud dashboard. Follow the VPN instructions and download the VPN configuration to connect to the Cloud. Using \u2018Tunnelblick\u2019 on my desktop to connect to US-South Public Cloud. Verify the connection by pinging from your terminal window","title":"Configure the VPN connection"},{"location":"toSaaS/twas/readme/#gather-the-credentials-required","text":"Next step is to capture the login credentials (root , WebSphere console login ,etc) from the dashboard for all WebSphere systems. Gather the ports WebSphere processes using from : /opt/IBM/WebSphere/Profiles/BrownDM01/logs/AboutThisMigratedProfile.txt SSH to the server as root user Eg : ssh root @ 169.55 . 3.25 Note down the ports for the deployment manager from /opt/IBM/WebSphere/Profiles/BrownDM01/logs/AboutThisMigratedProfile.txt","title":"Gather the credentials required"},{"location":"toSaaS/twas/readme/#open-the-ports-on-was-servers-to-connect-to-internal-services-and-external-systems","text":"Logon to the WAS servers: ssh root@ 169.55.3.25 Change directory to: /opt/IBM/WebSphere/AppServer/virtual/bin drwxrwxr-x 2 virtuser admingroup 4096 Aug 30 18:25 . drwxrwxr-x 3 virtuser admingroup 4096 Aug 30 18:25 .. -rwxrwxr-x 1 virtuser admingroup 1604 Aug 30 18:25 federate.sh -rwxrwxr-x 1 virtuser admingroup 1897 Aug 30 18:25 openFirewallPorts.sh -rwxrwxr-x 1 virtuser admingroup 1667 Aug 30 18:25 openWASPorts.sh Execute: ./openWASPorts.sh Execute: ./openFirewallPorts.sh to open ports to other systems like database or LDAP servers Eg: ./openFirewallPorts.sh \u2013ports 50000 \u2013persist true","title":"Open the ports on WAS servers to connect to internal services and external systems"},{"location":"toSaaS/twas/readme/#verify-the-connection-to-third-party-systems-such-as-the-cloud-database-instance","text":"For this lift and shift scenario we are using a Db2 on Cloud instance running on Bluemix Database details: hostname : dashdb - txn - sbox - yp - dal09 - 04 . services . dal . bluemix . net User Id : vvb46996 password : ******** Database : BLUDB port : 50000 Telnet to the DB server with the port to verify the connection.","title":"Verify the connection to third party systems such as the Cloud database instance"},{"location":"toSaaS/twas/readme/#logon-to-websphere-console-and-verify-the-migrated-configuration","text":"Logon to the WebSphere Console Verify the version, server and applications got migrated","title":"Logon to WebSphere console and verify the migrated configuration"},{"location":"toSaaS/twas/readme/#reconfigure-the-data-source-to-connect-to-cloud-database-and-validate","text":"Change the existing data source configuration to point to the Cloud database instance. Check the JDBC driver Path and actual location of the jar files Test the Data Source connection","title":"Reconfigure the data source to connect to Cloud Database and validate"},{"location":"toSaaS/twas/readme/#review-the-binary-scan-report-make-the-updates-recommended-and-redeploy-the-applications","text":"Highlights from the binary scan report Apply all the changes recommended under the \u2018Severe Rules\u2019 section and redeploy the application.","title":"Review the binary scan report, make the updates recommended and redeploy the applications"},{"location":"vm/","text":"Use Vagrant to run different backend services of hybrid integration Our environment to develop and test the different Brown or Green compute is using vSphere and a lot of hardware. You may not be able to replicate this environment easily. Therefore for development purpose, we propose to setup use one of the two approach: different virtual machine using Vagrant . Pre-requisites This solution requires you have internet access, experience with WebSphere Application Server, DB2 and MQ and its admin console and familiarity with basic Linux commands. Be to to have clone the linked repositories: * git clone https://github.com/ibm-cloud-architecture/refarch-integration-inventory-db2.git Docker compose: The simplest way is to use IBM Docker images of all the needed product, and docker compose to manage their dependencies. The compose file and different dockerfiles needed to tune our configuration are under db2-mq-tWAS-docker folder. The DB2 image is built from https://hub.docker.com/r/ibmcom/db2express-c/ and tuned for our DB instance and data. Ubuntu, Liberty, DB2 The file is under ubuntu-db2-liberty-vagrant folder. vagrant up vagrant ssh Db2, tWAS, MQ image This approach is not finished The Vagrant file is under db2-mq-tWAS-vagrant folder. vagrant up vagrant ssh","title":"Use Vagrant to run different backend services of hybrid integration"},{"location":"vm/#use-vagrant-to-run-different-backend-services-of-hybrid-integration","text":"Our environment to develop and test the different Brown or Green compute is using vSphere and a lot of hardware. You may not be able to replicate this environment easily. Therefore for development purpose, we propose to setup use one of the two approach: different virtual machine using Vagrant .","title":"Use Vagrant to run different backend services of hybrid integration"},{"location":"vm/#pre-requisites","text":"This solution requires you have internet access, experience with WebSphere Application Server, DB2 and MQ and its admin console and familiarity with basic Linux commands. Be to to have clone the linked repositories: * git clone https://github.com/ibm-cloud-architecture/refarch-integration-inventory-db2.git","title":"Pre-requisites"},{"location":"vm/#docker-compose","text":"The simplest way is to use IBM Docker images of all the needed product, and docker compose to manage their dependencies. The compose file and different dockerfiles needed to tune our configuration are under db2-mq-tWAS-docker folder. The DB2 image is built from https://hub.docker.com/r/ibmcom/db2express-c/ and tuned for our DB instance and data.","title":"Docker compose:"},{"location":"vm/#ubuntu-liberty-db2","text":"The file is under ubuntu-db2-liberty-vagrant folder. vagrant up vagrant ssh","title":"Ubuntu, Liberty, DB2"},{"location":"vm/#db2-twas-mq-image","text":"This approach is not finished The Vagrant file is under db2-mq-tWAS-vagrant folder. vagrant up vagrant ssh","title":"Db2, tWAS, MQ image"}]}